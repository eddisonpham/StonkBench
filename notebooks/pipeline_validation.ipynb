{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "import yaml\n",
    "from abc import ABC, abstractmethod\n",
    "import inspect\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.preprocessing.preprocessing import preprocess_data\n",
    "\n",
    "from src.models.base.base_model import ParametricModel, DeepLearningModel\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OrnsteinUhlenbeckProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.parametric.block_bootstrap import BlockBootstrap\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "\n",
    "from src.utils.display_utils import show_with_start_divider, show_with_end_divider\n",
    "from src.utils.transformations_utils import create_dataloaders\n",
    "from src.utils.configs_utils import get_dataset_cfgs, get_model_cfgs\n",
    "from src.utils.evaluation_classes_utils import (\n",
    "    TaxonomyEvaluator,\n",
    "    DiversityEvaluator,\n",
    "    FidelityEvaluator,\n",
    "    RuntimeEvaluator,\n",
    "    StylizedFactsEvaluator,\n",
    "    VisualAssessmentEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator class to initialize the MLFlow experiment and evaluate the models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        experiment_name: str,\n",
    "        model_cfgs: Dict[str, Any],\n",
    "        nonparametric_dataset_cfgs: Dict[str, Any],\n",
    "        parametric_dataset_cfgs: Dict[str, Any]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with MLFlow experiment.\n",
    "        \n",
    "        Args:\n",
    "            experiment_name (str): Name of the MLFlow experiment\n",
    "            model_cfgs (Dict[str, Any]): Configuration for models\n",
    "            nonparametric_dataset_cfgs (Dict[str, Any]): Configuration for non-parametric dataset\n",
    "            parametric_dataset_cfgs (Dict[str, Any]): Configuration for parametric dataset\n",
    "        \"\"\"\n",
    "        self.model_cfgs = model_cfgs\n",
    "        self.nonparametric_dataset_cfgs = nonparametric_dataset_cfgs\n",
    "        self.parametric_dataset_cfgs = parametric_dataset_cfgs\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        self.results = {}\n",
    "        self.results_dir = project_root / \"results\"\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        real_data: np.ndarray,\n",
    "        train_data,\n",
    "        num_samples: int = 500,\n",
    "        generation_kwargs: Dict[str, Any] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Unified evaluation for both parametric and non-parametric models.\n",
    "\n",
    "        Args:\n",
    "            model: The type of generative model to evaluate\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison\n",
    "            train_data: Training data\n",
    "            generation_kwargs: Optional kwargs for model.generate() (e.g., linear_timestamps, output_length)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        show_with_start_divider(f\"Evaluating {model_name}\")\n",
    "        generation_kwargs = generation_kwargs or {}\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            mlflow.log_param(\n",
    "                \"model_type\",\n",
    "                \"parametric\" if isinstance(model, ParametricModel) else \"non_parametric\"\n",
    "            )\n",
    "\n",
    "            evaluation_results: Dict[str, Any] = {}\n",
    "\n",
    "            print(f\"Training {model_name}...\")\n",
    "            model.fit(train_data)\n",
    "            print(f\"Training {model_name} completed!\")\n",
    "\n",
    "            print(f\"\\nGenerating {num_samples} samples...\")\n",
    "            runtime_evaluator = RuntimeEvaluator(\n",
    "                generate_func=lambda n, **kwargs: model.generate(n, **generation_kwargs),\n",
    "                num_samples=num_samples,\n",
    "                generation_kwargs=generation_kwargs\n",
    "            )\n",
    "            runtime_results = runtime_evaluator.evaluate()\n",
    "            mlflow.log_metric(\n",
    "                f\"generation_time_{num_samples}_samples\",\n",
    "                runtime_results[f\"generation_time_{num_samples}_samples\"]\n",
    "            )\n",
    "            evaluation_results.update(runtime_results)\n",
    "\n",
    "            generated_data = model.generate(num_samples, **generation_kwargs)\n",
    "\n",
    "            if \"torch\" in str(type(generated_data)):\n",
    "                generated_data = generated_data.detach().cpu().numpy()\n",
    "            if \"torch\" in str(type(real_data)):\n",
    "                real_data = real_data.detach().cpu().numpy()\n",
    "            else:\n",
    "                real_data = np.asarray(real_data)\n",
    "\n",
    "            if real_data.ndim == 2:\n",
    "                l, N = real_data.shape\n",
    "                B = generated_data.shape[1]\n",
    "                if l >= B:\n",
    "                    num_windows = l - B + 1\n",
    "                    real_data_3d = np.lib.stride_tricks.sliding_window_view(real_data, (B, N), axis=(0, 1))\n",
    "                    real_data_3d = real_data_3d.squeeze()\n",
    "                    A_real = min(num_windows, num_samples)\n",
    "                    real_data = real_data_3d[:A_real]\n",
    "                else:\n",
    "                    real_data = real_data[np.newaxis, :, :]\n",
    "\n",
    "            print(f\"Generated data shape: {generated_data.shape}\")\n",
    "            print(f\"Real data shape: {real_data.shape}\")\n",
    "\n",
    "            evaluators = [\n",
    "                FidelityEvaluator(real_data, generated_data),\n",
    "                DiversityEvaluator(real_data, generated_data),\n",
    "                StylizedFactsEvaluator(real_data, generated_data),\n",
    "                VisualAssessmentEvaluator(real_data, generated_data, self.results_dir, self.timestamp)\n",
    "            ]\n",
    "\n",
    "            for evaluator in evaluators:\n",
    "                print(f\"Computing {evaluator.__class__.__name__}...\")\n",
    "                results = evaluator.evaluate(model_name) if isinstance(evaluator, VisualAssessmentEvaluator) else evaluator.evaluate()\n",
    "                \n",
    "                if results:\n",
    "                    evaluation_results.update(results)\n",
    "                    for metric_name, metric_score in results.items():\n",
    "                        if isinstance(metric_score, (int, float)):\n",
    "                            mlflow.log_metric(metric_name, metric_score)\n",
    "                            continue\n",
    "\n",
    "                        if isinstance(metric_score, (np.ndarray, list)):\n",
    "                            channel_scores = np.array(metric_score)\n",
    "                            mlflow.log_metric(f\"{metric_name}_mean\", float(np.mean(channel_scores)))\n",
    "                            mlflow.log_metric(f\"{metric_name}_std\", float(np.std(channel_scores)))\n",
    "\n",
    "            self.results[model_name] = evaluation_results\n",
    "\n",
    "            results_path = self.results_dir / f\"metrics_{model_name}_{self.timestamp}.json\"\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "            mlflow.log_artifact(str(results_path))\n",
    "\n",
    "            print(f\"Evaluation completed for {model_name}!\")\n",
    "\n",
    "    \n",
    "    def run_complete_evaluation(self, num_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run complete evaluation on all models with 500 generated samples per model.\n",
    "        \n",
    "        Args:\n",
    "            dataset_config: Configuration for data preprocessing\n",
    "            models_config: Configuration for models\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing results for all models\n",
    "        \"\"\"\n",
    "        show_with_start_divider(\"Starting Complete Evaluation Pipeline\")\n",
    "        \n",
    "        print(\"  Preprocessing data for non-parametric models...\")\n",
    "        train_data_np, valid_data_np = preprocess_data(self.nonparametric_dataset_cfgs)\n",
    "        train_data_para, valid_data_para = preprocess_data(self.parametric_dataset_cfgs)\n",
    "        \n",
    "        batch_size = 32\n",
    "        train_loader, valid_loader = create_dataloaders(\n",
    "            train_data_np, valid_data_np,\n",
    "            batch_size=batch_size,\n",
    "            train_seed=42,\n",
    "            valid_seed=123,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        num_samples_real, length, num_channels = train_data_np.shape\n",
    "        print(f\"  - Non-parametric data shape: {train_data_np.shape}\")\n",
    "        print(f\"  - Parametric data shape: {train_data_para.shape}\")\n",
    "        \n",
    "        models = {}\n",
    "        \n",
    "        models[\"GBM\"] = GeometricBrownianMotion(length=length, num_channels=num_channels)\n",
    "        models[\"OU_Process\"] = OrnsteinUhlenbeckProcess(length=length, num_channels=num_channels)\n",
    "        models[\"Merton_Jump_Diffusion\"] = MertonJumpDiffusion(length=length, num_channels=num_channels)\n",
    "        models[\"GARCH11\"] = GARCH11(length=length, num_channels=num_channels)\n",
    "        models[\"Double_Exponential_Jump_Diffusion\"] = DoubleExponentialJumpDiffusion(length=length, num_channels=num_channels)\n",
    "        # models[\"BlockBootstrap\"] = BlockBootstrap(length=length, num_channels=num_channels, block_size=125)\n",
    "\n",
    "        models[\"TimeGAN\"] = TimeGAN(\n",
    "            seq_length=length,\n",
    "            num_channels=num_channels,\n",
    "            latent_dim=self.model_cfgs.get(\"latent_dim\", 64),\n",
    "            hidden_dim=self.model_cfgs.get(\"hidden_dim\", 128),\n",
    "            lr=self.model_cfgs.get(\"lr\", 0.0002)\n",
    "        )\n",
    "        \n",
    "        all_results = {}\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                is_parametric = isinstance(model, ParametricModel)\n",
    "                \n",
    "                results = self.evaluate_model(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    real_data=valid_data_para if is_parametric else valid_data_np,\n",
    "                    train_data=train_data_para if is_parametric else train_loader,\n",
    "                    num_samples=num_samples,\n",
    "                    generation_kwargs={'linear_timestamps': True, 'output_length': length, 'seed': 42} if is_parametric else {}\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                all_results[model_name] = {\"error\": str(e)}\n",
    "        \n",
    "        results_file = self.results_dir / f\"complete_evaluation_{self.timestamp}.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(str(results_file))\n",
    "        \n",
    "        show_with_end_divider(\"EVALUATION COMPLETE\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"MLFlow experiment: {self.experiment_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    nonparametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
    "    model_cfgs = get_model_cfgs()\n",
    "    \n",
    "    evaluator = UnifiedEvaluator(\n",
    "        experiment_name=\"TimeSeries_Generation_Comprehensive_Evaluation\",\n",
    "        model_cfgs = model_cfgs,\n",
    "        nonparametric_dataset_cfgs = nonparametric_dataset_cfgs,\n",
    "        parametric_dataset_cfgs = parametric_dataset_cfgs\n",
    "    )\n",
    "    evaluator.run_complete_evaluation(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Starting Complete Evaluation Pipeline\n",
      "  Preprocessing data for non-parametric models...\n",
      "====================\n",
      "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'valid_ratio': 0.2, 'do_normalization': True, 'seed': 42}\n",
      "Data shape: (1131, 125, 5)\n",
      "Preprocessing done.\n",
      "====================\n",
      "\n",
      "====================\n",
      "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'is_parametric': True, 'seed': 42}\n",
      "Data shape: (1255, 5)\n",
      "  - Non-parametric data shape: (904, 125, 5)\n",
      "  - Parametric data shape: torch.Size([1129, 5])\n",
      "====================\n",
      "Evaluating BlockBootstrap\n",
      "Training BlockBootstrap...\n",
      "Training BlockBootstrap completed!\n",
      "\n",
      "Generating 500 samples...\n",
      "Generated data shape: (500, 1129, 5)\n",
      "Real data shape: (1, 126, 5)\n",
      "Computing FidelityEvaluator...\n",
      "Error evaluating BlockBootstrap: list index out of range\n",
      "EVALUATION COMPLETE\n",
      "====================\n",
      "\n",
      "Results saved to: C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\results\\complete_evaluation_20251017_211224.json\n",
      "MLFlow experiment: TimeSeries_Generation_Comprehensive_Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\14165\\AppData\\Local\\Temp\\ipykernel_28712\\2736437663.py\", line 205, in run_complete_evaluation\n",
      "    results = self.evaluate_model(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\14165\\AppData\\Local\\Temp\\ipykernel_28712\\2736437663.py\", line 118, in evaluate_model\n",
      "    results = evaluator.evaluate(model_name) if isinstance(evaluator, VisualAssessmentEvaluator) else evaluator.evaluate()\n",
      "                                                                                                      ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\src\\utils\\evaluation_classes_utils.py\", line 52, in evaluate\n",
      "    self.results = {name: fn(self.ori_data, self.syn_data) for name, fn in fidelity_metrics.items()}\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\src\\taxonomies\\fidelity.py\", line 172, in calculate_mdd\n",
      "    mdd = (HistoLoss(ori_data[:, 1:, :], n_bins=50, name='marginal_distribution')(gen_data[:, 1:, :])).detach().cpu().numpy()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\src\\taxonomies\\fidelity.py\", line 67, in forward\n",
      "    self.loss_componentwise = self.compute(x_fake)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\src\\taxonomies\\fidelity.py\", line 135, in compute\n",
      "    loc = self.locs[i][t].view(1, -1).to(x_fake.device)\n",
      "          ~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
