{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "# Dynamically set the project root based on the script's location\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.preprocessing.preprocessing import preprocess_data\n",
    "\n",
    "from src.models.base.base_model import ParametricModel, DeepLearningModel\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OrnsteinUhlenbeckProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "\n",
    "from src.taxonomies.diversity import calculate_icd\n",
    "from src.taxonomies.efficiency import measure_runtime\n",
    "from src.taxonomies.fidelity import (\n",
    "    calculate_mdd, calculate_md, calculate_sdd, calculate_sd, calculate_kd, calculate_acd, visualize_tsne, visualize_distribution\n",
    ")\n",
    "from src.taxonomies.stylized_facts import (\n",
    "    heavy_tails, autocorr_raw, volatility_clustering, long_memory_abs, non_stationarity\n",
    ")\n",
    "\n",
    "from src.utils.display_utils import show_with_start_divider, show_with_end_divider\n",
    "from src.utils.transformations_utils import create_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnifiedEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator for time series generative models using MLFlow for experiment tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str = \"TimeSeries_Generation_Evaluation\"):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with MLFlow experiment.\n",
    "        \n",
    "        Args:\n",
    "            experiment_name (str): Name of the MLFlow experiment\n",
    "        \"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        self.results_dir = Path(\"results/evaluation_results\") # Updated path\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for this evaluation run\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def evaluate_model(self, \n",
    "                      model, \n",
    "                      model_name: str,\n",
    "                      real_data: np.ndarray,\n",
    "                      train_data,\n",
    "                      num_generated_samples: int = 500,\n",
    "                      generation_kwargs: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Unified evaluation for both parametric and non-parametric models.\n",
    "        \n",
    "        Args:\n",
    "            model: The generative model to evaluate (ParametricModel or DeepLearningModel)\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison (shape: A, B, C)\n",
    "            train_data: Training data (DataLoader for non-parametric, array/tensor for parametric)\n",
    "            num_generated_samples: Number of samples to generate for evaluation\n",
    "            generation_kwargs: Optional kwargs for model.generate() (e.g., linear_timestamps, output_length)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        show_with_start_divider(f\"Evaluating {model_name}\")\n",
    "        \n",
    "        if generation_kwargs is None:\n",
    "            generation_kwargs = {}\n",
    "        \n",
    "        # Start MLFlow run\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            # Log model name and type\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            mlflow.log_param(\"model_type\", \"parametric\" if isinstance(model, ParametricModel) else \"non_parametric\")\n",
    "            \n",
    "            evaluation_results = {}\n",
    "            \n",
    "            # 1. Train the model\n",
    "            print(f\"Training {model_name}...\")\n",
    "            train_start = time.time()\n",
    "            \n",
    "            model.fit(train_data)\n",
    "            train_time = time.time() - train_start\n",
    "            mlflow.log_metric(\"training_time\", train_time)\n",
    "            evaluation_results[\"training_time\"] = train_time\n",
    "            \n",
    "            # 2. Generate synthetic data\n",
    "            print(f\"Generating {num_generated_samples} samples...\")\n",
    "            # Efficiency metrics: Measure generation time\n",
    "            gen_time = measure_runtime(\n",
    "                lambda n: model.generate(n, **generation_kwargs), \n",
    "                num_generated_samples\n",
    "            )\n",
    "            mlflow.log_metric(\"generation_time_500_samples\", gen_time)\n",
    "            evaluation_results[\"generation_time_500_samples\"] = gen_time\n",
    "\n",
    "            # Actually generate the synthetic data\n",
    "            generated_data = model.generate(num_generated_samples, **generation_kwargs)\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if torch.is_tensor(generated_data):\n",
    "                generated_data = generated_data.detach().cpu().numpy()\n",
    "            \n",
    "            # Convert real_data to numpy and ensure 3D shape (A, B, C)\n",
    "            if torch.is_tensor(real_data):\n",
    "                real_data = real_data.detach().cpu().numpy()\n",
    "            else:\n",
    "                real_data = np.asarray(real_data)\n",
    "            \n",
    "            # For parametric models, real_data is (l, N) but generated is (A, B, C)\n",
    "            # We need to convert real_data to (A, B, C) by creating sliding windows\n",
    "            if real_data.ndim == 2:\n",
    "                # Create multiple overlapping samples from the single time series\n",
    "                l, N = real_data.shape\n",
    "                B = generated_data.shape[1]  # Use the same length as generated data\n",
    "                \n",
    "                if l >= B:\n",
    "                    # Create overlapping windows with stride=1\n",
    "                    num_windows = l - B + 1\n",
    "                    real_data_3d = np.lib.stride_tricks.sliding_window_view(real_data, (B, N), axis=(0, 1))\n",
    "                    real_data_3d = real_data_3d.squeeze()\n",
    "                    # Limit to match generated sample count or use all available\n",
    "                    A_real = min(num_windows, num_generated_samples)\n",
    "                    real_data = real_data_3d[:A_real]\n",
    "                else:\n",
    "                    # If real data is shorter, just expand dims to make it (1, l, N)\n",
    "                    real_data = real_data[np.newaxis, :, :]\n",
    "            \n",
    "            print(f\"Generated data shape: {generated_data.shape}\")\n",
    "            print(f\"Real data shape: {real_data.shape}\")\n",
    "            \n",
    "            # 3. Diversity Metrics\n",
    "            print(\"Computing diversity metrics...\")\n",
    "            diversity_results = self._evaluate_diversity(generated_data)\n",
    "            evaluation_results.update(diversity_results)\n",
    "            \n",
    "            # 4. Fidelity Metrics\n",
    "            print(\"Computing fidelity metrics...\")\n",
    "            fidelity_results = self._evaluate_fidelity(real_data, generated_data)\n",
    "            evaluation_results.update(fidelity_results)\n",
    "            \n",
    "            # 5. Stylized Facts (for financial data)\n",
    "            print(\"Computing stylized facts...\")\n",
    "            stylized_results = self._evaluate_stylized_facts(real_data, generated_data)\n",
    "            evaluation_results.update(stylized_results)\n",
    "            \n",
    "            # 6. Visual Assessments\n",
    "            print(\"Creating visual assessments...\")\n",
    "            self._create_visual_assessments(real_data, generated_data, model_name)\n",
    "            \n",
    "            # Log all metrics to MLFlow and save to results dictionary\n",
    "            for metric_name, value in evaluation_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    mlflow.log_metric(f\"{metric_name}_mean\", float(np.mean(value)))\n",
    "                    mlflow.log_metric(f\"{metric_name}_std\", float(np.std(value)))\n",
    "                # Save all metrics to results dictionary\n",
    "                self.results[model_name] = evaluation_results\n",
    "\n",
    "            # Save evaluation results to JSON\n",
    "            results_path = self.results_dir / f\"metrics_{model_name}_{self.timestamp}.json\"\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "            mlflow.log_artifact(str(results_path))\n",
    "            \n",
    "            # Save model (with error handling for models without save_model method)\n",
    "            try:\n",
    "                model_path = self.results_dir / f\"{model_name}_{self.timestamp}\"\n",
    "                model.save_model(str(model_path))\n",
    "                mlflow.log_artifacts(str(model_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save model {model_name}: {e}\")\n",
    "            \n",
    "            # Save synthetic data\n",
    "            synthetic_path = self.results_dir / f\"synthetic_{model_name}_{self.timestamp}.npy\"\n",
    "            np.save(synthetic_path, generated_data)\n",
    "            mlflow.log_artifact(str(synthetic_path))\n",
    "            \n",
    "            print(f\"Evaluation completed for {model_name}\")\n",
    "            return evaluation_results\n",
    "    \n",
    "    def _evaluate_diversity(self, synthetic_data: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate diversity metrics (sequential).\"\"\"\n",
    "        results = {}\n",
    "        results[\"icd_euclidean\"] = calculate_icd(synthetic_data, metric=\"euclidean\")\n",
    "        results[\"icd_dtw\"] = calculate_icd(synthetic_data, metric=\"dtw\")\n",
    "        return results\n",
    "    \n",
    "    def _evaluate_fidelity(self, real_data: np.ndarray, synthetic_data: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate fidelity metrics (sequential).\"\"\"\n",
    "        results = {}\n",
    "        results[\"mdd\"] = calculate_mdd(real_data, synthetic_data)\n",
    "        results[\"md\"] = calculate_md(real_data, synthetic_data)\n",
    "        results[\"sdd\"] = calculate_sdd(real_data, synthetic_data)\n",
    "        results[\"sd\"] = calculate_sd(real_data, synthetic_data)\n",
    "        results[\"kd\"] = calculate_kd(real_data, synthetic_data)\n",
    "        results[\"acd\"] = calculate_acd(real_data, synthetic_data)\n",
    "        return results\n",
    "    \n",
    "    def _evaluate_stylized_facts(self, real_data: np.ndarray, synthetic_data: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate stylized facts for financial data (sequential).\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # Heavy tails (excess kurtosis)\n",
    "            heavy_tails_real = heavy_tails(real_data)\n",
    "            heavy_tails_synth = heavy_tails(synthetic_data)\n",
    "            results[\"heavy_tails_real\"] = heavy_tails_real.tolist()\n",
    "            results[\"heavy_tails_synth\"] = heavy_tails_synth.tolist()\n",
    "            results[\"heavy_tails_diff\"] = np.abs(heavy_tails_real - heavy_tails_synth).tolist()\n",
    "            \n",
    "            # Autocorrelation of raw returns\n",
    "            autocorr_real = autocorr_raw(real_data)\n",
    "            autocorr_synth = autocorr_raw(synthetic_data)\n",
    "            results[\"autocorr_raw_real\"] = autocorr_real.tolist()\n",
    "            results[\"autocorr_raw_synth\"] = autocorr_synth.tolist()\n",
    "            results[\"autocorr_raw_diff\"] = np.abs(autocorr_real - autocorr_synth).tolist()\n",
    "            \n",
    "            # Volatility clustering\n",
    "            vol_clust_real = volatility_clustering(real_data)\n",
    "            vol_clust_synth = volatility_clustering(synthetic_data)\n",
    "            results[\"volatility_clustering_real\"] = vol_clust_real.tolist()\n",
    "            results[\"volatility_clustering_synth\"] = vol_clust_synth.tolist()\n",
    "            results[\"volatility_clustering_diff\"] = np.abs(vol_clust_real - vol_clust_synth).tolist()\n",
    "            \n",
    "            # Long memory in absolute returns\n",
    "            long_mem_real = long_memory_abs(real_data)\n",
    "            long_mem_synth = long_memory_abs(synthetic_data)\n",
    "            results[\"long_memory_real\"] = long_mem_real.tolist()\n",
    "            results[\"long_memory_synth\"] = long_mem_synth.tolist()\n",
    "            results[\"long_memory_diff\"] = np.abs(long_mem_real - long_mem_synth).tolist()\n",
    "            \n",
    "            # Non-stationarity\n",
    "            nonstat_real = non_stationarity(real_data)\n",
    "            nonstat_synth = non_stationarity(synthetic_data)\n",
    "            results[\"non_stationarity_real\"] = nonstat_real.tolist()\n",
    "            results[\"non_stationarity_synth\"] = nonstat_synth.tolist()\n",
    "            results[\"non_stationarity_diff\"] = np.abs(nonstat_real - nonstat_synth).tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Stylized facts evaluation failed: {e}\")\n",
    "            results[\"stylized_facts_error\"] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_visual_assessments(self, real_data: np.ndarray, synthetic_data: np.ndarray, model_name: str):\n",
    "        \"\"\"Create visual assessment plots (sequential).\"\"\"\n",
    "        try:\n",
    "            # Create model-specific results directory\n",
    "            model_results_dir = self.results_dir / f\"visualizations_{model_name}_{self.timestamp}\"\n",
    "            model_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Generate visualizations sequentially\n",
    "            visualize_tsne(real_data, synthetic_data, str(model_results_dir), model_name)\n",
    "            visualize_distribution(real_data, synthetic_data, str(model_results_dir), model_name)\n",
    "            \n",
    "            # Log visualizations to MLFlow\n",
    "            mlflow.log_artifacts(str(model_results_dir))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Visual assessment failed: {e}\")\n",
    "    \n",
    "    def run_complete_evaluation(self, \n",
    "                              nonpara_cfg: Dict[str, Any],\n",
    "                              para_cfg: Dict[str, Any],\n",
    "                              models_config: Dict[str, Any],\n",
    "                              num_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run complete evaluation on all models with 500 generated samples per model.\n",
    "        \n",
    "        Args:\n",
    "            nonpara_cfg: Configuration for non-parametric model data preprocessing\n",
    "            para_cfg: Configuration for parametric model data preprocessing\n",
    "            models_config: Configuration for models\n",
    "            num_samples: Number of samples to generate for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing results for all models\n",
    "        \"\"\"\n",
    "        show_with_start_divider(\"Starting Complete Evaluation Pipeline\")\n",
    "        \n",
    "        # Separate preprocessing for parametric and non-parametric models\n",
    "        print(\"Preprocessing data for non-parametric models...\")\n",
    "        train_data_np, valid_data_np = preprocess_data(nonpara_cfg)\n",
    "        train_data_para, valid_data_para = preprocess_data(para_cfg)\n",
    "        \n",
    "        # Create data loaders for non-parametric models\n",
    "        batch_size = 32\n",
    "        train_loader, valid_loader = create_dataloaders(\n",
    "            train_data_np, valid_data_np,\n",
    "            batch_size=batch_size,\n",
    "            train_seed=42,\n",
    "            valid_seed=123,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Get data dimensions\n",
    "        num_samples_real, length, num_channels = train_data_np.shape\n",
    "        print(f\"Non-parametric data shape: {train_data_np.shape}\")\n",
    "        print(f\"Parametric data shape: {train_data_para.shape}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {}\n",
    "        \n",
    "        # Parametric models\n",
    "        models[\"GBM\"] = GeometricBrownianMotion(length=length, num_channels=num_channels)\n",
    "        models[\"OU_Process\"] = OrnsteinUhlenbeckProcess(length=length, num_channels=num_channels)\n",
    "        models[\"Merton_Jump_Diffusion\"] = MertonJumpDiffusion(length=length, num_channels=num_channels)\n",
    "        models[\"GARCH11\"] = GARCH11(length=length, num_channels=num_channels)\n",
    "        models[\"Double_Exponential_Jump_Diffusion\"] = DoubleExponentialJumpDiffusion(length=length, num_channels=num_channels)\n",
    "\n",
    "        # Non-parametric models\n",
    "        models[\"TimeGAN\"] = TimeGAN(\n",
    "            l=length,\n",
    "            N=num_channels,\n",
    "            latent_dim=models_config.get(\"latent_dim\", 64),\n",
    "            hidden_dim=models_config.get(\"hidden_dim\", 128),\n",
    "            lr=models_config.get(\"lr\", 0.0002)\n",
    "        )\n",
    "        \n",
    "        # Evaluate each model using unified evaluate_model method\n",
    "        all_results = {}\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Determine model type and select appropriate data\n",
    "                is_parametric = isinstance(model, ParametricModel)\n",
    "                \n",
    "                results = self.evaluate_model(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    real_data=valid_data_para if is_parametric else valid_data_np,\n",
    "                    train_data=train_data_para if is_parametric else train_loader,\n",
    "                    num_generated_samples=num_samples,\n",
    "                    generation_kwargs={'linear_timestamps': True, 'output_length': length, 'seed': 42} if is_parametric else {}\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                all_results[model_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Save comprehensive results for all models\n",
    "        results_file = self.results_dir / f\"complete_evaluation_{self.timestamp}.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(str(results_file))\n",
    "        \n",
    "        show_with_end_divider(\"EVALUATION COMPLETE\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"MLFlow experiment: {self.experiment_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    nonpara_ds_cfg = {\n",
    "        'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
    "        'valid_ratio': 0.2,\n",
    "        'do_normalization': True,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    para_ds_cfg = {\n",
    "        'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
    "        'is_parametric': True,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Configuration for models\n",
    "    TimeGAN_cfg = {\n",
    "        'latent_dim': 64,\n",
    "        'hidden_dim': 128,\n",
    "        'lr': 0.0002,\n",
    "        'n_critic': 5,\n",
    "        'clip_value': 0.01\n",
    "    }\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = UnifiedEvaluator(experiment_name=\"TimeSeries_Generation_Comprehensive_Evaluation\")\n",
    "    \n",
    "    # Run complete evaluation\n",
    "    results = evaluator.run_complete_evaluation(\n",
    "        para_cfg=para_ds_cfg,\n",
    "        nonpara_cfg=nonpara_ds_cfg,\n",
    "        models_config=TimeGAN_cfg,\n",
    "        num_samples=500\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    show_with_start_divider(\"EVALUATION SUMMARY\") # Using utility function\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if \"error\" in model_results:\n",
    "            print(f\"  Error: {model_results['error']}\")\n",
    "        else:\n",
    "            print(f\"  Training Time: {model_results.get('training_time', 'N/A'):.2f}s\")\n",
    "            print(f\"  Generation Time (500 samples): {model_results.get('generation_time_500_samples', 'N/A'):.4f}s\")\n",
    "            print(f\"  MDD: {model_results.get('mdd', 'N/A'):.4f}\")\n",
    "            print(f\"  MD: {model_results.get('md', 'N/A'):.4f}\")\n",
    "            print(f\"  SDD: {model_results.get('sdd', 'N/A'):.4f}\")\n",
    "            print(f\"  ICD (Euclidean): {model_results.get('icd_euclidean', 'N/A'):.4f}\")\n",
    "            print(f\"  ICD (DTW): {model_results.get('icd_dtw', 'N/A'):.4f}\")\n",
    "    \n",
    "    show_with_end_divider(\"PIPELINE VALIDATION COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}