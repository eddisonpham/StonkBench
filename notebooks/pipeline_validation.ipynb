{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "import yaml\n",
    "from abc import ABC, abstractmethod\n",
    "import inspect\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.models.base.base_model import ParametricModel, DeepLearningModel\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OUProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.block_bootstrap import BlockBootstrap\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "from src.models.non_parametric.quant_gan import QuantGAN\n",
    "from src.models.non_parametric.time_vae import TimeVAE\n",
    "from src.models.non_parametric.takahashi import TakahashiDiffusion\n",
    "\n",
    "from src.utils.display_utils import show_with_start_divider, show_with_end_divider\n",
    "from src.utils.preprocessing_utils import (\n",
    "    create_dataloaders,\n",
    "    preprocess_data,\n",
    "    LogReturnTransformation,\n",
    "    sliding_window_view,\n",
    ")\n",
    "from src.utils.configs_utils import get_dataset_cfgs\n",
    "from src.utils.evaluation_classes_utils import (\n",
    "    TaxonomyEvaluator,\n",
    "    DiversityEvaluator,\n",
    "    FidelityEvaluator,\n",
    "    RuntimeEvaluator,\n",
    "    StylizedFactsEvaluator,\n",
    "    VisualAssessmentEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator class to initialize the MLFlow experiment and evaluate the models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        experiment_name: str,\n",
    "        parametric_dataset_cfgs: Dict[str, Any],\n",
    "        non_parametric_dataset_cfgs: Dict[str, Any]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with MLFlow experiment.\n",
    "\n",
    "        Args:\n",
    "            experiment_name (str): Name of the MLFlow experiment\n",
    "            parametric_dataset_cfgs (Dict[str, Any]): Configuration for parametric dataset\n",
    "        \"\"\"\n",
    "        self.parametric_dataset_cfgs = parametric_dataset_cfgs\n",
    "        self.non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        self.results = {}\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.results_dir = project_root / \"results\" / f\"evaluation_{self.timestamp}\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        real_data: np.ndarray,\n",
    "        train_data,\n",
    "        generation_kwargs: Dict[str, Any],\n",
    "        fit_kwargs: Dict[str, Any] = None,\n",
    "        seed: int = 42\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Unified evaluation for both parametric.\n",
    "\n",
    "        Args:\n",
    "            model: The type of generative model to evaluate\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison\n",
    "            train_data: Training data\n",
    "            generation_kwargs: Optional kwargs for model.generate()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        show_with_start_divider(f\"Evaluating {model_name}\")\n",
    "        num_samples = generation_kwargs.get('num_samples', 500)\n",
    "        if fit_kwargs is not None:\n",
    "            num_epochs = fit_kwargs.get('num_epochs', 1)\n",
    "            \n",
    "        model_dir = self.results_dir / model_name\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "            evaluation_results: Dict[str, Any] = {}\n",
    "\n",
    "            print(f\"Training {model_name}...\")\n",
    "            if isinstance(model, DeepLearningModel):\n",
    "                model.fit(train_data, num_epochs=num_epochs)\n",
    "            else:\n",
    "                model.fit(train_data)\n",
    "\n",
    "            print(f\"\\nGenerating {num_samples} samples...\")\n",
    "            runtime_evaluator = RuntimeEvaluator(\n",
    "                generate_func=model.generate,\n",
    "                generation_kwargs=generation_kwargs\n",
    "            )\n",
    "            runtime_results = runtime_evaluator.evaluate()\n",
    "            mlflow.log_metric(\n",
    "                f\"generation_time_{num_samples}_samples\",\n",
    "                runtime_results[f\"generation_time_{num_samples}_samples\"]\n",
    "            )\n",
    "            evaluation_results.update(runtime_results)\n",
    "\n",
    "            generated_data = model.generate(**generation_kwargs)\n",
    "\n",
    "            if \"torch\" in str(type(generated_data)):\n",
    "                generated_data = generated_data.numpy()\n",
    "            if \"torch\" in str(type(real_data)):\n",
    "                real_data = real_data.numpy()\n",
    "            else:\n",
    "                real_data = np.asarray(real_data)\n",
    "\n",
    "            if real_data.ndim == 1:\n",
    "                window_size = generation_kwargs.get('generation_length', 1)\n",
    "                real_data = sliding_window_view(real_data, window_size, 1)\n",
    "            idx = np.random.permutation(real_data.shape[0])[:num_samples]\n",
    "            real_data = real_data[idx]\n",
    "\n",
    "            print(f\"Generated data shape: {generated_data.shape}\")\n",
    "            print(f\"Real data shape: {real_data.shape}\")\n",
    "\n",
    "            evaluators = [\n",
    "                FidelityEvaluator(real_data, generated_data),\n",
    "                DiversityEvaluator(real_data, generated_data),\n",
    "                StylizedFactsEvaluator(real_data, generated_data),\n",
    "                VisualAssessmentEvaluator(real_data, generated_data, model_dir)\n",
    "            ]\n",
    "\n",
    "            for evaluator in evaluators:\n",
    "                print(f\"Computing {evaluator.__class__.__name__}...\")\n",
    "                results = evaluator.evaluate()\n",
    "                if results is not None:\n",
    "                    evaluation_results.update(results)\n",
    "\n",
    "            metrics_path = model_dir / \"metrics.json\"\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "            mlflow.log_artifact(str(metrics_path))\n",
    "            print(f\"Evaluation completed for {model_name} (results saved at {metrics_path}).\")\n",
    "\n",
    "            return evaluation_results\n",
    "\n",
    "    def run_complete_evaluation(self, num_samples: int = 500, seed: int = 42) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run complete evaluation on all models with 500 generated samples per model.\n",
    "\n",
    "        Args:\n",
    "            dataset_config: Configuration for data preprocessing\n",
    "            models_config: Configuration for models\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing results for all models\n",
    "        \"\"\"\n",
    "        show_with_start_divider(\"Starting Complete Evaluation Pipeline\")\n",
    "        train_data_para, valid_data_para, test_data_para = preprocess_data(self.parametric_dataset_cfgs)\n",
    "\n",
    "        length_para = train_data_para.shape[0]\n",
    "        print(f\"  - Parametric train data shape: {train_data_para.shape}\")\n",
    "        print(f\"  - Parametric valid data shape: {valid_data_para.shape}\")\n",
    "        print(f\"  - Parametric test data shape: {test_data_para.shape}\")\n",
    "\n",
    "        train_data_non_para, valid_data_non_para, test_data_non_para = preprocess_data(self.non_parametric_dataset_cfgs)\n",
    "        train_loader_non_para, valid_loader_non_para, test_loader_non_para = create_dataloaders(\n",
    "            train_data_non_para, valid_data_non_para, test_data_non_para, batch_size=32, train_seed=42, valid_seed=42, test_seed=42)\n",
    "        \n",
    "\n",
    "        num_timeseries, generation_length = train_data_non_para.shape\n",
    "        print(f\"  - Non-parametric train data shape: {train_data_non_para.shape}\")\n",
    "        print(f\"  - Non-parametric valid data shape: {valid_data_non_para.shape}\")\n",
    "        print(f\"  - Non-parametric test data shape: {test_data_non_para.shape}\")\n",
    "\n",
    "        parametric_models = {}\n",
    "        parametric_models[\"GBM\"] = GeometricBrownianMotion()\n",
    "        parametric_models[\"OU Process\"] = OUProcess()\n",
    "        parametric_models[\"MJD\"] = MertonJumpDiffusion()\n",
    "        parametric_models[\"GARCH11\"] = GARCH11()\n",
    "        parametric_models[\"DEJD\"] = DoubleExponentialJumpDiffusion()\n",
    "        parametric_models[\"BlockBootstrap\"] = BlockBootstrap(block_size=generation_length)\n",
    "\n",
    "        non_parametric_models = {}\n",
    "        non_parametric_models[\"TimeGAN\"] = TimeGAN(seq_len=generation_length, hidden_dim=24, num_layers=3, learning_rate=1e-5)\n",
    "        non_parametric_models[\"QuantGAN\"] = QuantGAN()\n",
    "        non_parametric_models[\"TimeVAE\"] = TimeVAE(\n",
    "            length=None,\n",
    "            num_channels=1,\n",
    "            latent_dim=10,\n",
    "            hidden_layer_sizes=[100, 200, 400],\n",
    "            trend_poly=0,\n",
    "            custom_seas=None,\n",
    "            use_residual_conn=True,\n",
    "            reconstruction_wt=3.0,\n",
    "            lr=1e-5\n",
    "        )\n",
    "        non_parametric_models[\"Takahashi DDPM\"] = TakahashiDiffusion(\n",
    "            length=None,\n",
    "            num_channels=1,\n",
    "            num_steps=100,\n",
    "            beta_start=0.0001,\n",
    "            beta_end=0.02,\n",
    "            wavelet='haar',\n",
    "            lr=1e-5\n",
    "        )\n",
    "\n",
    "        all_results = {}\n",
    "\n",
    "        generation_kwargs_para = {'num_samples': num_samples, 'generation_length': generation_length}\n",
    "        for model_name, model in parametric_models.items():\n",
    "            results = self.evaluate_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                real_data=valid_data_para,\n",
    "                train_data=train_data_para,\n",
    "                generation_kwargs=generation_kwargs_para,\n",
    "                seed=seed\n",
    "            )\n",
    "            all_results[model_name] = results\n",
    "\n",
    "        # Evaluate non-parametric models on non-parametric dataset (use DataLoader and num_epochs)\n",
    "        generation_kwargs_non_para = {'num_samples': num_samples, 'generation_length': generation_length}\n",
    "        fit_kwargs_non_para = {'num_epochs': 10}\n",
    "        for model_name, model in non_parametric_models.items():\n",
    "            results = self.evaluate_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                real_data=valid_data_non_para,\n",
    "                train_data=train_loader_non_para,\n",
    "                generation_kwargs=generation_kwargs_non_para,\n",
    "                fit_kwargs=fit_kwargs_non_para,\n",
    "                seed=seed\n",
    "            )\n",
    "            all_results[model_name] = results\n",
    "\n",
    "        results_file = self.results_dir / \"complete_evaluation.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(all_results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(str(results_file))\n",
    "\n",
    "        show_with_end_divider(\"EVALUATION COMPLETE\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"MLFlow experiment: {self.experiment_name}\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
    "    evaluator = UnifiedEvaluator(\n",
    "        experiment_name=\"TimeSeries_Generation_Comprehensive_Evaluation\",\n",
    "        parametric_dataset_cfgs = parametric_dataset_cfgs,\n",
    "    non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n",
    "    )\n",
    "    evaluator.run_complete_evaluation(num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Starting Complete Evaluation Pipeline\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "  - Parametric train data shape: torch.Size([9056])\n",
      "  - Parametric valid data shape: torch.Size([1132])\n",
      "  - Parametric test data shape: torch.Size([1133])\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "Desired time series sample length (lag with max PACF >0): 103\n",
      "PACF at that lag: 0.040741497942971425\n",
      "  - Non-parametric train data shape: (8975, 103)\n",
      "  - Non-parametric valid data shape: (1122, 103)\n",
      "  - Non-parametric test data shape: (1122, 103)\n",
      "====================\n",
      "Evaluating GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The compiled dtaidistance C library is not available.\n",
      "See the documentation for alternative installation options.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GBM...\n",
      "mu: 0.0005937263937975437, sigma: 0.0299900515594167\n",
      "\n",
      "Generating 1000 samples...\n",
      "Generated data shape: (1000, 103)\n",
      "Real data shape: (1000, 103)\n",
      "Computing FidelityEvaluator...\n",
      "Computing DiversityEvaluator...\n"
     ]
    },
    {
     "ename": "CythonException",
     "evalue": "The compiled dtaidistance C library is not available.\nSee the documentation for alternative installation options.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 232\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    226\u001b[39m non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n\u001b[32m    227\u001b[39m evaluator = UnifiedEvaluator(\n\u001b[32m    228\u001b[39m     experiment_name=\u001b[33m\"\u001b[39m\u001b[33mTimeSeries_Generation_Comprehensive_Evaluation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    229\u001b[39m     parametric_dataset_cfgs = parametric_dataset_cfgs,\n\u001b[32m    230\u001b[39m non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n\u001b[32m    231\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mUnifiedEvaluator.run_complete_evaluation\u001b[39m\u001b[34m(self, num_samples, seed)\u001b[39m\n\u001b[32m    186\u001b[39m generation_kwargs_para = {\u001b[33m'\u001b[39m\u001b[33mnum_samples\u001b[39m\u001b[33m'\u001b[39m: num_samples, \u001b[33m'\u001b[39m\u001b[33mgeneration_length\u001b[39m\u001b[33m'\u001b[39m: generation_length}\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m parametric_models.items():\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_data_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_kwargs_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     all_results[model_name] = results\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Evaluate non-parametric models on non-parametric dataset (use DataLoader and num_epochs)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mUnifiedEvaluator.evaluate_model\u001b[39m\u001b[34m(self, model, model_name, real_data, train_data, generation_kwargs, fit_kwargs, seed)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m evaluator \u001b[38;5;129;01min\u001b[39;00m evaluators:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    112\u001b[39m         evaluation_results.update(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/utils/evaluation_classes_utils.py:39\u001b[39m, in \u001b[36mDiversityEvaluator.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, np.ndarray]:\n\u001b[32m     38\u001b[39m     metrics = [\u001b[33m\"\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdtw\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = {\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33micd_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: \u001b[43mcalculate_icd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msyn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics}\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/taxonomies/diversity.py:50\u001b[39m, in \u001b[36mcalculate_icd\u001b[39m\u001b[34m(comp_data, metric)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_icd_euclidean(comp_data)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_icd_dtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomp_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/taxonomies/diversity.py:34\u001b[39m, in \u001b[36mcompute_icd_dtw\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     32\u001b[39m n_samples, _ = data.shape\n\u001b[32m     33\u001b[39m sequences = [data[i, :].astype(np.double) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples)]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m dist_matrix = \u001b[43mdtw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistance_matrix_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompact\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m upper_sum = np.sum(dist_matrix)\n\u001b[32m     36\u001b[39m icd = upper_sum / (n_samples * (n_samples - \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/dtaidistance/dtw.py:924\u001b[39m, in \u001b[36mdistance_matrix_fast\u001b[39m\u001b[34m(s, max_dist, use_pruning, max_length_diff, window, max_step, penalty, psi, block, compact, parallel, use_mp, only_triu, inner_dist, use_c)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistance_matrix_fast\u001b[39m(s, max_dist=\u001b[38;5;28;01mNone\u001b[39;00m, use_pruning=\u001b[38;5;28;01mFalse\u001b[39;00m, max_length_diff=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    914\u001b[39m                          window=\u001b[38;5;28;01mNone\u001b[39;00m, max_step=\u001b[38;5;28;01mNone\u001b[39;00m, penalty=\u001b[38;5;28;01mNone\u001b[39;00m, psi=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    915\u001b[39m                          block=\u001b[38;5;28;01mNone\u001b[39;00m, compact=\u001b[38;5;28;01mFalse\u001b[39;00m, parallel=\u001b[38;5;28;01mTrue\u001b[39;00m, use_mp=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    916\u001b[39m                          only_triu=\u001b[38;5;28;01mFalse\u001b[39;00m, inner_dist=innerdistance.default, use_c=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    917\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Same as :meth:`distance_matrix` but with different defaults to choose the\u001b[39;00m\n\u001b[32m    918\u001b[39m \u001b[33;03m    fast parallized C version (use_c = True and parallel = True).\u001b[39;00m\n\u001b[32m    919\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m \u001b[33;03m    the parallelization is changed to use Python's multiprocessing library.\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     \u001b[43m_check_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_omp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_mp \u001b[38;5;129;01mand\u001b[39;00m parallel:\n\u001b[32m    926\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/dtaidistance/dtw.py:89\u001b[39m, in \u001b[36m_check_library\u001b[39m\u001b[34m(include_omp, raise_exception)\u001b[39m\n\u001b[32m     87\u001b[39m     logger.error(msg)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CythonException(msg)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_omp \u001b[38;5;129;01mand\u001b[39;00m (dtw_cc_omp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtw_cc_omp.is_openmp_supported()):\n\u001b[32m     91\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe compiled dtaidistance C-OMP library \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mCythonException\u001b[39m: The compiled dtaidistance C library is not available.\nSee the documentation for alternative installation options."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
