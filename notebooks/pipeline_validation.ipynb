{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Dynamically set the project root based on the script's location\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.preprocessing.preprocessing import preprocess_data\n",
    "\n",
    "from src.models.base.base_model import ParametricModel, DeepLearningModel\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OrnsteinUhlenbeckProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "\n",
    "from src.taxonomies.diversity import calculate_icd\n",
    "from src.taxonomies.efficiency import measure_runtime\n",
    "from src.taxonomies.fidelity import (\n",
    "    calculate_mdd, calculate_md, calculate_sdd, calculate_sd, calculate_kd, calculate_acd, visualize_tsne, visualize_distribution\n",
    ")\n",
    "from src.taxonomies.stylized_facts import (\n",
    "    heavy_tails, autocorr_raw, volatility_clustering, long_memory_abs, non_stationarity\n",
    ")\n",
    "\n",
    "from src.utils.display_utils import show_with_start_divider, show_with_end_divider\n",
    "from src.utils.transformations_utils import create_dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnifiedEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator for time series generative models using MLFlow for experiment tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str = \"TimeSeries_Generation_Evaluation\"):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with MLFlow experiment.\n",
    "        \n",
    "        Args:\n",
    "            experiment_name (str): Name of the MLFlow experiment\n",
    "        \"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        self.results_dir = Path(\"results/evaluation_results\") # Updated path\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for this evaluation run\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    def evaluate_parametric(self, \n",
    "                      model: ParametricModel, \n",
    "                      model_name: str,\n",
    "                      real_data: np.ndarray,\n",
    "                      train_data,\n",
    "                      num_generated_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a single parametric model across all metrics.\n",
    "        Args:\n",
    "            model: The parametric generative model to evaluate\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison (shape: R, L, N)\n",
    "            train_loader: Not used for parametric models\n",
    "            num_generated_samples: Number of samples to generate for evaluation\n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            # Log model name only\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            \n",
    "            evaluation_results = {}\n",
    "            \n",
    "            print(f\"Training {model_name}...\")\n",
    "            train_start = time.time()\n",
    "            model.fit(train_data)\n",
    "            train_time = time.time() - train_start\n",
    "            mlflow.log_metric(\"training_time\", train_time)\n",
    "            evaluation_results[\"training_time\"] = train_time\n",
    "            \n",
    "            print(f\"Generating {num_generated_samples} samples...\")\n",
    "            gen_time = measure_runtime(model.generate, num_generated_samples)\n",
    "            mlflow.log_metric(\"generation_time_500_samples\", gen_time)\n",
    "            evaluation_results[\"generation_time_500_samples\"] = gen_time\n",
    "\n",
    "            generated_data = model.generate(num_generated_samples, linear_timestamps=True, output_length=125, seed=42)\n",
    "            print(f\"Generated data shape: {generated_data.shape}\")\n",
    "            \n",
    "            # 3. Diversity Metrics\n",
    "            print(\"Computing diversity metrics...\")\n",
    "            diversity_results = self._evaluate_diversity(generated_data)\n",
    "            evaluation_results.update(diversity_results)\n",
    "            \n",
    "            # 4. Fidelity Metrics\n",
    "            print(\"Computing fidelity metrics...\")\n",
    "            fidelity_results = self._evaluate_fidelity(real_data, generated_data)\n",
    "            evaluation_results.update(fidelity_results)\n",
    "            \n",
    "            # 5. Stylized Facts (for financial data)\n",
    "            print(\"Computing stylized facts...\")\n",
    "            stylized_results = self._evaluate_stylized_facts(real_data, generated_data)\n",
    "            evaluation_results.update(stylized_results)\n",
    "            \n",
    "            # 6. Visual Assessments\n",
    "            print(\"Creating visual assessments...\")\n",
    "            self._create_visual_assessments(real_data, generated_data, model_name)\n",
    "            \n",
    "            # Log all metrics to MLFlow and save to results dictionary\n",
    "            for metric_name, value in evaluation_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    mlflow.log_metric(f\"{metric_name}_mean\", float(np.mean(value)))\n",
    "                    mlflow.log_metric(f\"{metric_name}_std\", float(np.std(value)))\n",
    "                # Save all metrics to results dictionary\n",
    "                self.results[model_name] = evaluation_results\n",
    "\n",
    "            # Save evaluation results to JSON\n",
    "            results_path = self.results_dir / f\"metrics_{model_name}_{self.timestamp}.json\"\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "            mlflow.log_artifact(str(results_path))\n",
    "            \n",
    "            # Save model\n",
    "            model_path = self.results_dir / f\"{model_name}_{self.timestamp}\"\n",
    "            model.save_model(str(model_path))\n",
    "            mlflow.log_artifacts(str(model_path))\n",
    "            \n",
    "            # Save synthetic data\n",
    "            synthetic_path = self.results_dir / f\"synthetic_{model_name}_{self.timestamp}.npy\"\n",
    "            np.save(synthetic_path, generated_data)\n",
    "            mlflow.log_artifact(str(synthetic_path))\n",
    "            \n",
    "            print(f\"Evaluation completed for {model_name}\")\n",
    "            return evaluation_results\n",
    "            \n",
    "    def evaluate_nonparametric(self, \n",
    "                      model: DeepLearningModel, \n",
    "                      model_name: str,\n",
    "                      real_data: np.ndarray,\n",
    "                      train_loader,\n",
    "                      num_generated_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a single model across all metrics.\n",
    "        \n",
    "        Args:\n",
    "            model: The generative model to evaluate\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison (shape: R, L, N)\n",
    "            train_loader: Training data loader for model fitting\n",
    "            num_generated_samples: Number of samples to generate for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        show_with_start_divider(f\"Evaluating {model_name}\")\n",
    "        \n",
    "        # Start MLFlow run\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            # Log model name only\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            \n",
    "            evaluation_results = {}\n",
    "            \n",
    "            # 1. Train the model\n",
    "            print(f\"Training {model_name}...\")\n",
    "            train_start = time.time()\n",
    "            model.fit(train_loader)\n",
    "            train_time = time.time() - train_start\n",
    "            mlflow.log_metric(\"training_time\", train_time)\n",
    "            evaluation_results[\"training_time\"] = train_time\n",
    "            \n",
    "            # 2. Generate synthetic data\n",
    "            print(f\"Generating {num_generated_samples} samples...\")\n",
    "            # Efficiency metrics: Measure generation time\n",
    "            gen_time = measure_runtime(model.generate, num_generated_samples)\n",
    "            mlflow.log_metric(\"generation_time_500_samples\", gen_time)\n",
    "            evaluation_results[\"generation_time_500_samples\"] = gen_time\n",
    "\n",
    "            # Actually generate the synthetic data\n",
    "            generated_data = model.generate(num_generated_samples)\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if torch.is_tensor(generated_data):\n",
    "                generated_data = generated_data.detach().cpu().numpy()\n",
    "            \n",
    "            # 3. Diversity Metrics\n",
    "            print(\"Computing diversity metrics...\")\n",
    "            diversity_results = self._evaluate_diversity(generated_data)\n",
    "            evaluation_results.update(diversity_results)\n",
    "            \n",
    "            # 4. Fidelity Metrics\n",
    "            print(\"Computing fidelity metrics...\")\n",
    "            fidelity_results = self._evaluate_fidelity(real_data, generated_data)\n",
    "            evaluation_results.update(fidelity_results)\n",
    "            \n",
    "            # 5. Stylized Facts (for financial data)\n",
    "            print(\"Computing stylized facts...\")\n",
    "            stylized_results = self._evaluate_stylized_facts(real_data, generated_data)\n",
    "            evaluation_results.update(stylized_results)\n",
    "            \n",
    "            # 6. Visual Assessments\n",
    "            print(\"Creating visual assessments...\")\n",
    "            self._create_visual_assessments(real_data, generated_data, model_name)\n",
    "            \n",
    "            # Log all metrics to MLFlow and save to results dictionary\n",
    "            for metric_name, value in evaluation_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    mlflow.log_metric(f\"{metric_name}_mean\", float(np.mean(value)))\n",
    "                    mlflow.log_metric(f\"{metric_name}_std\", float(np.std(value)))\n",
    "                # Save all metrics to results dictionary\n",
    "                self.results[model_name] = evaluation_results\n",
    "\n",
    "            # Save evaluation results to JSON\n",
    "            results_path = self.results_dir / f\"metrics_{model_name}_{self.timestamp}.json\"\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "            mlflow.log_artifact(str(results_path))\n",
    "            \n",
    "            # Save model\n",
    "            model_path = self.results_dir / f\"{model_name}_{self.timestamp}\"\n",
    "            model.save_model(str(model_path))\n",
    "            mlflow.log_artifacts(str(model_path))\n",
    "            \n",
    "            # Save synthetic data\n",
    "            synthetic_path = self.results_dir / f\"synthetic_{model_name}_{self.timestamp}.npy\"\n",
    "            np.save(synthetic_path, generated_data)\n",
    "            mlflow.log_artifact(str(synthetic_path))\n",
    "            \n",
    "            print(f\"Evaluation completed for {model_name}\")\n",
    "            return evaluation_results\n",
    "    \n",
    "    def _evaluate_diversity(self, synthetic_data: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate diversity metrics.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Intra-Class Distance with Euclidean metric\n",
    "        icd_euclidean = calculate_icd(synthetic_data, metric=\"euclidean\")\n",
    "        results[\"icd_euclidean\"] = icd_euclidean\n",
    "        \n",
    "        # Intra-Class Distance with DTW metric\n",
    "        icd_dtw = calculate_icd(synthetic_data, metric=\"dtw\")\n",
    "        results[\"icd_dtw\"] = icd_dtw\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_fidelity(self, real_data: np.ndarray, synthetic_data: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate fidelity metrics.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Feature-based metrics\n",
    "        results[\"mdd\"] = calculate_mdd(real_data, synthetic_data)\n",
    "        results[\"md\"] = calculate_md(real_data, synthetic_data)\n",
    "        results[\"sdd\"] = calculate_sdd(real_data, synthetic_data)\n",
    "        results[\"sd\"] = calculate_sd(real_data, synthetic_data)\n",
    "        results[\"kd\"] = calculate_kd(real_data, synthetic_data)\n",
    "        results[\"acd\"] = calculate_acd(real_data, synthetic_data)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_stylized_facts(self, real_data: np.ndarray, synthetic_data: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate stylized facts for financial data.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # Heavy tails (excess kurtosis)\n",
    "            heavy_tails_real = heavy_tails(real_data)\n",
    "            heavy_tails_synth = heavy_tails(synthetic_data)\n",
    "            results[\"heavy_tails_real\"] = heavy_tails_real.tolist()\n",
    "            results[\"heavy_tails_synth\"] = heavy_tails_synth.tolist()\n",
    "            results[\"heavy_tails_diff\"] = np.abs(heavy_tails_real - heavy_tails_synth).tolist()\n",
    "            \n",
    "            # Autocorrelation of raw returns\n",
    "            autocorr_real = autocorr_raw(real_data)\n",
    "            autocorr_synth = autocorr_raw(synthetic_data)\n",
    "            results[\"autocorr_raw_real\"] = autocorr_real.tolist()\n",
    "            results[\"autocorr_raw_synth\"] = autocorr_synth.tolist()\n",
    "            results[\"autocorr_raw_diff\"] = np.abs(autocorr_real - autocorr_synth).tolist()\n",
    "            \n",
    "            # Volatility clustering\n",
    "            vol_clust_real = volatility_clustering(real_data)\n",
    "            vol_clust_synth = volatility_clustering(synthetic_data)\n",
    "            results[\"volatility_clustering_real\"] = vol_clust_real.tolist()\n",
    "            results[\"volatility_clustering_synth\"] = vol_clust_synth.tolist()\n",
    "            results[\"volatility_clustering_diff\"] = np.abs(vol_clust_real - vol_clust_synth).tolist()\n",
    "            \n",
    "            # Long memory in absolute returns\n",
    "            long_mem_real = long_memory_abs(real_data)\n",
    "            long_mem_synth = long_memory_abs(synthetic_data)\n",
    "            results[\"long_memory_real\"] = long_mem_real.tolist()\n",
    "            results[\"long_memory_synth\"] = long_mem_synth.tolist()\n",
    "            results[\"long_memory_diff\"] = np.abs(long_mem_real - long_mem_synth).tolist()\n",
    "            \n",
    "            # Non-stationarity\n",
    "            nonstat_real = non_stationarity(real_data)\n",
    "            nonstat_synth = non_stationarity(synthetic_data)\n",
    "            results[\"non_stationarity_real\"] = nonstat_real.tolist()\n",
    "            results[\"non_stationarity_synth\"] = nonstat_synth.tolist()\n",
    "            results[\"non_stationarity_diff\"] = np.abs(nonstat_real - nonstat_synth).tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Stylized facts evaluation failed: {e}\")\n",
    "            results[\"stylized_facts_error\"] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_visual_assessments(self, real_data: np.ndarray, synthetic_data: np.ndarray, model_name: str):\n",
    "        \"\"\"Create visual assessment plots.\"\"\"\n",
    "        try:\n",
    "            # Create model-specific results directory\n",
    "            model_results_dir = self.results_dir / f\"visualizations_{model_name}_{self.timestamp}\"\n",
    "            model_results_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # t-SNE visualization\n",
    "            visualize_tsne(real_data, synthetic_data, str(model_results_dir), model_name)\n",
    "            \n",
    "            # Distribution visualization\n",
    "            visualize_distribution(real_data, synthetic_data, str(model_results_dir), model_name)\n",
    "            \n",
    "            # Log visualizations to MLFlow\n",
    "            mlflow.log_artifacts(str(model_results_dir))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Visual assessment failed: {e}\")\n",
    "    \n",
    "    def run_complete_evaluation(self, \n",
    "                              nonpara_cfg: Dict[str, Any],\n",
    "                              para_cfg: Dict[str, Any],\n",
    "                              models_config: Dict[str, Any],\n",
    "                              num_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run complete evaluation on all models with 500 generated samples per model.\n",
    "        \n",
    "        Args:\n",
    "            dataset_config: Configuration for data preprocessing\n",
    "            models_config: Configuration for models\n",
    "            num_samples: Number of samples to generate for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing results for all models\n",
    "        \"\"\"\n",
    "        show_with_start_divider(\"Starting Complete Evaluation Pipeline\")\n",
    "        \n",
    "        # Separate preprocessing for parametric and non-parametric models\n",
    "        print(\"Preprocessing data for non-parametric models...\")\n",
    "        train_data_np, valid_data_np = preprocess_data(nonpara_cfg)\n",
    "        train_data_para, valid_data_para = preprocess_data(para_cfg)\n",
    "        \n",
    "        # Create data loaders for non-parametric models\n",
    "        batch_size = 32\n",
    "        train_loader, valid_loader = create_dataloaders(\n",
    "            train_data_np, valid_data_np,\n",
    "            batch_size=batch_size,\n",
    "            train_seed=42,\n",
    "            valid_seed=123,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Get data dimensions\n",
    "        num_samples_real, length, num_channels = train_data_np.shape\n",
    "        print(f\"Non-parametric data shape: {train_data_np.shape}\")\n",
    "        print(f\"Parametric data shape: {train_data_para.shape}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {}\n",
    "        \n",
    "        # Parametric models\n",
    "        models[\"GBM\"] = GeometricBrownianMotion(length=length, num_channels=num_channels)\n",
    "        models[\"OU_Process\"] = OrnsteinUhlenbeckProcess(length=length, num_channels=num_channels)\n",
    "        models[\"Merton_Jump_Diffusion\"] = MertonJumpDiffusion(length=length, num_channels=num_channels)\n",
    "        models[\"GARCH11\"] = GARCH11(length=length, num_channels=num_channels)\n",
    "        models[\"Double_Exponential_Jump_Diffusion\"] = DoubleExponentialJumpDiffusion(length=length, num_channels=num_channels)\n",
    "\n",
    "        # Non-parametric models\n",
    "        models[\"TimeGAN\"] = TimeGAN(\n",
    "            l=length,\n",
    "            N=num_channels,\n",
    "            latent_dim=models_config.get(\"latent_dim\", 64),\n",
    "            hidden_dim=models_config.get(\"hidden_dim\", 128),\n",
    "            lr=models_config.get(\"lr\", 0.0002)\n",
    "        )\n",
    "        \n",
    "        # Evaluate each model\n",
    "        all_results = {}\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                if isinstance(model, ParametricModel):\n",
    "                    results = self.evaluate_parametric(\n",
    "                        model=model,\n",
    "                        model_name=model_name,\n",
    "                        real_data=valid_data_para,  # Parametric data\n",
    "                        train_data=train_data_para,  # Training data for parametric models\n",
    "                        num_generated_samples=num_samples\n",
    "                    )\n",
    "                else:\n",
    "                    results = self.evaluate_nonparametric(\n",
    "                        model=model,\n",
    "                        model_name=model_name,\n",
    "                        real_data=valid_data_np,  # Non-parametric data\n",
    "                        train_loader=train_loader,  # DataLoader for non-parametric models\n",
    "                        num_generated_samples=num_samples\n",
    "                    )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                all_results[model_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Save comprehensive results for all models\n",
    "        results_file = self.results_dir / f\"complete_evaluation_{self.timestamp}.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(str(results_file))\n",
    "        \n",
    "        show_with_end_divider(\"EVALUATION COMPLETE\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"MLFlow experiment: {self.experiment_name}\")\n",
    "        \n",
    "        return all_results\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    nonpara_ds_cfg = {\n",
    "        'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
    "        'valid_ratio': 0.2,\n",
    "        'do_normalization': True,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    para_ds_cfg = {\n",
    "        'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
    "        'is_parametric': True,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Configuration for models\n",
    "    TimeGAN_cfg = {\n",
    "        'latent_dim': 64,\n",
    "        'hidden_dim': 128,\n",
    "        'lr': 0.0002,\n",
    "        'n_critic': 5,\n",
    "        'clip_value': 0.01\n",
    "    }\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = UnifiedEvaluator(experiment_name=\"TimeSeries_Generation_Comprehensive_Evaluation\")\n",
    "    \n",
    "    # Run complete evaluation\n",
    "    results = evaluator.run_complete_evaluation(\n",
    "        para_cfg=para_ds_cfg,\n",
    "        nonpara_cfg=nonpara_ds_cfg,\n",
    "        models_config=TimeGAN_cfg,\n",
    "        num_samples=500\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    show_with_start_divider(\"EVALUATION SUMMARY\") # Using utility function\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if \"error\" in model_results:\n",
    "            print(f\"  Error: {model_results['error']}\")\n",
    "        else:\n",
    "            print(f\"  Training Time: {model_results.get('training_time', 'N/A'):.2f}s\")\n",
    "            print(f\"  Generation Time (500 samples): {model_results.get('generation_time_500_samples', 'N/A'):.4f}s\")\n",
    "            print(f\"  MDD: {model_results.get('mdd', 'N/A'):.4f}\")\n",
    "            print(f\"  MD: {model_results.get('md', 'N/A'):.4f}\")\n",
    "            print(f\"  SDD: {model_results.get('sdd', 'N/A'):.4f}\")\n",
    "            print(f\"  ICD (Euclidean): {model_results.get('icd_euclidean', 'N/A'):.4f}\")\n",
    "            print(f\"  ICD (DTW): {model_results.get('icd_dtw', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Starting Complete Evaluation Pipeline\n",
      "Preprocessing data for non-parametric models...\n",
      "====================\n",
      "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\SWE_Work\\\\Unified-benchmark-for-SDGFTS\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'valid_ratio': 0.2, 'do_normalization': True, 'seed': 42}\n",
      "Data shape: (1131, 125, 5)\n",
      "Preprocessing done.\n",
      "====================\n",
      "\n",
      "====================\n",
      "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\SWE_Work\\\\Unified-benchmark-for-SDGFTS\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'is_parametric': True, 'seed': 42}\n",
      "Data shape: (1255, 5)\n",
      "Non-parametric data shape: (904, 125, 5)\n",
      "Parametric data shape: torch.Size([1129, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14165\\SWE_Work\\Unified-benchmark-for-SDGFTS\\src\\utils\\dat_io_utils.py:38: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df['Date'] = df['Date'].view('int64') // 10**9\n",
      "C:\\Users\\14165\\SWE_Work\\Unified-benchmark-for-SDGFTS\\src\\utils\\dat_io_utils.py:38: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df['Date'] = df['Date'].view('int64') // 10**9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GBM...\n",
      "Generating 500 samples...\n",
      "Generated data shape: torch.Size([500, 125, 5])\n",
      "Computing diversity metrics...\n",
      "Error evaluating GBM: 'Tensor' object has no attribute 'astype'\n",
      "Training OU_Process...\n",
      "Generating 500 samples...\n",
      "Generated data shape: torch.Size([500, 125, 5])\n",
      "Computing diversity metrics...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 422\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    419\u001b[39m evaluator = UnifiedEvaluator(experiment_name=\u001b[33m\"\u001b[39m\u001b[33mTimeSeries_Generation_Comprehensive_Evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# Run complete evaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpara_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpara_ds_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnonpara_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnonpara_ds_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTimeGAN_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m    427\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m    430\u001b[39m show_with_start_divider(\u001b[33m\"\u001b[39m\u001b[33mEVALUATION SUMMARY\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Using utility function\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 363\u001b[39m, in \u001b[36mUnifiedEvaluator.run_complete_evaluation\u001b[39m\u001b[34m(self, nonpara_cfg, para_cfg, models_config, num_samples)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, ParametricModel):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_parametric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_data_para\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Parametric data\u001b[39;49;00m\n\u001b[32m    367\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_para\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data for parametric models\u001b[39;49;00m\n\u001b[32m    368\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_generated_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    371\u001b[39m         results = \u001b[38;5;28mself\u001b[39m.evaluate_nonparametric(\n\u001b[32m    372\u001b[39m             model=model,\n\u001b[32m    373\u001b[39m             model_name=model_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    376\u001b[39m             num_generated_samples=num_samples\n\u001b[32m    377\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mUnifiedEvaluator.evaluate_parametric\u001b[39m\u001b[34m(self, model, model_name, real_data, train_data, num_generated_samples)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 3. Diversity Metrics\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing diversity metrics...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m diversity_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_diversity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m evaluation_results.update(diversity_results)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 4. Fidelity Metrics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mUnifiedEvaluator._evaluate_diversity\u001b[39m\u001b[34m(self, synthetic_data)\u001b[39m\n\u001b[32m    209\u001b[39m results = {}\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Intra-Class Distance with Euclidean metric\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m icd_euclidean = \u001b[43mcalculate_icd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meuclidean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33micd_euclidean\u001b[39m\u001b[33m\"\u001b[39m] = icd_euclidean\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Intra-Class Distance with DTW metric\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\SWE_Work\\Unified-benchmark-for-SDGFTS\\src\\taxonomies\\diversity.py:45\u001b[39m, in \u001b[36mcalculate_icd\u001b[39m\u001b[34m(comp_data, metric)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     44\u001b[39m         diff = comp_data[i, :, ch] - comp_data[j, :, ch]\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m         dist += \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mdtw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     48\u001b[39m         \u001b[38;5;66;03m# Ensure data is in the correct format for DTW\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
