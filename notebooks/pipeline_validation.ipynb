{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "import yaml\n",
    "from abc import ABC, abstractmethod\n",
    "import inspect\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.models.base.base_model import ParametricModel, DeepLearningModel\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OUProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.block_bootstrap import BlockBootstrap\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "from src.models.non_parametric.quant_gan import QuantGAN\n",
    "from src.models.non_parametric.time_vae import TimeVAE\n",
    "from src.models.non_parametric.takahashi import TakahashiDiffusion\n",
    "\n",
    "from src.utils.display_utils import show_with_start_divider, show_with_end_divider\n",
    "from src.utils.preprocessing_utils import (\n",
    "    create_dataloaders,\n",
    "    preprocess_data,\n",
    "    LogReturnTransformation,\n",
    "    sliding_window_view,\n",
    ")\n",
    "from src.utils.configs_utils import get_dataset_cfgs\n",
    "from src.utils.evaluation_classes_utils import (\n",
    "    TaxonomyEvaluator,\n",
    "    DiversityEvaluator,\n",
    "    FidelityEvaluator,\n",
    "    RuntimeEvaluator,\n",
    "    StylizedFactsEvaluator,\n",
    "    VisualAssessmentEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator class to initialize the MLFlow experiment and evaluate the models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        experiment_name: str,\n",
    "        parametric_dataset_cfgs: Dict[str, Any],\n",
    "        non_parametric_dataset_cfgs: Dict[str, Any]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with MLFlow experiment.\n",
    "\n",
    "        Args:\n",
    "            experiment_name (str): Name of the MLFlow experiment\n",
    "            parametric_dataset_cfgs (Dict[str, Any]): Configuration for parametric dataset\n",
    "        \"\"\"\n",
    "        self.parametric_dataset_cfgs = parametric_dataset_cfgs\n",
    "        self.non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        self.results = {}\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.results_dir = project_root / \"results\" / f\"evaluation_{self.timestamp}\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        real_data: np.ndarray,\n",
    "        train_data,\n",
    "        generation_kwargs: Dict[str, Any],\n",
    "        fit_kwargs: Dict[str, Any] = None,\n",
    "        seed: int = 42\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Unified evaluation for both parametric.\n",
    "\n",
    "        Args:\n",
    "            model: The type of generative model to evaluate\n",
    "            model_name: Name of the model for logging\n",
    "            real_data: Real data for comparison\n",
    "            train_data: Training data\n",
    "            generation_kwargs: Optional kwargs for model.generate()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        show_with_start_divider(f\"Evaluating {model_name}\")\n",
    "        num_samples = generation_kwargs.get('num_samples', 500)\n",
    "        if fit_kwargs is not None:\n",
    "            num_epochs = fit_kwargs.get('num_epochs', 1)\n",
    "            \n",
    "        model_dir = self.results_dir / model_name\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{self.timestamp}\"):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "            evaluation_results: Dict[str, Any] = {}\n",
    "\n",
    "            print(f\"Training {model_name}...\")\n",
    "            if isinstance(model, DeepLearningModel):\n",
    "                model.fit(train_data, num_epochs=num_epochs)\n",
    "            else:\n",
    "                model.fit(train_data)\n",
    "\n",
    "            print(f\"\\nGenerating {num_samples} samples...\")\n",
    "            runtime_evaluator = RuntimeEvaluator(\n",
    "                generate_func=model.generate,\n",
    "                generation_kwargs=generation_kwargs\n",
    "            )\n",
    "            runtime_results = runtime_evaluator.evaluate()\n",
    "            mlflow.log_metric(\n",
    "                f\"generation_time_{num_samples}_samples\",\n",
    "                runtime_results[f\"generation_time_{num_samples}_samples\"]\n",
    "            )\n",
    "            evaluation_results.update(runtime_results)\n",
    "\n",
    "            generated_data = model.generate(**generation_kwargs)\n",
    "\n",
    "            if \"torch\" in str(type(generated_data)):\n",
    "                generated_data = generated_data.numpy()\n",
    "            if \"torch\" in str(type(real_data)):\n",
    "                real_data = real_data.numpy()\n",
    "            else:\n",
    "                real_data = np.asarray(real_data)\n",
    "\n",
    "            if real_data.ndim == 1:\n",
    "                window_size = generation_kwargs.get('generation_length', 1)\n",
    "                real_data = sliding_window_view(real_data, window_size, 1)\n",
    "            idx = np.random.permutation(real_data.shape[0])[:num_samples]\n",
    "            real_data = real_data[idx]\n",
    "\n",
    "            print(f\"Generated data shape: {generated_data.shape}\")\n",
    "            print(f\"Real data shape: {real_data.shape}\")\n",
    "\n",
    "            evaluators = [\n",
    "                FidelityEvaluator(real_data, generated_data),\n",
    "                # DiversityEvaluator(real_data, generated_data),\n",
    "                StylizedFactsEvaluator(real_data, generated_data),\n",
    "                VisualAssessmentEvaluator(real_data, generated_data, model_dir)\n",
    "            ]\n",
    "\n",
    "            for evaluator in evaluators:\n",
    "                print(f\"Computing {evaluator.__class__.__name__}...\")\n",
    "                results = evaluator.evaluate()\n",
    "                if results is not None:\n",
    "                    evaluation_results.update(results)\n",
    "\n",
    "            metrics_path = model_dir / \"metrics.json\"\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "            mlflow.log_artifact(str(metrics_path))\n",
    "            print(f\"Evaluation completed for {model_name} (results saved at {metrics_path}).\")\n",
    "\n",
    "            return evaluation_results\n",
    "\n",
    "    def run_complete_evaluation(self, num_samples: int = 500, seed: int = 42) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run complete evaluation on all models with 500 generated samples per model.\n",
    "\n",
    "        Args:\n",
    "            dataset_config: Configuration for data preprocessing\n",
    "            models_config: Configuration for models\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing results for all models\n",
    "        \"\"\"\n",
    "        show_with_start_divider(\"Starting Complete Evaluation Pipeline\")\n",
    "        train_data_para, valid_data_para, test_data_para, _, _, _ = preprocess_data(self.parametric_dataset_cfgs)\n",
    "\n",
    "        length_para = train_data_para.shape[0]\n",
    "        print(f\"  - Parametric train data shape: {train_data_para.shape}\")\n",
    "        print(f\"  - Parametric valid data shape: {valid_data_para.shape}\")\n",
    "        print(f\"  - Parametric test data shape: {test_data_para.shape}\")\n",
    "\n",
    "        (\n",
    "            train_data_non_para,\n",
    "            valid_data_non_para,\n",
    "            test_data_non_para,\n",
    "            train_initial_non_para,\n",
    "            valid_initial_non_para,\n",
    "            test_initial_non_para\n",
    "        ) = preprocess_data(self.non_parametric_dataset_cfgs)\n",
    "        train_loader_non_para, valid_loader_non_para, test_loader_non_para = create_dataloaders(\n",
    "            train_data_non_para, \n",
    "            valid_data_non_para, \n",
    "            test_data_non_para, \n",
    "            batch_size=32, \n",
    "            train_seed=42, \n",
    "            valid_seed=42, \n",
    "            test_seed=42,\n",
    "            train_initial=train_initial_non_para,\n",
    "            valid_initial=valid_initial_non_para,\n",
    "            test_initial=test_initial_non_para,\n",
    "        )\n",
    "\n",
    "        num_timeseries, generation_length = train_data_non_para.shape\n",
    "        print(f\"  - Non-parametric train data shape: {train_data_non_para.shape}\")\n",
    "        print(f\"  - Non-parametric valid data shape: {valid_data_non_para.shape}\")\n",
    "        print(f\"  - Non-parametric test data shape: {test_data_non_para.shape}\")\n",
    "\n",
    "        parametric_models = {}\n",
    "        # parametric_models[\"GBM\"] = GeometricBrownianMotion()\n",
    "        # parametric_models[\"OU Process\"] = OUProcess()\n",
    "        # parametric_models[\"MJD\"] = MertonJumpDiffusion()\n",
    "        # parametric_models[\"GARCH11\"] = GARCH11()\n",
    "        # parametric_models[\"DEJD\"] = DoubleExponentialJumpDiffusion()\n",
    "        # parametric_models[\"BlockBootstrap\"] = BlockBootstrap(block_size=generation_length)\n",
    "\n",
    "        non_parametric_models = {}\n",
    "        # non_parametric_models[\"TimeGAN\"] = TimeGAN(seq_len=generation_length, hidden_dim=24, num_layers=3, learning_rate=1e-5)\n",
    "        # non_parametric_models[\"QuantGAN\"] = QuantGAN()\n",
    "        non_parametric_models[\"TimeVAE\"] = TimeVAE(\n",
    "            length=None,\n",
    "            num_channels=1,\n",
    "            latent_dim=10,\n",
    "            hidden_layer_sizes=[100, 200, 400],\n",
    "            trend_poly=0,\n",
    "            custom_seas=None,\n",
    "            use_residual_conn=True,\n",
    "            reconstruction_wt=3.0,\n",
    "            lr=1e-5\n",
    "        )\n",
    "        # non_parametric_models[\"Takahashi DDPM\"] = TakahashiDiffusion(\n",
    "        #     length=None,\n",
    "        #     num_channels=1,\n",
    "        #     num_steps=100,\n",
    "        #     beta_start=0.0001,\n",
    "        #     beta_end=0.02,\n",
    "        #     wavelet='haar',\n",
    "        #     lr=1e-5\n",
    "        # )\n",
    "\n",
    "        all_results = {}\n",
    "\n",
    "        generation_kwargs_para = {'num_samples': num_samples, 'generation_length': generation_length}\n",
    "        for model_name, model in parametric_models.items():\n",
    "            results = self.evaluate_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                real_data=valid_data_para,\n",
    "                train_data=train_data_para,\n",
    "                generation_kwargs=generation_kwargs_para,\n",
    "                seed=seed\n",
    "            )\n",
    "            all_results[model_name] = results\n",
    "\n",
    "        # Evaluate non-parametric models on non-parametric dataset (use DataLoader and num_epochs)\n",
    "        generation_kwargs_non_para = {'num_samples': num_samples, 'generation_length': generation_length}\n",
    "        fit_kwargs_non_para = {'num_epochs': 10}\n",
    "        for model_name, model in non_parametric_models.items():\n",
    "            results = self.evaluate_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                real_data=valid_data_non_para,\n",
    "                train_data=train_loader_non_para,\n",
    "                generation_kwargs=generation_kwargs_non_para,\n",
    "                fit_kwargs=fit_kwargs_non_para,\n",
    "                seed=seed\n",
    "            )\n",
    "            all_results[model_name] = results\n",
    "\n",
    "        results_file = self.results_dir / \"complete_evaluation.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(all_results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(str(results_file))\n",
    "\n",
    "        show_with_end_divider(\"EVALUATION COMPLETE\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"MLFlow experiment: {self.experiment_name}\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
    "    evaluator = UnifiedEvaluator(\n",
    "        experiment_name=\"TimeSeries_Generation_Comprehensive_Evaluation\",\n",
    "        parametric_dataset_cfgs = parametric_dataset_cfgs,\n",
    "    non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n",
    "    )\n",
    "    evaluator.run_complete_evaluation(num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Starting Complete Evaluation Pipeline\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "  - Parametric train data shape: torch.Size([9056])\n",
      "  - Parametric valid data shape: torch.Size([1132])\n",
      "  - Parametric test data shape: torch.Size([1133])\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "Desired time series sample length (lag with max PACF >0): 103\n",
      "PACF at that lag: 0.040741497942971425\n",
      "  - Non-parametric train data shape: (8975, 103)\n",
      "  - Non-parametric valid data shape: (1122, 103)\n",
      "  - Non-parametric test data shape: (1122, 103)\n",
      "====================\n",
      "Evaluating TimeVAE\n",
      "Training TimeVAE...\n",
      "Inferred sequence length: 103\n",
      "Batch: [tensor([[ 0.0264, -0.0220,  0.0645,  ...,  0.0389, -0.0231, -0.0008],\n",
      "        [-0.0380,  0.0206, -0.0212,  ..., -0.0124,  0.0303, -0.0311],\n",
      "        [-0.0083,  0.0328, -0.0245,  ..., -0.0127, -0.0074,  0.0018],\n",
      "        ...,\n",
      "        [ 0.0167,  0.0365,  0.0047,  ..., -0.0593, -0.0034, -0.0901],\n",
      "        [ 0.0064, -0.0110,  0.0083,  ..., -0.0501,  0.0025,  0.0028],\n",
      "        [-0.0193, -0.0347,  0.0000,  ..., -0.0229, -0.0116,  0.0116]]), tensor([ 0.2500,  2.8229,  0.2701,  0.1535,  0.1016,  0.4654,  6.1629,  0.2472,\n",
      "         4.4436,  2.2696,  0.1808,  0.0843, 31.5750,  0.1133,  4.7396,  0.3705,\n",
      "         0.4531,  2.6754,  1.0286,  1.9789,  0.1099,  0.1652,  6.5350,  6.7096,\n",
      "         0.6747,  0.2920, 14.3764,  0.1166, 23.7750,  5.7214,  1.2536,  0.1166])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    242\u001b[39m non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n\u001b[32m    243\u001b[39m evaluator = UnifiedEvaluator(\n\u001b[32m    244\u001b[39m     experiment_name=\u001b[33m\"\u001b[39m\u001b[33mTimeSeries_Generation_Comprehensive_Evaluation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    245\u001b[39m     parametric_dataset_cfgs = parametric_dataset_cfgs,\n\u001b[32m    246\u001b[39m non_parametric_dataset_cfgs = non_parametric_dataset_cfgs\n\u001b[32m    247\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 218\u001b[39m, in \u001b[36mUnifiedEvaluator.run_complete_evaluation\u001b[39m\u001b[34m(self, num_samples, seed)\u001b[39m\n\u001b[32m    216\u001b[39m fit_kwargs_non_para = {\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10\u001b[39m}\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m non_parametric_models.items():\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_data_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_kwargs_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     all_results[model_name] = results\n\u001b[32m    229\u001b[39m results_file = \u001b[38;5;28mself\u001b[39m.results_dir / \u001b[33m\"\u001b[39m\u001b[33mcomplete_evaluation.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mUnifiedEvaluator.evaluate_model\u001b[39m\u001b[34m(self, model, model_name, real_data, train_data, generation_kwargs, fit_kwargs, seed)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, DeepLearningModel):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     model.fit(train_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/models/non_parametric/time_vae.py:330\u001b[39m, in \u001b[36mTimeVAE.fit\u001b[39m\u001b[34m(self, data_loader, num_epochs, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     x_recons, z_mean, z_log_var = \u001b[38;5;28mself\u001b[39m.forward(x)\n\u001b[32m    333\u001b[39m     reconstruction_loss = \u001b[38;5;28mself\u001b[39m._reconstruction_loss(x, x_recons)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/models/non_parametric/time_vae.py:296\u001b[39m, in \u001b[36mTimeVAE._prepare_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: torch.Tensor) -> torch.Tensor:\n\u001b[32m    295\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert batch from (batch_size, seq_len) to (batch_size, seq_len, 1).\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m() == \u001b[32m1\u001b[39m:\n\u001b[32m    297\u001b[39m         batch = batch.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    298\u001b[39m     x = batch.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
