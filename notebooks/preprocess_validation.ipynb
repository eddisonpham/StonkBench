{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Preprocessor Validation\n",
        "\n",
        "This notebook verifies that the modified preprocessing pipeline with PyTorch Dataset support creates the correct data structures and handles batching properly with seed support.\n",
        "\n",
        "## Key Features to Verify:\n",
        "- Time series data shape `(R, l, N)` where R=sequences, l=length, N=variables\n",
        "- Seed-based reproducible shuffling\n",
        "- Proper PyTorch Dataset implementation\n",
        "- Efficient DataLoader batching\n",
        "- Dynamic seed changing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root added to sys.path: C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\n",
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path().resolve().parents[0]\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "print(\"Project root added to sys.path:\", project_root)\n",
        "\n",
        "from src.utils.preprocessing_utils import (\n",
        "    TimeSeriesDataset, \n",
        "    create_dataloaders,\n",
        "    preprocess_data\n",
        ")\n",
        "\n",
        "from src.utils.configs_utils import get_dataset_cfgs\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Preprocessing with Seed Support\n",
        "\n",
        "### Testing: \n",
        "- GOOG: `data/GOOG/GOOG.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_data_shapes(train_data, valid_data, shape_labels):\n",
        "    print(f\"Data shapes:\")\n",
        "    print(f\"  Train shape: {train_data.shape} {shape_labels['train']}\")\n",
        "    print(f\"  Valid shape: {valid_data.shape} {shape_labels['valid']}\")\n",
        "    if 'sequence_length' in shape_labels:\n",
        "        print(f\"  Sequence length (l): {shape_labels['sequence_length']}\")\n",
        "    if 'num_channels' in shape_labels:\n",
        "        print(f\"  Number of channels (N): {shape_labels['num_channels']}\")\n",
        "    if 'total_samples' in shape_labels:\n",
        "        print(f\"  Total time-series samples (R): {shape_labels['total_samples']}\")\n",
        "\n",
        "def print_channel_statistics(train_data, valid_data, num_channels):\n",
        "    print(f\"\\nLog return statistics:\")\n",
        "    for channel in range(num_channels):\n",
        "        # Handle both 2D and 3D data\n",
        "        if train_data.ndim == 3:\n",
        "            train_min = train_data[:, :, channel].min()\n",
        "            train_max = train_data[:, :, channel].max()\n",
        "            train_mean = train_data[:, :, channel].mean()\n",
        "        else:\n",
        "            train_min = train_data[:, channel].min()\n",
        "            train_max = train_data[:, channel].max()\n",
        "            train_mean = train_data[:, channel].mean()\n",
        "\n",
        "        if valid_data.ndim == 3:\n",
        "            valid_min = valid_data[:, :, channel].min()\n",
        "            valid_max = valid_data[:, :, channel].max()\n",
        "            valid_mean = valid_data[:, :, channel].mean()\n",
        "        else:\n",
        "            valid_min = valid_data[:, channel].min()\n",
        "            valid_max = valid_data[:, channel].max()\n",
        "            valid_mean = valid_data[:, channel].mean()\n",
        "\n",
        "        print(f\"  Channel {channel}:\")\n",
        "        print(f\"    Train log-return range: [{train_min:.4f}, {train_max:.4f}]\")\n",
        "        print(f\"    Valid log-return range: [{valid_min:.4f}, {valid_max:.4f}]\")\n",
        "        print(f\"    Train log-return mean: {train_mean:.4f}\")\n",
        "        print(f\"    Valid log-return mean: {valid_mean:.4f}\")\n",
        "\n",
        "def test_non_parametric_preprocessing(train_data, valid_data):\n",
        "    \"\"\"\n",
        "    Run Example 1.1: Preprocessing for Non-Parametric models\n",
        "    \"\"\"\n",
        "    if train_data is not None and valid_data is not None:\n",
        "        print(f\"\\nPreprocessing successful!\")\n",
        "        shape_labels = {\n",
        "            'train': \"(R_train, l, N)\",\n",
        "            'valid': \"(R_valid, l, N)\",\n",
        "            'sequence_length': train_data.shape[1],\n",
        "            'num_channels': train_data.shape[2],\n",
        "            'total_samples': train_data.shape[0] + valid_data.shape[0],\n",
        "        }\n",
        "        print_data_shapes(train_data, valid_data, shape_labels)\n",
        "        print_channel_statistics(train_data, valid_data, train_data.shape[2])\n",
        "    else:\n",
        "        print(\"Preprocessing failed...\")\n",
        "\n",
        "def test_parametric_preprocessing(train_data, valid_data):\n",
        "    \"\"\"\n",
        "    Run Example 1.2: Preprocessing for Parametric models\n",
        "    \"\"\"\n",
        "    if train_data is not None and valid_data is not None:\n",
        "        print(f\"\\nPreprocessing successful!\")\n",
        "        shape_labels = {\n",
        "            'train': \"(l, N)\",\n",
        "            'valid': \"(l, N)\",\n",
        "            'sequence_length': train_data.shape[0],\n",
        "            'num_channels': train_data.shape[1],\n",
        "        }\n",
        "        print_data_shapes(train_data, valid_data, shape_labels)\n",
        "        print_channel_statistics(train_data, valid_data, train_data.shape[1])\n",
        "    else:\n",
        "        print(\"Preprocessing failed...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXAMPLE 1: Preprocessing for both parametric and non-parametric models\n",
            "============================================================\n",
            "Configuration Dataset: {'ticker': 'AAPL', 'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\AAPL\\\\AAPL.csv', 'valid_ratio': 0.2, 'do_transformation': True, 'seed': 42}\n",
            "\n",
            "Starting preprocessing for non-parametric...\n",
            "====================\n",
            "Data preprocessing with settings:{'ticker': 'AAPL', 'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\AAPL\\\\AAPL.csv', 'valid_ratio': 0.2, 'do_transformation': True, 'seed': 42}\n",
            "Data shape: (11179, 125, 4)\n",
            "Preprocessing for non-parametric models done.\n",
            "====================\n",
            "\n",
            "\n",
            "Preprocessing successful!\n",
            "Data shapes:\n",
            "  Train shape: (8943, 125, 4) (R_train, l, N)\n",
            "  Valid shape: (2236, 125, 4) (R_valid, l, N)\n",
            "  Sequence length (l): 125\n",
            "  Number of channels (N): 4\n",
            "  Total time-series samples (R): 11179\n",
            "\n",
            "Log return statistics:\n",
            "  Channel 0:\n",
            "    Train log-return range: [-0.2677, 0.2362]\n",
            "    Valid log-return range: [-0.1468, 0.1664]\n",
            "    Train log-return mean: 0.0009\n",
            "    Valid log-return mean: 0.0009\n",
            "  Channel 1:\n",
            "    Train log-return range: [-0.2424, 0.3275]\n",
            "    Valid log-return range: [-0.1506, 0.1699]\n",
            "    Train log-return mean: 0.0009\n",
            "    Valid log-return mean: 0.0011\n",
            "  Channel 2:\n",
            "    Train log-return range: [-0.2912, 0.2493]\n",
            "    Valid log-return range: [-0.1567, 0.1731]\n",
            "    Train log-return mean: 0.0009\n",
            "    Valid log-return mean: 0.0011\n",
            "  Channel 3:\n",
            "    Train log-return range: [-0.2791, 0.2869]\n",
            "    Valid log-return range: [-0.1721, 0.1648]\n",
            "    Train log-return mean: 0.0010\n",
            "    Valid log-return mean: 0.0014\n",
            "Configuration Dataset: {'ticker': 'AAPL', 'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\AAPL\\\\AAPL.csv', 'valid_ratio': 0.2, 'is_parametric': True, 'do_transformation': True, 'seed': 42}\n",
            "\n",
            "Starting preprocessing for parametric...\n",
            "====================\n",
            "Data preprocessing with settings:{'ticker': 'AAPL', 'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\AAPL\\\\AAPL.csv', 'valid_ratio': 0.2, 'is_parametric': True, 'do_transformation': True, 'seed': 42}\n",
            "Data shape: (11303, 4)\n",
            "Preprocessing for parametric models done.\n",
            "====================\n",
            "\n",
            "\n",
            "Preprocessing successful!\n",
            "Data shapes:\n",
            "  Train shape: torch.Size([9042, 4]) (l, N)\n",
            "  Valid shape: torch.Size([2261, 4]) (l, N)\n",
            "  Sequence length (l): 9042\n",
            "  Number of channels (N): 4\n",
            "\n",
            "Log return statistics:\n",
            "  Channel 0:\n",
            "    Train log-return range: [-0.5593, 0.2240]\n",
            "    Valid log-return range: [-0.1292, 0.2362]\n",
            "    Train log-return mean: 0.0004\n",
            "    Valid log-return mean: 0.0016\n",
            "  Channel 1:\n",
            "    Train log-return range: [-0.6182, 0.2174]\n",
            "    Valid log-return range: [-0.1365, 0.3275]\n",
            "    Train log-return mean: 0.0004\n",
            "    Valid log-return mean: 0.0017\n",
            "  Channel 2:\n",
            "    Train log-return range: [-0.6400, 0.2026]\n",
            "    Valid log-return range: [-0.2150, 0.2493]\n",
            "    Train log-return mean: 0.0005\n",
            "    Valid log-return mean: 0.0013\n",
            "  Channel 3:\n",
            "    Train log-return range: [-0.7312, 0.2127]\n",
            "    Valid log-return range: [-0.2373, 0.2869]\n",
            "    Train log-return mean: 0.0004\n",
            "    Valid log-return mean: 0.0016\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"EXAMPLE 1: Preprocessing for both parametric and non-parametric models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "nonparametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
        "\n",
        "print(f\"Configuration Dataset: {nonparametric_dataset_cfgs}\")\n",
        "print(\"\\nStarting preprocessing for non-parametric...\")\n",
        "train_data_goog, valid_data_goog = preprocess_data(nonparametric_dataset_cfgs)\n",
        "\n",
        "test_non_parametric_preprocessing(train_data_goog, valid_data_goog)\n",
        "\n",
        "print(f\"Configuration Dataset: {parametric_dataset_cfgs}\")\n",
        "print(\"\\nStarting preprocessing for parametric...\")\n",
        "train_data_para, valid_data_para = preprocess_data(parametric_dataset_cfgs)\n",
        "\n",
        "test_parametric_preprocessing(train_data_para, valid_data_para)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: PyTorch Dataset and DataLoader Creation\n",
        "\n",
        "Now let's create PyTorch datasets and dataloaders to verify proper batching and seed support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXAMPLE 2: PyTorch Dataset and DataLoader Creation\n",
            "============================================================\n",
            "Creating TimeSeriesDataset objects...\n",
            "Created datasets:\n",
            "  Train dataset length: 8943\n",
            "  Valid dataset length: 2236\n",
            "  Sample shape: torch.Size([125, 4])\n",
            "  Sample dtype: torch.float32\n",
            "  Sample is PyTorch tensor: <class 'torch.Tensor'>\n",
            "\n",
            "Creating DataLoaders...\n",
            "Created dataloaders:\n",
            "  Train batches: 280\n",
            "  Valid batches: 70\n",
            "  Batch size: 32\n",
            "\n",
            "Batch Information:\n",
            "Batch 1: shape torch.Size([32, 125, 4]), dtype torch.float32\n",
            "  Per-feature value range (excluding time channel): ['[-0.1931, 0.1512]', '[-0.1967, 0.1727]', '[-0.1962, 0.1735]']\n",
            "Batch 2: shape torch.Size([32, 125, 4]), dtype torch.float32\n",
            "  Per-feature value range (excluding time channel): ['[-0.1365, 0.1217]', '[-0.2912, 0.1771]', '[-0.2791, 0.1283]']\n",
            "Batch 3: shape torch.Size([32, 125, 4]), dtype torch.float32\n",
            "  Per-feature value range (excluding time channel): ['[-0.1656, 0.1317]', '[-0.2150, 0.1523]', '[-0.2373, 0.1283]']\n",
            "... and 277 more batches\n",
            "\n",
            "Batch shapes are correct: torch.Size([32, 125, 4]) == (32, 125, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXAMPLE 2: PyTorch Dataset and DataLoader Creation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Creating TimeSeriesDataset objects...\")\n",
        "train_dataset = TimeSeriesDataset(train_data_goog, seed=42)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_goog, seed=123)\n",
        "\n",
        "print(f\"Created datasets:\")\n",
        "print(f\"  Train dataset length: {len(train_dataset)}\")\n",
        "print(f\"  Valid dataset length: {len(valid_dataset)}\")\n",
        "print(f\"  Sample shape: {train_dataset[0].shape}\")\n",
        "print(f\"  Sample dtype: {train_dataset[0].dtype}\")\n",
        "\n",
        "sample = train_dataset[0]\n",
        "if isinstance(sample, torch.Tensor):\n",
        "    print(f\"  Sample is PyTorch tensor: {type(sample)}\")\n",
        "else:\n",
        "    print(f\"  Sample is not PyTorch tensor: {type(sample)}\")\n",
        "\n",
        "print(f\"\\nCreating DataLoaders...\")\n",
        "batch_size = 32\n",
        "train_loader, valid_loader = create_dataloaders(\n",
        "    train_data_goog, valid_data_goog,\n",
        "    batch_size=batch_size,\n",
        "    train_seed=42,\n",
        "    valid_seed=123,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"Created dataloaders:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Valid batches: {len(valid_loader)}\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "\n",
        "print(f\"\\nBatch Information:\")\n",
        "for i, batch in enumerate(train_loader):\n",
        "    print(f\"Batch {i+1}: shape {batch.shape}, dtype {batch.dtype}\")\n",
        "    feature_channels = batch[:, :, 1:]\n",
        "    if feature_channels.shape[-1] > 0:\n",
        "        ranges = []\n",
        "        for ch in range(feature_channels.shape[-1]):\n",
        "            ch_min = feature_channels[:, :, ch].min()\n",
        "            ch_max = feature_channels[:, :, ch].max()\n",
        "            ranges.append(f\"[{ch_min:.4f}, {ch_max:.4f}]\")\n",
        "        print(f\"  Per-feature value range (excluding time channel): {ranges}\")\n",
        "    else:\n",
        "        print(\"  No feature channels (only time present)\")\n",
        "    if i >= 2:\n",
        "        print(f\"... and {len(train_loader) - 3} more batches\")\n",
        "        break\n",
        "\n",
        "first_batch = next(iter(train_loader))\n",
        "expected_shape = (batch_size, train_data_goog.shape[1], train_data_goog.shape[2])\n",
        "if first_batch.shape == expected_shape:\n",
        "    print(f\"\\nBatch shapes are correct: {first_batch.shape} == {expected_shape}\")\n",
        "else:\n",
        "    print(f\"\\nBatch shape mismatch: {first_batch.shape} != {expected_shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Reproducible Training with Seed Control\n",
        "\n",
        "Let's verify that seeds produce reproducible and different shuffling patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXAMPLE 3: Reproducible Training with Seed Control\n",
            "============================================================\n",
            "Testing reproducibility with same seeds...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets with same seed produce identical order: True\n",
            "  First 10 indices (dataset1): [1011, 6599, 8677, 1005, 2819, 5709, 4134, 8623, 2402, 3772]\n",
            "  First 10 indices (dataset2): [1011, 6599, 8677, 1005, 2819, 5709, 4134, 8623, 2402, 3772]\n",
            "\n",
            "Testing different seeds produce different orders...\n",
            "Datasets with different seeds produce different order: True\n",
            "  First 10 indices (seed=42): [1011, 6599, 8677, 1005, 2819, 5709, 4134, 8623, 2402, 3772]\n",
            "  First 10 indices (seed=123): [4305, 5591, 7443, 1637, 3162, 2436, 4685, 7466, 8839, 8155]\n",
            "\n",
            "Testing dynamic seed changing...\n",
            "Seed change produces different order: True\n",
            "  Original (seed=42): [1011, 6599, 8677, 1005, 2819, 5709, 4134, 8623, 2402, 3772]\n",
            "  New (seed=999):     [5479, 5293, 1707, 1439, 2092, 6337, 6344, 1183, 6693, 2577]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXAMPLE 3: Reproducible Training with Seed Control\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Testing reproducibility with same seeds...\")\n",
        "dataset1 = TimeSeriesDataset(train_data_goog, seed=42)\n",
        "dataset2 = TimeSeriesDataset(train_data_goog, seed=42)\n",
        "\n",
        "indices1 = dataset1.get_original_indices()\n",
        "indices2 = dataset2.get_original_indices()\n",
        "\n",
        "print(f\"Datasets with same seed produce identical order: {indices1[:10] == indices2[:10]}\")\n",
        "print(f\"  First 10 indices (dataset1): {indices1[:10]}\")\n",
        "print(f\"  First 10 indices (dataset2): {indices2[:10]}\")\n",
        "\n",
        "print(f\"\\nTesting different seeds produce different orders...\")\n",
        "dataset3 = TimeSeriesDataset(train_data_goog, seed=123)\n",
        "indices3 = dataset3.get_original_indices()\n",
        "\n",
        "print(f\"Datasets with different seeds produce different order: {indices1[:10] != indices3[:10]}\")\n",
        "print(f\"  First 10 indices (seed=42): {indices1[:10]}\")\n",
        "print(f\"  First 10 indices (seed=123): {indices3[:10]}\")\n",
        "\n",
        "print(f\"\\nTesting dynamic seed changing...\")\n",
        "original_indices = dataset1.get_original_indices()[:10]\n",
        "dataset1.set_seed(999)\n",
        "new_indices = dataset1.get_original_indices()[:10]\n",
        "\n",
        "print(f\"Seed change produces different order: {original_indices != new_indices}\")\n",
        "print(f\"  Original (seed=42): {original_indices}\")\n",
        "print(f\"  New (seed=999):     {new_indices}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
