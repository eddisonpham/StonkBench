{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging Models Test - Utility-Based Evaluation\n",
    "\n",
    "This notebook tests the four deep hedging models on synthetic data using utility-based evaluation methods:\n",
    "\n",
    "1. **Augmented Testing**: Mix synthetic with real training data (50/50), train hedger, compare with real-only\n",
    "2. **Algorithm Comparison**: Train 4 hedgers on both real and synthetic data, evaluate on test sets\n",
    "\n",
    "Results are saved to the latest  directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OUProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.block_bootstrap import BlockBootstrap\n",
    "\n",
    "from src.deep_hedgers.feedforward_layers import FeedforwardDeepHedger\n",
    "from src.deep_hedgers.feedforward_time import FeedforwardTimeDeepHedger\n",
    "from src.deep_hedgers.rnn_hedger import RNNDeepHedger\n",
    "from src.deep_hedgers.lstm_hedger import LSTMDeepHedger\n",
    "\n",
    "from src.utils.preprocessing_utils import preprocess_data\n",
    "from src.utils.configs_utils import get_dataset_cfgs\n",
    "from src.taxonomies.utility import (\n",
    "    AugmentedTestingEvaluator,\n",
    "    AlgorithmComparisonEvaluator\n",
    ")\n",
    "from src.utils.metric_plot_utils import find_latest_evaluation_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Preprocessing data for AAPL\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "Desired time series sample length (lag with max PACF >0): 103\n",
      "PACF at that lag: 0.040741497942971425\n",
      "Real data shapes:\n",
      "  Train: (8975, 103)\n",
      "  Val: (1122, 103)\n",
      "  Test: (1122, 103)\n",
      "Sequence length: 103\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
    "\n",
    "train_data_para, valid_data_para, test_data_para, train_init_para, valid_init_para, test_init_para = preprocess_data(\n",
    "    parametric_dataset_cfgs\n",
    ")\n",
    "\n",
    "(\n",
    "    train_data_non_para,\n",
    "    valid_data_non_para,\n",
    "    test_data_non_para,\n",
    "    train_init_non_para,\n",
    "    valid_init_non_para,\n",
    "    test_init_non_para,\n",
    ") = preprocess_data(non_parametric_dataset_cfgs)\n",
    "\n",
    "# Convert to numpy arrays for utility evaluators\n",
    "def to_numpy(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.cpu().numpy()\n",
    "    return np.asarray(data)\n",
    "\n",
    "# Real data log returns (univariate: R, L)\n",
    "real_train_log_returns = to_numpy(train_data_non_para)\n",
    "real_val_log_returns = to_numpy(valid_data_non_para)\n",
    "real_test_log_returns = to_numpy(test_data_non_para)\n",
    "\n",
    "# Get configuration\n",
    "original_data_path = non_parametric_dataset_cfgs.get(\"original_data_path\")\n",
    "seq_length = train_data_non_para.shape[1]\n",
    "\n",
    "print(f\"Real data shapes:\")\n",
    "print(f\"  Train: {real_train_log_returns.shape}\")\n",
    "print(f\"  Val: {real_val_log_returns.shape}\")\n",
    "print(f\"  Test: {real_test_log_returns.shape}\")\n",
    "print(f\"Sequence length: {seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 samples of length 103...\n",
      "Fitting models...\n",
      "Fitting GBM...\n",
      "mu: 0.0005937263937975437, sigma: 0.0299900515594167\n",
      "Fitting OU Process...\n",
      "mu: 0.0005998738612877731, theta: 3.71058070622147, sigma: 0.08168810309975477\n",
      "Fitting MJD...\n",
      "mu: 0.001147970840732297, sigma: 0.029988395702553322, kappa: 0.009970418497181388, lam: 0.010490282685512367, mu_j: -0.0017242599942500548, sigma_j: 0.15261259320151815\n",
      "Fitting GARCH11...\n",
      "mu: 0.0015174979719101576, omega: 1.895775584754838e-05, alpha: 0.10009742465402663, beta: 0.8800128770993371\n",
      "Fitting DEJD...\n",
      "mu: 0.10455820676790209, sigma: 0.018964296626492114, lam: 0.10291519434628975, p: 0.5450643776824035, eta1: 15.493428897886261, eta2: 14.601712934992657, kappa: 1.0084483516750737\n",
      "Fitting Block Bootstrap...\n",
      "Generating synthetic data...\n",
      "Generating GBM...\n",
      "Generating OU Process...\n",
      "Generating MJD...\n",
      "Generating GARCH11...\n",
      "Generating DEJD...\n",
      "Generating Block Bootstrap...\n",
      "Synthetic data generation complete!\n",
      "GBM: (1000, 103)\n",
      "OU Process: (1000, 103)\n",
      "MJD: (1000, 103)\n",
      "GARCH11: (1000, 103)\n",
      "DEJD: (1000, 103)\n",
      "BlockBootstrap: (1000, 103)\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data for all models\n",
    "num_samples = 1000\n",
    "seed = 42\n",
    "generation_length = seq_length\n",
    "\n",
    "print(f\"Generating {num_samples} samples of length {generation_length}...\")\n",
    "\n",
    "# Initialize models\n",
    "parametric_models = {}\n",
    "parametric_models[\"GBM\"] = GeometricBrownianMotion()\n",
    "parametric_models[\"OU Process\"] = OUProcess()\n",
    "parametric_models[\"MJD\"] = MertonJumpDiffusion()\n",
    "parametric_models[\"GARCH11\"] = GARCH11()\n",
    "parametric_models[\"DEJD\"] = DoubleExponentialJumpDiffusion()\n",
    "\n",
    "block_bootstrap = BlockBootstrap(block_size=generation_length)\n",
    "\n",
    "# Fit models\n",
    "print(\"Fitting models...\")\n",
    "for name, model in parametric_models.items():\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(train_data_para)\n",
    "\n",
    "print(\"Fitting Block Bootstrap...\")\n",
    "if isinstance(train_data_para, torch.Tensor):\n",
    "    block_bootstrap.fit(train_data_para)\n",
    "else:\n",
    "    block_bootstrap.fit(torch.from_numpy(train_data_para).float())\n",
    "\n",
    "# Generate synthetic data (univariate)\n",
    "synthetic_data = {}\n",
    "generation_kwargs = {\"num_samples\": num_samples, \"generation_length\": generation_length, \"seed\": seed}\n",
    "\n",
    "print(\"Generating synthetic data...\")\n",
    "for name, model in parametric_models.items():\n",
    "    print(f\"Generating {name}...\")\n",
    "    syn_data = model.generate(**generation_kwargs)\n",
    "    if isinstance(syn_data, torch.Tensor):\n",
    "        syn_data = syn_data.cpu().numpy()\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "    else:\n",
    "        syn_data = np.asarray(syn_data)\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "    synthetic_data[name] = syn_data\n",
    "\n",
    "print(\"Generating Block Bootstrap...\")\n",
    "syn_data_bb = block_bootstrap.generate(**generation_kwargs)\n",
    "if isinstance(syn_data_bb, torch.Tensor):\n",
    "    syn_data_bb = syn_data_bb.cpu().numpy()\n",
    "    if syn_data_bb.ndim == 3:\n",
    "        syn_data_bb = syn_data_bb[:, :, 0]\n",
    "else:\n",
    "    syn_data_bb = np.asarray(syn_data_bb)\n",
    "    if syn_data_bb.ndim == 3:\n",
    "        syn_data_bb = syn_data_bb[:, :, 0]\n",
    "synthetic_data[\"BlockBootstrap\"] = syn_data_bb\n",
    "\n",
    "print(\"Synthetic data generation complete!\")\n",
    "for name, data in synthetic_data.items():\n",
    "    print(f\"{name}: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n",
      "OU Process splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n",
      "MJD splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n",
      "GARCH11 splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n",
      "DEJD splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n",
      "BlockBootstrap splits - Train: (800, 103), Val: (100, 103), Test: (100, 103)\n"
     ]
    }
   ],
   "source": [
    "# Split synthetic data into train/val/test\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    R = data.shape[0]\n",
    "    train_end = int(R * train_ratio)\n",
    "    val_end = int(R * (train_ratio + val_ratio))\n",
    "    return data[:train_end], data[train_end:val_end], data[val_end:]\n",
    "\n",
    "synthetic_splits = {}\n",
    "for name, data in synthetic_data.items():\n",
    "    train, val, test = split_data(data)\n",
    "    synthetic_splits[name] = {\n",
    "        \"train\": train,\n",
    "        \"val\": val,\n",
    "        \"test\": test\n",
    "    }\n",
    "    print(f\"{name} splits - Train: {train.shape}, Val: {val.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new evaluation directory: /Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/results/evaluation_20251114_010548\n"
     ]
    }
   ],
   "source": [
    "# Find latest evaluation directory or create new one\n",
    "try:\n",
    "    latest_eval_dir = find_latest_evaluation_folder()\n",
    "    print(f\"Found latest evaluation directory: {latest_eval_dir}\")\n",
    "except FileNotFoundError:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    latest_eval_dir = project_root / \"results\" / f\"evaluation_{timestamp}\"\n",
    "    latest_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created new evaluation directory: {latest_eval_dir}\")\n",
    "\n",
    "eval_dir = Path(latest_eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUGMENTED TESTING EVALUATION\n",
      "================================================================================\n",
      "================================================================================\n",
      "Evaluating GBM\n",
      "================================================================================\n",
      "Converting log returns to prices for real training data...\n",
      "Converting log returns to prices for real validation data...\n",
      "Converting log returns to prices for synthetic training data...\n",
      "Evaluating Feedforward_L-1...\n"
     ]
    }
   ],
   "source": [
    "# Run Augmented Testing Evaluation for each model\n",
    "print(\"=\"*80)\n",
    "print(\"AUGMENTED TESTING EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "augmented_results = {}\n",
    "hedger_classes = {\n",
    "    \"Feedforward_L-1\": FeedforwardDeepHedger,\n",
    "    \"Feedforward_Time\": FeedforwardTimeDeepHedger,\n",
    "    \"RNN\": RNNDeepHedger,\n",
    "    \"LSTM\": LSTMDeepHedger\n",
    "}\n",
    "\n",
    "for model_name, syn_data in synthetic_data.items():\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    \n",
    "    # Use first 500 samples for training\n",
    "    syn_train = syn_data[:500]\n",
    "    \n",
    "    evaluator = AugmentedTestingEvaluator(\n",
    "        real_train_log_returns=real_train_log_returns,\n",
    "        real_val_log_returns=real_val_log_returns,\n",
    "        real_train_initial=train_init_non_para,\n",
    "        real_val_initial=valid_init_non_para,\n",
    "        synthetic_train_log_returns=syn_train,\n",
    "        seq_length=seq_length,\n",
    "        strike=None,\n",
    "        hidden_size=64,\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    model_results = {}\n",
    "    for hedger_name, hedger_class in hedger_classes.items():\n",
    "        print(f\"Evaluating {hedger_name}...\")\n",
    "        results = evaluator.evaluate(hedger_class)\n",
    "        model_results[hedger_name] = results\n",
    "    \n",
    "    augmented_results[model_name] = model_results\n",
    "\n",
    "print(\"Augmented Testing Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Algorithm Comparison Evaluation for each model\n",
    "print(\"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "algorithm_comparison_results = {}\n",
    "\n",
    "for model_name, splits in synthetic_splits.items():\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    \n",
    "    evaluator = AlgorithmComparisonEvaluator(\n",
    "        real_train_log_returns=real_train_log_returns,\n",
    "        real_val_log_returns=real_val_log_returns,\n",
    "        real_test_log_returns=real_test_log_returns,\n",
    "        synthetic_train_log_returns=splits[\"train\"],\n",
    "        synthetic_val_log_returns=splits[\"val\"],\n",
    "        synthetic_test_log_returns=splits[\"test\"],\n",
    "        real_train_initial=train_init_non_para,\n",
    "        real_val_initial=valid_init_non_para,\n",
    "        real_test_initial=test_init_non_para,\n",
    "        seq_length=seq_length,\n",
    "        original_data_path=original_data_path,\n",
    "        strike=None,\n",
    "        hidden_size=64,\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    results = evaluator.evaluate()\n",
    "    algorithm_comparison_results[model_name] = results\n",
    "\n",
    "print(\"Algorithm Comparison Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to evaluation directory\n",
    "all_results = {\n",
    "    \"augmented_testing\": augmented_results,\n",
    "    \"algorithm_comparison\": algorithm_comparison_results,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"config\": {\n",
    "        \"seq_length\": seq_length,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"original_data_path\": str(original_data_path),\n",
    "        \"hedger_classes\": list(hedger_classes.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = eval_dir / \"deep_hedging_utility_results.json\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Also save individual model results\n",
    "for model_name in synthetic_data.keys():\n",
    "    model_dir = eval_dir / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model_results = {\n",
    "        \"augmented_testing\": augmented_results.get(model_name, {}),\n",
    "        \"algorithm_comparison\": algorithm_comparison_results.get(model_name, {})\n",
    "    }\n",
    "    \n",
    "    model_file = model_dir / \"utility_results.json\"\n",
    "    with open(model_file, \"w\") as f:\n",
    "        json.dump(model_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of results\n",
    "print(\"=\"*80)\n",
    "print(\"DEEP HEDGING UTILITY EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in synthetic_data.keys():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Augmented Testing:\")\n",
    "    if model_name in augmented_results:\n",
    "        for hedger_name, results in augmented_results[model_name].items():\n",
    "            score = results.get(\"score\", {})\n",
    "            print(f\"    {hedger_name}:\")\n",
    "            print(f\"      MSE Mean: {score.get(\"mse_mean\", \"N/A\")}\")\n",
    "            print(f\"      MSE P95: {score.get(\"mse_p95\", \"N/A\")}\")\n",
    "            print(f\"      MSE P05: {score.get(\"mse_p05\", \"N/A\")}\")\n",
    "    \n",
    "    print(f\"  Algorithm Comparison:\")\n",
    "    if model_name in algorithm_comparison_results:\n",
    "        for hedger_name, results in algorithm_comparison_results[model_name].items():\n",
    "            print(f\"    {hedger_name}:\")\n",
    "            real_test = results.get(\"real_test\", {})\n",
    "            syn_test = results.get(\"synthetic_test\", {})\n",
    "            print(f\"      Real Test MSE(X): {real_test.get(\"mse_X\", \"N/A\")}\")\n",
    "            print(f\"      Synthetic Test MSE(X): {syn_test.get(\"mse_X\", \"N/A\")}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
