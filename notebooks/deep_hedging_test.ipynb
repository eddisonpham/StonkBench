{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging Models Test - Utility-Based Evaluation\n",
    "\n",
    "This notebook tests the four deep hedging models on synthetic data using utility-based evaluation methods:\n",
    "\n",
    "1. **Augmented Testing**: Mix synthetic with real training data (50/50), train hedger, compare with real-only\n",
    "2. **Algorithm Comparison**: Train 4 hedgers on both real and synthetic data, evaluate on test sets\n",
    "\n",
    "Results are saved to the latest  directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.models.parametric.gbm import GeometricBrownianMotion\n",
    "from src.models.parametric.ou_process import OUProcess\n",
    "from src.models.parametric.merton_jump_diffusion import MertonJumpDiffusion\n",
    "from src.models.parametric.garch11 import GARCH11\n",
    "from src.models.parametric.de_jump_diffusion import DoubleExponentialJumpDiffusion\n",
    "from src.models.non_parametric.block_bootstrap import BlockBootstrap\n",
    "from src.models.non_parametric.time_gan import TimeGAN\n",
    "from src.models.non_parametric.time_vae import TimeVAE\n",
    "from src.models.non_parametric.quant_gan import QuantGAN\n",
    "from src.models.non_parametric.takahashi import TakahashiDiffusion\n",
    "\n",
    "from src.hedging_models.feedforward_layers import FeedforwardLayers\n",
    "from src.hedging_models.feedforward_time import FeedforwardTime\n",
    "from src.hedging_models.rnn_hedger import RNN\n",
    "from src.hedging_models.lstm_hedger import LSTM\n",
    "from src.hedging_models.black_scholes import BlackScholes\n",
    "from src.hedging_models.delta_gamma import DeltaGamma\n",
    "from src.hedging_models.random_forest import RandomForest\n",
    "from src.hedging_models.linear_regression import LinearRegression\n",
    "from src.hedging_models.xgboost import XGBoost\n",
    "from src.hedging_models.lightgbm import LightGBM\n",
    "from src.utils.preprocessing_utils import preprocess_data, create_dataloaders\n",
    "from src.utils.configs_utils import get_dataset_cfgs\n",
    "from src.taxonomies.utility import (\n",
    "    AugmentedTestingEvaluator,\n",
    "    AlgorithmComparisonEvaluator\n",
    ")\n",
    "from src.utils.metric_plot_utils import find_latest_evaluation_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Preprocessing data for AAPL\n",
      "====================\n",
      "Preprocessing data for AAPL\n",
      "Desired time series sample length (lag with max PACF >0): 103\n",
      "PACF at that lag: 0.040741497942971425\n",
      "Real data shapes:\n",
      "  Train: (8975, 103)\n",
      "  Val: (1122, 103)\n",
      "  Test: (1122, 103)\n",
      "Sequence length: 103\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "non_parametric_dataset_cfgs, parametric_dataset_cfgs = get_dataset_cfgs()\n",
    "\n",
    "train_data_para, valid_data_para, test_data_para, train_init_para, valid_init_para, test_init_para = preprocess_data(\n",
    "    parametric_dataset_cfgs\n",
    ")\n",
    "\n",
    "(\n",
    "    train_data_non_para,\n",
    "    valid_data_non_para,\n",
    "    test_data_non_para,\n",
    "    train_init_non_para,\n",
    "    valid_init_non_para,\n",
    "    test_init_non_para,\n",
    ") = preprocess_data(non_parametric_dataset_cfgs)\n",
    "\n",
    "\n",
    "# Real data log returns (univariate: R, L)\n",
    "real_train_log_returns = train_data_non_para\n",
    "real_val_log_returns = valid_data_non_para\n",
    "real_test_log_returns = test_data_non_para\n",
    "\n",
    "# Get configuration\n",
    "original_data_path = non_parametric_dataset_cfgs.get(\"original_data_path\")\n",
    "seq_length = train_data_non_para.shape[1]\n",
    "\n",
    "print(f\"Real data shapes:\")\n",
    "print(f\"  Train: {real_train_log_returns.shape}\")\n",
    "print(f\"  Val: {real_val_log_returns.shape}\")\n",
    "print(f\"  Test: {real_test_log_returns.shape}\")\n",
    "print(f\"Sequence length: {seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 samples of length 103...\n",
      "Fitting parametric models...\n",
      "Fitting Block Bootstrap...\n",
      "Creating DataLoaders for non-parametric deep learning models...\n",
      "Fitting non-parametric deep learning models...\n",
      "Fitting TimeVAE...\n",
      "Inferred sequence length: 103\n",
      "TimeVAE Epoch 1/10 - Loss: 5387.9596 | Recon Loss: 1794.0125 | KL Loss: 5.9219\n",
      "TimeVAE Epoch 2/10 - Loss: 1408.1304 | Recon Loss: 465.3695 | KL Loss: 12.0220\n",
      "TimeVAE Epoch 3/10 - Loss: 1098.8867 | Recon Loss: 364.3327 | KL Loss: 5.8884\n",
      "TimeVAE Epoch 4/10 - Loss: 921.6994 | Recon Loss: 303.2312 | KL Loss: 12.0059\n",
      "TimeVAE Epoch 5/10 - Loss: 790.1929 | Recon Loss: 254.9986 | KL Loss: 25.1970\n",
      "TimeVAE Epoch 6/10 - Loss: 699.4535 | Recon Loss: 219.8711 | KL Loss: 39.8402\n",
      "TimeVAE Epoch 7/10 - Loss: 632.8622 | Recon Loss: 194.3914 | KL Loss: 49.6879\n",
      "TimeVAE Epoch 8/10 - Loss: 598.4615 | Recon Loss: 180.8657 | KL Loss: 55.8646\n",
      "TimeVAE Epoch 9/10 - Loss: 552.1480 | Recon Loss: 164.9634 | KL Loss: 57.2580\n",
      "TimeVAE Epoch 10/10 - Loss: 512.7692 | Recon Loss: 152.6815 | KL Loss: 54.7248\n",
      "Generating synthetic data from parametric models...\n",
      "Generating Block Bootstrap...\n",
      "Generating synthetic data from non-parametric deep learning models...\n",
      "Generating TimeVAE...\n",
      "Synthetic data generation complete!\n",
      "TimeVAE: (10000, 103)\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data for all models\n",
    "num_samples = 10000\n",
    "seed = 42\n",
    "generation_length = seq_length\n",
    "\n",
    "print(f\"Generating {num_samples} samples of length {generation_length}...\")\n",
    "\n",
    "# Initialize parametric models\n",
    "parametric_models = {}\n",
    "# parametric_models[\"GBM\"] = GeometricBrownianMotion()\n",
    "# parametric_models[\"OU Process\"] = OUProcess()\n",
    "# parametric_models[\"MJD\"] = MertonJumpDiffusion()\n",
    "# parametric_models[\"GARCH11\"] = GARCH11()\n",
    "# parametric_models[\"DEJD\"] = DoubleExponentialJumpDiffusion()\n",
    "\n",
    "# block_bootstrap = BlockBootstrap(block_size=generation_length)\n",
    "\n",
    "# Fit parametric models\n",
    "print(\"Fitting parametric models...\")\n",
    "for name, model in parametric_models.items():\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(train_data_para)\n",
    "\n",
    "print(\"Fitting Block Bootstrap...\")\n",
    "# if isinstance(train_data_para, torch.Tensor):\n",
    "#     block_bootstrap.fit(train_data_para)\n",
    "# else:\n",
    "#     block_bootstrap.fit(torch.from_numpy(train_data_para).float())\n",
    "\n",
    "# Create DataLoaders for non-parametric deep learning models\n",
    "# Note: Deep learning models only need log returns, not initial values\n",
    "print(\"Creating DataLoaders for non-parametric deep learning models...\")\n",
    "train_loader_non_para, _, _ = create_dataloaders(\n",
    "    train_data_non_para,\n",
    "    valid_data_non_para,\n",
    "    test_data_non_para,\n",
    "    batch_size=128,\n",
    "    train_seed=seed,\n",
    "    train_initial=None,\n",
    "    valid_initial=None,\n",
    "    test_initial=None\n",
    ")\n",
    "\n",
    "# Initialize non-parametric deep learning models\n",
    "non_parametric_models = {}\n",
    "# non_parametric_models[\"TimeGAN\"] = TimeGAN(seq_len=generation_length, hidden_dim=24, num_layers=3, learning_rate=1e-5)\n",
    "non_parametric_models[\"TimeVAE\"] = TimeVAE(\n",
    "    length=None,\n",
    "    num_channels=1,\n",
    "    latent_dim=10,\n",
    "    hidden_layer_sizes=[100, 200, 400],\n",
    "    trend_poly=0,\n",
    "    custom_seas=None,\n",
    "    use_residual_conn=True,\n",
    "    reconstruction_wt=3.0,\n",
    "    lr=1e-5\n",
    ")\n",
    "# non_parametric_models[\"QuantGAN\"] = QuantGAN()\n",
    "# non_parametric_models[\"TakahashiDiffusion\"] = TakahashiDiffusion(\n",
    "#     length=None,\n",
    "#     num_channels=1,\n",
    "#     num_steps=100,\n",
    "#     beta_start=0.0001,\n",
    "#     beta_end=0.02,\n",
    "#     wavelet='haar',\n",
    "#     lr=1e-5\n",
    "# )\n",
    "\n",
    "# Fit non-parametric deep learning models\n",
    "print(\"Fitting non-parametric deep learning models...\")\n",
    "for name, model in non_parametric_models.items():\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(train_loader_non_para, num_epochs=10)\n",
    "\n",
    "# Generate synthetic data (univariate)\n",
    "synthetic_data = {}\n",
    "generation_kwargs = {\"num_samples\": num_samples, \"generation_length\": generation_length, \"seed\": seed}\n",
    "\n",
    "print(\"Generating synthetic data from parametric models...\")\n",
    "for name, model in parametric_models.items():\n",
    "    print(f\"Generating {name}...\")\n",
    "    syn_data = model.generate(**generation_kwargs)\n",
    "    if isinstance(syn_data, torch.Tensor):\n",
    "        syn_data = syn_data.cpu().numpy()\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "    else:\n",
    "        syn_data = np.asarray(syn_data)\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "    synthetic_data[name] = syn_data\n",
    "\n",
    "print(\"Generating Block Bootstrap...\")\n",
    "# syn_data_bb = block_bootstrap.generate(**generation_kwargs)\n",
    "# if isinstance(syn_data_bb, torch.Tensor):\n",
    "#     syn_data_bb = syn_data_bb.cpu().numpy()\n",
    "#     if syn_data_bb.ndim == 3:\n",
    "#         syn_data_bb = syn_data_bb[:, :, 0]\n",
    "# else:\n",
    "#     syn_data_bb = np.asarray(syn_data_bb)\n",
    "#     if syn_data_bb.ndim == 3:\n",
    "#         syn_data_bb = syn_data_bb[:, :, 0]\n",
    "# synthetic_data[\"BlockBootstrap\"] = syn_data_bb\n",
    "\n",
    "print(\"Generating synthetic data from non-parametric deep learning models...\")\n",
    "for name, model in non_parametric_models.items():\n",
    "    print(f\"Generating {name}...\")\n",
    "    syn_data = model.generate(**generation_kwargs)\n",
    "    if isinstance(syn_data, torch.Tensor):\n",
    "        syn_data = syn_data.cpu().numpy()\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "        elif syn_data.ndim == 1:\n",
    "            syn_data = syn_data.reshape(-1, generation_length)\n",
    "    else:\n",
    "        syn_data = np.asarray(syn_data)\n",
    "        if syn_data.ndim == 3:\n",
    "            syn_data = syn_data[:, :, 0]\n",
    "        elif syn_data.ndim == 1:\n",
    "            syn_data = syn_data.reshape(-1, generation_length)\n",
    "    synthetic_data[name] = syn_data\n",
    "\n",
    "print(\"Synthetic data generation complete!\")\n",
    "for name, data in synthetic_data.items():\n",
    "    print(f\"{name}: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeVAE splits - Train: (8000, 103), Val: (1000, 103), Test: (1000, 103)\n"
     ]
    }
   ],
   "source": [
    "# Split synthetic data into train/val/test\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    R = data.shape[0]\n",
    "    train_end = int(R * train_ratio)\n",
    "    val_end = int(R * (train_ratio + val_ratio))\n",
    "    return data[:train_end], data[train_end:val_end], data[val_end:]\n",
    "\n",
    "synthetic_splits = {}\n",
    "for name, data in synthetic_data.items():\n",
    "    train, val, test = split_data(data)\n",
    "    synthetic_splits[name] = {\n",
    "        \"train\": train,\n",
    "        \"val\": val,\n",
    "        \"test\": test\n",
    "    }\n",
    "    print(f\"{name} splits - Train: {train.shape}, Val: {val.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new evaluation directory: /Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/results/evaluation_20251115_171027\n"
     ]
    }
   ],
   "source": [
    "# Find latest evaluation directory or create new one\n",
    "try:\n",
    "    latest_eval_dir = find_latest_evaluation_folder()\n",
    "    print(f\"Found latest evaluation directory: {latest_eval_dir}\")\n",
    "except FileNotFoundError:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    latest_eval_dir = project_root / \"results\" / f\"evaluation_{timestamp}\"\n",
    "    latest_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created new evaluation directory: {latest_eval_dir}\")\n",
    "\n",
    "eval_dir = Path(latest_eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUGMENTED TESTING EVALUATION\n",
      "================================================================================\n",
      "================================================================================\n",
      "Evaluating TimeVAE\n",
      "================================================================================\n",
      "[AugmentedTestingEvaluator] Initialization started...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m evaluator = \u001b[43mAugmentedTestingEvaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_train_log_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreal_train_log_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_val_log_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreal_val_log_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_train_initial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_init_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_val_initial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_init_non_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynthetic_train_log_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[43msyn_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrike\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m model_results = {}\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hedger_name, hedger_class \u001b[38;5;129;01min\u001b[39;00m hedger_classes.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Unified-benchmark-for-SDGFTS/src/taxonomies/utility.py:256\u001b[39m, in \u001b[36mAugmentedTestingEvaluator.__init__\u001b[39m\u001b[34m(self, real_train_log_returns, real_val_log_returns, synthetic_train_log_returns, real_train_initial, real_val_initial, synthetic_train_initial, seq_length, strike, hidden_size, num_epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthetic_train_initial = synthetic_train_initial\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthetic_train_initial = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_train_initial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.float()\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Convert log returns to prices (returns torch tensors)\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[AugmentedTestingEvaluator] Converting log returns to prices for real training data...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Run Augmented Testing Evaluation for each model\n",
    "print(\"=\"*80)\n",
    "print(\"AUGMENTED TESTING EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "augmented_results = {}\n",
    "\n",
    "# Deep learning hedging models\n",
    "hedger_classes = {\n",
    "    \"Feedforward_L-1\": FeedforwardLayers,\n",
    "    \"Feedforward_Time\": FeedforwardTime,\n",
    "    \"RNN\": RNN,\n",
    "    \"LSTM\": LSTM\n",
    "}\n",
    "\n",
    "# Non-deep learning hedging models\n",
    "hedger_classes.update({\n",
    "    \"BlackScholes\": BlackScholes,\n",
    "    \"DeltaGamma\": DeltaGamma,\n",
    "    \"RandomForest\": RandomForest,\n",
    "    \"LinearRegression\": LinearRegression,\n",
    "    \"XGBoost\": XGBoost,\n",
    "    \"LightGBM\": LightGBM\n",
    "})\n",
    "\n",
    "for model_name, syn_data in synthetic_data.items():\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    \n",
    "    evaluator = AugmentedTestingEvaluator(\n",
    "        real_train_log_returns=real_train_log_returns,\n",
    "        real_val_log_returns=real_val_log_returns,\n",
    "        real_train_initial=train_init_non_para,\n",
    "        real_val_initial=valid_init_non_para,\n",
    "        synthetic_train_log_returns=syn_data,\n",
    "        seq_length=seq_length,\n",
    "        strike=None,\n",
    "        hidden_size=64,\n",
    "        num_epochs=70,\n",
    "        batch_size=128,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    model_results = {}\n",
    "    for hedger_name, hedger_class in hedger_classes.items():\n",
    "        print(f\"Evaluating {hedger_name}...\")\n",
    "        results = evaluator.evaluate(hedger_class)\n",
    "        model_results[hedger_name] = results\n",
    "    \n",
    "    augmented_results[model_name] = model_results\n",
    "\n",
    "print(\"Augmented Testing Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALGORITHM COMPARISON EVALUATION\n",
      "================================================================================\n",
      "================================================================================\n",
      "Evaluating GBM\n",
      "================================================================================\n",
      "[AlgorithmComparisonEvaluator] Initialization started...\n",
      "Converting log returns to prices for real data...\n",
      "[AlgorithmComparisonEvaluator] Converting log returns to prices for synthetic data...\n",
      "[AlgorithmComparisonEvaluator] Initialization complete.\n",
      "[AlgorithmComparisonEvaluator] Starting evaluation procedure for all hedgers...\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_L-1 ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on real data...\n",
      "Epoch 10/70, Loss: 46.224549\n",
      "Epoch 20/70, Loss: 44.316120\n",
      "Epoch 30/70, Loss: 44.314973\n",
      "Epoch 40/70, Loss: 43.117066\n",
      "Epoch 50/70, Loss: 42.896970\n",
      "Epoch 60/70, Loss: 42.509133\n",
      "Epoch 70/70, Loss: 42.131216\n",
      "Training completed. Final premium: 2.603165\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on synthetic data...\n",
      "Epoch 10/70, Loss: 0.170632\n",
      "Epoch 20/70, Loss: 0.160448\n",
      "Epoch 30/70, Loss: 0.161203\n",
      "Epoch 40/70, Loss: 0.159636\n",
      "Epoch 50/70, Loss: 0.159843\n",
      "Epoch 60/70, Loss: 0.160508\n",
      "Epoch 70/70, Loss: 0.159759\n",
      "Training completed. Final premium: 0.499607\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_L-1 on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_L-1 ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_Time ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on real data...\n",
      "Epoch 10/70, Loss: 45.522656\n",
      "Epoch 20/70, Loss: 41.136376\n",
      "Epoch 30/70, Loss: 37.533051\n",
      "Epoch 40/70, Loss: 36.452948\n",
      "Epoch 50/70, Loss: 35.379686\n",
      "Epoch 60/70, Loss: 35.538913\n",
      "Epoch 70/70, Loss: 34.842836\n",
      "Training completed. Final premium: 2.519889\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on synthetic data...\n",
      "Epoch 10/70, Loss: 0.049317\n",
      "Epoch 20/70, Loss: 0.023392\n",
      "Epoch 30/70, Loss: 0.017552\n",
      "Epoch 40/70, Loss: 0.015160\n",
      "Epoch 50/70, Loss: 0.014493\n",
      "Epoch 60/70, Loss: 0.014101\n",
      "Epoch 70/70, Loss: 0.013862\n",
      "Training completed. Final premium: 0.521472\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_Time on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_Time ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating RNN ---\n",
      "[AlgorithmComparisonEvaluator] Training RNN on real data...\n",
      "Epoch 10/70, Loss: 42.589598\n",
      "Epoch 20/70, Loss: 31.708227\n",
      "Epoch 30/70, Loss: 30.913697\n",
      "Epoch 40/70, Loss: 29.219824\n",
      "Epoch 50/70, Loss: 24.964171\n",
      "Epoch 60/70, Loss: 22.952437\n",
      "Epoch 70/70, Loss: 24.188751\n",
      "Training completed. Final premium: 1.816295\n",
      "[AlgorithmComparisonEvaluator] Training RNN on synthetic data...\n",
      "Epoch 10/70, Loss: 0.027658\n",
      "Epoch 20/70, Loss: 0.015855\n",
      "Epoch 30/70, Loss: 0.015397\n",
      "Epoch 40/70, Loss: 0.014538\n",
      "Epoch 50/70, Loss: 0.014582\n",
      "Epoch 60/70, Loss: 0.014143\n",
      "Epoch 70/70, Loss: 0.013681\n",
      "Training completed. Final premium: 0.521725\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for RNN on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with RNN ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating LSTM ---\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on real data...\n",
      "Epoch 10/70, Loss: 40.926537\n",
      "Epoch 20/70, Loss: 30.481348\n",
      "Epoch 30/70, Loss: 18.811359\n",
      "Epoch 40/70, Loss: 13.110541\n",
      "Epoch 50/70, Loss: 14.027953\n",
      "Epoch 60/70, Loss: 8.469024\n",
      "Epoch 70/70, Loss: 6.538154\n",
      "Training completed. Final premium: 0.720779\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on synthetic data...\n",
      "Epoch 10/70, Loss: 0.034944\n",
      "Epoch 20/70, Loss: 0.017444\n",
      "Epoch 30/70, Loss: 0.013554\n",
      "Epoch 40/70, Loss: 0.011538\n",
      "Epoch 50/70, Loss: 0.010129\n",
      "Epoch 60/70, Loss: 0.009706\n",
      "Epoch 70/70, Loss: 0.009329\n",
      "Training completed. Final premium: 0.519944\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for LSTM on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with LSTM ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] Evaluation of all hedgers complete.\n",
      "================================================================================\n",
      "Evaluating MJD\n",
      "================================================================================\n",
      "[AlgorithmComparisonEvaluator] Initialization started...\n",
      "Converting log returns to prices for real data...\n",
      "[AlgorithmComparisonEvaluator] Converting log returns to prices for synthetic data...\n",
      "[AlgorithmComparisonEvaluator] Initialization complete.\n",
      "[AlgorithmComparisonEvaluator] Starting evaluation procedure for all hedgers...\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_L-1 ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on real data...\n",
      "Epoch 10/70, Loss: 46.799067\n",
      "Epoch 20/70, Loss: 44.231550\n",
      "Epoch 30/70, Loss: 43.542763\n",
      "Epoch 40/70, Loss: 43.429544\n",
      "Epoch 50/70, Loss: 42.664789\n",
      "Epoch 60/70, Loss: 42.337113\n",
      "Epoch 70/70, Loss: 43.064741\n",
      "Training completed. Final premium: 2.618420\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on synthetic data...\n",
      "Epoch 10/70, Loss: 0.215698\n",
      "Epoch 20/70, Loss: 0.194322\n",
      "Epoch 30/70, Loss: 0.194790\n",
      "Epoch 40/70, Loss: 0.194553\n",
      "Epoch 50/70, Loss: 0.194775\n",
      "Epoch 60/70, Loss: 0.193077\n",
      "Epoch 70/70, Loss: 0.196838\n",
      "Training completed. Final premium: 0.524041\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_L-1 on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_L-1 ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_Time ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on real data...\n",
      "Epoch 10/70, Loss: 45.522656\n",
      "Epoch 20/70, Loss: 41.136376\n",
      "Epoch 30/70, Loss: 37.533051\n",
      "Epoch 40/70, Loss: 36.452948\n",
      "Epoch 50/70, Loss: 35.379686\n",
      "Epoch 60/70, Loss: 35.538913\n",
      "Epoch 70/70, Loss: 34.842836\n",
      "Training completed. Final premium: 2.519889\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on synthetic data...\n",
      "Epoch 10/70, Loss: 0.088447\n",
      "Epoch 20/70, Loss: 0.040471\n",
      "Epoch 30/70, Loss: 0.032616\n",
      "Epoch 40/70, Loss: 0.030292\n",
      "Epoch 50/70, Loss: 0.028544\n",
      "Epoch 60/70, Loss: 0.028298\n",
      "Epoch 70/70, Loss: 0.028018\n",
      "Training completed. Final premium: 0.579423\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_Time on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_Time ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating RNN ---\n",
      "[AlgorithmComparisonEvaluator] Training RNN on real data...\n",
      "Epoch 10/70, Loss: 42.589598\n",
      "Epoch 20/70, Loss: 31.708227\n",
      "Epoch 30/70, Loss: 30.913697\n",
      "Epoch 40/70, Loss: 29.219824\n",
      "Epoch 50/70, Loss: 24.964171\n",
      "Epoch 60/70, Loss: 22.952437\n",
      "Epoch 70/70, Loss: 24.188751\n",
      "Training completed. Final premium: 1.816295\n",
      "[AlgorithmComparisonEvaluator] Training RNN on synthetic data...\n",
      "Epoch 10/70, Loss: 0.050321\n",
      "Epoch 20/70, Loss: 0.031289\n",
      "Epoch 30/70, Loss: 0.029839\n",
      "Epoch 40/70, Loss: 0.029548\n",
      "Epoch 50/70, Loss: 0.028962\n",
      "Epoch 60/70, Loss: 0.028852\n",
      "Epoch 70/70, Loss: 0.029225\n",
      "Training completed. Final premium: 0.578437\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for RNN on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with RNN ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating LSTM ---\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on real data...\n",
      "Epoch 10/70, Loss: 40.926537\n",
      "Epoch 20/70, Loss: 30.481348\n",
      "Epoch 30/70, Loss: 18.811359\n",
      "Epoch 40/70, Loss: 13.110541\n",
      "Epoch 50/70, Loss: 14.027953\n",
      "Epoch 60/70, Loss: 8.469024\n",
      "Epoch 70/70, Loss: 6.538154\n",
      "Training completed. Final premium: 0.720779\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on synthetic data...\n",
      "Epoch 10/70, Loss: 0.063762\n",
      "Epoch 20/70, Loss: 0.033603\n",
      "Epoch 30/70, Loss: 0.031016\n",
      "Epoch 40/70, Loss: 0.029931\n",
      "Epoch 50/70, Loss: 0.027695\n",
      "Epoch 60/70, Loss: 0.027041\n",
      "Epoch 70/70, Loss: 0.026341\n",
      "Training completed. Final premium: 0.578720\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for LSTM on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with LSTM ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] Evaluation of all hedgers complete.\n",
      "================================================================================\n",
      "Evaluating BlockBootstrap\n",
      "================================================================================\n",
      "[AlgorithmComparisonEvaluator] Initialization started...\n",
      "Converting log returns to prices for real data...\n",
      "[AlgorithmComparisonEvaluator] Converting log returns to prices for synthetic data...\n",
      "[AlgorithmComparisonEvaluator] Initialization complete.\n",
      "[AlgorithmComparisonEvaluator] Starting evaluation procedure for all hedgers...\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_L-1 ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on real data...\n",
      "Epoch 10/70, Loss: 46.799067\n",
      "Epoch 20/70, Loss: 44.231550\n",
      "Epoch 30/70, Loss: 43.542763\n",
      "Epoch 40/70, Loss: 43.429544\n",
      "Epoch 50/70, Loss: 42.664789\n",
      "Epoch 60/70, Loss: 42.337113\n",
      "Epoch 70/70, Loss: 43.064741\n",
      "Training completed. Final premium: 2.618420\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_L-1 on synthetic data...\n",
      "Epoch 10/70, Loss: 0.196884\n",
      "Epoch 20/70, Loss: 0.185741\n",
      "Epoch 30/70, Loss: 0.187176\n",
      "Epoch 40/70, Loss: 0.187046\n",
      "Epoch 50/70, Loss: 0.185399\n",
      "Epoch 60/70, Loss: 0.185046\n",
      "Epoch 70/70, Loss: 0.184988\n",
      "Training completed. Final premium: 0.480990\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_L-1 on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_L-1 on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_L-1 ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating Feedforward_Time ---\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on real data...\n",
      "Epoch 10/70, Loss: 45.522656\n",
      "Epoch 20/70, Loss: 41.136376\n",
      "Epoch 30/70, Loss: 37.533051\n",
      "Epoch 40/70, Loss: 36.452948\n",
      "Epoch 50/70, Loss: 35.379686\n",
      "Epoch 60/70, Loss: 35.538913\n",
      "Epoch 70/70, Loss: 34.842836\n",
      "Training completed. Final premium: 2.519889\n",
      "[AlgorithmComparisonEvaluator] Training Feedforward_Time on synthetic data...\n",
      "Epoch 10/70, Loss: 0.074176\n",
      "Epoch 20/70, Loss: 0.050499\n",
      "Epoch 30/70, Loss: 0.046673\n",
      "Epoch 40/70, Loss: 0.044814\n",
      "Epoch 50/70, Loss: 0.043849\n",
      "Epoch 60/70, Loss: 0.043830\n",
      "Epoch 70/70, Loss: 0.043369\n",
      "Training completed. Final premium: 0.498342\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating Feedforward_Time on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for Feedforward_Time on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with Feedforward_Time ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating RNN ---\n",
      "[AlgorithmComparisonEvaluator] Training RNN on real data...\n",
      "Epoch 10/70, Loss: 42.589598\n",
      "Epoch 20/70, Loss: 31.708227\n",
      "Epoch 30/70, Loss: 30.913697\n",
      "Epoch 40/70, Loss: 29.219824\n",
      "Epoch 50/70, Loss: 24.964171\n",
      "Epoch 60/70, Loss: 22.952437\n",
      "Epoch 70/70, Loss: 24.188751\n",
      "Training completed. Final premium: 1.816295\n",
      "[AlgorithmComparisonEvaluator] Training RNN on synthetic data...\n",
      "Epoch 10/70, Loss: 0.041607\n",
      "Epoch 20/70, Loss: 0.031154\n",
      "Epoch 30/70, Loss: 0.028641\n",
      "Epoch 40/70, Loss: 0.025954\n",
      "Epoch 50/70, Loss: 0.022826\n",
      "Epoch 60/70, Loss: 0.019880\n",
      "Epoch 70/70, Loss: 0.018141\n",
      "Training completed. Final premium: 0.416042\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating RNN on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for RNN on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with RNN ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] --- Evaluating LSTM ---\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on real data...\n",
      "Epoch 10/70, Loss: 40.926537\n",
      "Epoch 20/70, Loss: 30.481348\n",
      "Epoch 30/70, Loss: 18.811359\n",
      "Epoch 40/70, Loss: 13.110541\n",
      "Epoch 50/70, Loss: 14.027953\n",
      "Epoch 60/70, Loss: 8.469024\n",
      "Epoch 70/70, Loss: 6.538154\n",
      "Training completed. Final premium: 0.720779\n",
      "[AlgorithmComparisonEvaluator] Training LSTM on synthetic data...\n",
      "Epoch 10/70, Loss: 0.065349\n",
      "Epoch 20/70, Loss: 0.039868\n",
      "Epoch 30/70, Loss: 0.026371\n",
      "Epoch 40/70, Loss: 0.022539\n",
      "Epoch 50/70, Loss: 0.017190\n",
      "Epoch 60/70, Loss: 0.013798\n",
      "Epoch 70/70, Loss: 0.012284\n",
      "Training completed. Final premium: 0.403207\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on real test set...\n",
      "[AlgorithmComparisonEvaluator] Evaluating LSTM on synthetic test set...\n",
      "[AlgorithmComparisonEvaluator] Computing metrics for LSTM on real and synthetic test sets...\n",
      "[AlgorithmComparisonEvaluator] --- Done with LSTM ---\n",
      "\n",
      "[AlgorithmComparisonEvaluator] Evaluation of all hedgers complete.\n",
      "Algorithm Comparison Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Algorithm Comparison Evaluation for each model\n",
    "print(\"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "algorithm_comparison_results = {}\n",
    "\n",
    "for model_name, splits in synthetic_splits.items():\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{\"=\"*80}\")\n",
    "    \n",
    "    evaluator = AlgorithmComparisonEvaluator(\n",
    "        real_train_log_returns=train_data_log_returns,\n",
    "        real_val_log_returns=real_val_log_returns,\n",
    "        real_test_log_returns=real_test_log_returns,\n",
    "        synthetic_train_log_returns=splits[\"train\"],\n",
    "        synthetic_val_log_returns=splits[\"val\"],\n",
    "        synthetic_test_log_returns=splits[\"test\"],\n",
    "        real_train_initial=train_init_non_para,\n",
    "        real_val_initial=valid_init_non_para,\n",
    "        real_test_initial=test_init_non_para,\n",
    "        seq_length=seq_length,\n",
    "        strike=None,\n",
    "        hidden_size=64,\n",
    "        num_epochs=70,\n",
    "        batch_size=128,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    results = evaluator.evaluate()\n",
    "    algorithm_comparison_results[model_name] = results\n",
    "\n",
    "print(\"Algorithm Comparison Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: /Users/eddisonpham/Projects/Unified-benchmark-for-SDGFTS/results/evaluation_20251114_090602/deep_hedging_utility_results.json\n",
      "All results saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save all results to evaluation directory\n",
    "all_results = {\n",
    "    \"augmented_testing\": augmented_results,\n",
    "    \"algorithm_comparison\": algorithm_comparison_results,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"config\": {\n",
    "        \"seq_length\": seq_length,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"original_data_path\": str(original_data_path),\n",
    "        \"hedger_classes\": list(hedger_classes.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = eval_dir / \"deep_hedging_utility_results.json\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Also save individual model results\n",
    "for model_name in synthetic_data.keys():\n",
    "    model_dir = eval_dir / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model_results = {\n",
    "        \"augmented_testing\": augmented_results.get(model_name, {}),\n",
    "        \"algorithm_comparison\": algorithm_comparison_results.get(model_name, {})\n",
    "    }\n",
    "    \n",
    "    model_file = model_dir / \"utility_results.json\"\n",
    "    with open(model_file, \"w\") as f:\n",
    "        json.dump(model_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"All results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
