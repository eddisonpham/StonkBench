{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Validation Notebook\n",
        "\n",
        "This notebook provides a comprehensive validation for all implemented time series generative models (parametric and non-parametric). It covers the entire pipeline from data preprocessing to model training and synthetic data generation, ensuring that each model functions as expected and produces output in the desired format `(R, l, N)`.\n",
        "\n",
        "## Table of Contents:\n",
        "1.  [Setup and Imports](#Setup-and-Imports)\n",
        "2.  [Data Preprocessing](#Data-Preprocessing)\n",
        "3.  [Parametric Model Validation](#Parametric-Model-Validation)\n",
        "    *   [Geometric Brownian Motion](#Geometric-Brownian-Motion)\n",
        "    *   [Ornstein-Uhlenbeck Process](#Ornstein-Uhlenbeck-Process)\n",
        "4.  [Non-Parametric Model Validation](#Non-Parametric-Model-Validation)\n",
        "    *   [Vanilla GAN](#Vanilla-GAN)\n",
        "    *   [Wasserstein GAN](#Wasserstein-GAN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root added to sys.path: C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\n",
            "All necessary modules imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path().resolve().parents[0]\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "print(f\"Project root added to sys.path: {project_root}\")\n",
        "\n",
        "from src.preprocessing.preprocessing import (\n",
        "    preprocess_data, \n",
        "    load_preprocessed_data,\n",
        "    create_dataset_from_preprocessed,\n",
        ")\n",
        "from src.preprocessing.transformers import (\n",
        "    TimeSeriesDataset,\n",
        "    create_dataloaders\n",
        ")\n",
        "\n",
        "from src.models.base.base_model import BaseGenerativeModel, ParametricModel, DeepLearningModel\n",
        "from src.models.parametric.gbm import GeometricBrownianMotion\n",
        "from src.models.parametric.ou_process import OrnsteinUhlenbeckProcess\n",
        "\n",
        "from src.models.non_parametric.vanilla_gan import VanillaGAN\n",
        "from src.models.non_parametric.wasserstein_gan import WassersteinGAN\n",
        "\n",
        "print(\"All necessary modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "This section demonstrates how to preprocess a sample dataset (`GOOG.csv`) using the provided utilities and create PyTorch `DataLoader` objects. This data will be used to train and validate our generative models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_OUTPUT_FOLDER = str(Path().resolve() / 'test_data_output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing normalized data with config: {'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\notebooks\\\\test_data_output', 'dataset_name': 'test_normalized', 'valid_ratio': 0.1, 'do_normalization': True, 'seed': 42}\n",
            "====================\n",
            "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\notebooks\\\\test_data_output', 'dataset_name': 'test_normalized', 'valid_ratio': 0.1, 'do_normalization': True, 'seed': 42}\n",
            "Data shape: (1132, 125, 5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing done. Preprocessed files saved to C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\notebooks\\test_data_output\\test_normalized.\n",
            "====================\n",
            "\n",
            "\n",
            "[Normalized] Train data shape: (1018, 125, 5)\n",
            "[Normalized] Valid data shape: (114, 125, 5)\n",
            "[Normalized] Number of training batches: 32\n",
            "[Normalized] Number of validation batches: 4\n",
            "\n",
            "[Normalized] Inferred model output dimensions: length=125, num_channels=5\n"
          ]
        }
      ],
      "source": [
        "# --- Normalized Data ---\n",
        "config_goog_norm = {\n",
        "    'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
        "    'output_ori_path': TEST_OUTPUT_FOLDER,\n",
        "    'dataset_name': 'test_normalized',\n",
        "    'valid_ratio': 0.1,\n",
        "    'do_normalization': True,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print(f\"Preprocessing normalized data with config: {config_goog_norm}\")\n",
        "\n",
        "train_data_norm_np, valid_data_norm_np = preprocess_data(config_goog_norm)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader_norm, valid_loader_norm = create_dataloaders(\n",
        "    train_data_norm_np, valid_data_norm_np,\n",
        "    batch_size=batch_size,\n",
        "    train_seed=42,\n",
        "    valid_seed=123,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"\\n[Normalized] Train data shape: {train_data_norm_np.shape}\")\n",
        "print(f\"[Normalized] Valid data shape: {valid_data_norm_np.shape}\")\n",
        "print(f\"[Normalized] Number of training batches: {len(train_loader_norm)}\")\n",
        "print(f\"[Normalized] Number of validation batches: {len(valid_loader_norm)}\")\n",
        "\n",
        "num_samples_real, length, num_channels = train_data_norm_np.shape\n",
        "print(f\"\\n[Normalized] Inferred model output dimensions: length={length}, num_channels={num_channels}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessing unnormalized data with config: {'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\notebooks\\\\test_data_output', 'dataset_name': 'test_unnormalized', 'valid_ratio': 0.1, 'do_normalization': False, 'seed': 42}\n",
            "====================\n",
            "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\raw\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\notebooks\\\\test_data_output', 'dataset_name': 'test_unnormalized', 'valid_ratio': 0.1, 'do_normalization': False, 'seed': 42}\n",
            "Data shape: (1132, 125, 5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing done. Preprocessed files saved to C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\notebooks\\test_data_output\\test_unnormalized.\n",
            "====================\n",
            "\n",
            "\n",
            "[Unnormalized] Train data shape: (1018, 125, 5)\n",
            "[Unnormalized] Valid data shape: (114, 125, 5)\n",
            "[Unnormalized] Number of training batches: 32\n",
            "[Unnormalized] Number of validation batches: 4\n"
          ]
        }
      ],
      "source": [
        "# --- Unnormalized Data ---\n",
        "config_goog_unnorm = {\n",
        "    'original_data_path': str(project_root / 'data' / 'raw' / 'GOOG' / 'GOOG.csv'),\n",
        "    'output_ori_path': TEST_OUTPUT_FOLDER,\n",
        "    'dataset_name': 'test_unnormalized',\n",
        "    'valid_ratio': 0.1,\n",
        "    'do_normalization': False,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print(f\"\\nPreprocessing unnormalized data with config: {config_goog_unnorm}\")\n",
        "\n",
        "train_data_unnorm_np, valid_data_unnorm_np = preprocess_data(config_goog_unnorm)\n",
        "\n",
        "train_loader_unnorm, valid_loader_unnorm = create_dataloaders(\n",
        "    train_data_unnorm_np, valid_data_unnorm_np,\n",
        "    batch_size=batch_size,\n",
        "    train_seed=42,\n",
        "    valid_seed=123,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"\\n[Unnormalized] Train data shape: {train_data_unnorm_np.shape}\")\n",
        "print(f\"[Unnormalized] Valid data shape: {valid_data_unnorm_np.shape}\")\n",
        "print(f\"[Unnormalized] Number of training batches: {len(train_loader_unnorm)}\")\n",
        "print(f\"[Unnormalized] Number of validation batches: {len(valid_loader_unnorm)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric Model Validation\n",
        "\n",
        "This section validates the functionality of each parametric time series generative model. For each model, we will:\n",
        "1.  Instantiate the model with appropriate parameters.\n",
        "2.  Train the model using the preprocessed training data.\n",
        "3.  Generate new synthetic time series samples.\n",
        "4.  Verify the shape and basic statistics of the generated data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Geometric Brownian Motion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Geometric Brownian Motion (GBM)\n",
            "==================================================\n",
            "GBM Model instantiated: <src.models.parametric.gbm.GeometricBrownianMotion object at 0x00000288D170DBB0>\n",
            "Fitting GBM model...\n",
            "GBM model parameters after fitting: mu=tensor([-3.7506e-06, -3.6300e-06, -3.7506e-06, -3.6532e-06,  5.7218e-07]), sigma=tensor([0.0349, 0.0338, 0.0343, 0.0350, 0.3321])\n",
            "Generated GBM data shape: torch.Size([100, 125, 5])\n",
            "GBM: Generated data shape is correct.\n",
            "GBM: Generated data min: 0.4420, max: 2.2903, mean: 0.9966\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Geometric Brownian Motion (GBM)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate GBM model\n",
        "gbm_model = GeometricBrownianMotion(length=length, num_channels=num_channels)\n",
        "print(f\"GBM Model instantiated: {gbm_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting GBM model...\")\n",
        "gbm_model.fit(train_loader_unnorm)\n",
        "print(f\"GBM model parameters after fitting: mu={gbm_model.mu.data}, sigma={gbm_model.sigma.data}\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "gbm_generated_data = gbm_model.generate(num_generated_samples)\n",
        "print(f\"Generated GBM data shape: {gbm_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert gbm_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"GBM: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {gbm_generated_data.shape}\"\n",
        "print(\"GBM: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"GBM: Generated data min: {gbm_generated_data.min():.4f}, max: {gbm_generated_data.max():.4f}, mean: {gbm_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ornstein-Uhlenbeck Process\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Ornstein-Uhlenbeck (O-U) Process\n",
            "==================================================\n",
            "O-U Model instantiated: <src.models.parametric.ou_process.OrnsteinUhlenbeckProcess object at 0x00000288D198E060>\n",
            "Fitting O-U model...\n",
            "O-U model parameters after fitting: theta=tensor([10., 10., 10., 10., 10.]), mu=tensor([1.3685e+02, 1.3843e+02, 1.3541e+02, 1.3693e+02, 2.4590e+07]), sigma=tensor([1., 1., 1., 1., 1.])\n",
            "Generated O-U data shape: torch.Size([100, 125, 5])\n",
            "O-U: Generated data shape is correct.\n",
            "O-U: Generated data min: 134.5321, max: 24590500.0000, mean: 4918209.5000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Ornstein-Uhlenbeck (O-U) Process\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate O-U model\n",
        "ou_model = OrnsteinUhlenbeckProcess(length=length, num_channels=num_channels)\n",
        "print(f\"O-U Model instantiated: {ou_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting O-U model...\")\n",
        "ou_model.fit(train_loader_unnorm)\n",
        "print(f\"O-U model parameters after fitting: theta={ou_model.theta.data}, mu={ou_model.mu.data}, sigma={ou_model.sigma.data}\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "ou_generated_data = ou_model.generate(num_generated_samples)\n",
        "print(f\"Generated O-U data shape: {ou_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert ou_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"O-U: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {ou_generated_data.shape}\"\n",
        "print(\"O-U: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"O-U: Generated data min: {ou_generated_data.min():.4f}, max: {ou_generated_data.max():.4f}, mean: {ou_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Parametric Model Validation\n",
        "\n",
        "This section validates the functionality of each non-parametric (GAN-based) time series generative model. For each model, we will:\n",
        "1.  Instantiate the model with appropriate parameters.\n",
        "2.  Train the model using the preprocessed training data.\n",
        "3.  Generate new synthetic time series samples.\n",
        "4.  Verify the shape and basic statistics of the generated data.\n",
        "\n",
        "Note: GAN training can be unstable and convergence is not guaranteed with simple validation. This is primarily to check code execution and output format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vanilla GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Vanilla GAN\n",
            "==================================================\n",
            "Vanilla GAN Model instantiated: VanillaGAN(\n",
            "  (generator): Generator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=256, out_features=625, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=625, out_features=256, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "      (5): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (bce_loss): BCELoss()\n",
            ")\n",
            "Fitting Vanilla GAN model (this may take a while)...\n",
            "Vanilla GAN model fitting complete.\n",
            "Generated Vanilla GAN data shape: torch.Size([100, 125, 5])\n",
            "Vanilla GAN: Generated data shape is correct.\n",
            "Vanilla GAN: Generated data min: -0.4109, max: 1.6130, mean: 0.3817\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Vanilla GAN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate Vanilla GAN model\n",
        "# Using a smaller num_epochs for quicker validation, adjust as needed\n",
        "vanilla_gan_model = VanillaGAN(length=length, num_channels=num_channels, latent_dim=64, hidden_dim=128, lr=0.0002)\n",
        "print(f\"Vanilla GAN Model instantiated: {vanilla_gan_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting Vanilla GAN model (this may take a while)...\")\n",
        "vanilla_gan_model.fit(train_loader_norm, num_epochs=50)\n",
        "print(\"Vanilla GAN model fitting complete.\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "vanilla_gan_generated_data = vanilla_gan_model.generate(num_generated_samples)\n",
        "print(f\"Generated Vanilla GAN data shape: {vanilla_gan_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert vanilla_gan_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"Vanilla GAN: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {vanilla_gan_generated_data.shape}\"\n",
        "print(\"Vanilla GAN: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"Vanilla GAN: Generated data min: {vanilla_gan_generated_data.min():.4f}, max: {vanilla_gan_generated_data.max():.4f}, mean: {vanilla_gan_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wasserstein GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Wasserstein GAN\n",
            "==================================================\n",
            "Wasserstein GAN Model instantiated: WassersteinGAN(\n",
            "  (generator): Generator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=256, out_features=625, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (critic): Critic(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=625, out_features=256, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Fitting Wasserstein GAN model (this may take a while)...\n",
            "Wasserstein GAN model fitting complete.\n",
            "Generated Wasserstein GAN data shape: torch.Size([100, 125, 5])\n",
            "Wasserstein GAN: Generated data shape is correct.\n",
            "Wasserstein GAN: Generated data min: -0.2017, max: 1.0379, mean: 0.3515\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Wasserstein GAN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate Wasserstein GAN model\n",
        "# Using a smaller num_epochs for quicker validation, adjust as needed\n",
        "wasserstein_gan_model = WassersteinGAN(length=length, num_channels=num_channels, latent_dim=64, hidden_dim=128, lr=0.00005, n_critic=5, clip_value=0.01)\n",
        "print(f\"Wasserstein GAN Model instantiated: {wasserstein_gan_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting Wasserstein GAN model (this may take a while)...\")\n",
        "wasserstein_gan_model.fit(train_loader_norm, num_epochs=50)\n",
        "print(\"Wasserstein GAN model fitting complete.\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "wasserstein_gan_generated_data = wasserstein_gan_model.generate(num_generated_samples)\n",
        "print(f\"Generated Wasserstein GAN data shape: {wasserstein_gan_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert wasserstein_gan_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"Wasserstein GAN: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {wasserstein_gan_generated_data.shape}\"\n",
        "print(\"Wasserstein GAN: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"Wasserstein GAN: Generated data min: {wasserstein_gan_generated_data.min():.4f}, max: {wasserstein_gan_generated_data.max():.4f}, mean: {wasserstein_gan_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
