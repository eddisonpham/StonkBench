{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Validation Notebook\n",
        "\n",
        "This notebook provides a comprehensive validation for all implemented time series generative models (parametric and non-parametric). It covers the entire pipeline from data preprocessing to model training and synthetic data generation, ensuring that each model functions as expected and produces output in the desired format `(R, l, N)`.\n",
        "\n",
        "## Table of Contents:\n",
        "1.  [Setup and Imports](#Setup-and-Imports)\n",
        "2.  [Data Preprocessing](#Data-Preprocessing)\n",
        "3.  [Parametric Model Validation](#Parametric-Model-Validation)\n",
        "    *   [Geometric Brownian Motion](#Geometric-Brownian-Motion)\n",
        "    *   [Ornstein-Uhlenbeck Process](#Ornstein-Uhlenbeck-Process)\n",
        "4.  [Non-Parametric Model Validation](#Non-Parametric-Model-Validation)\n",
        "    *   [Vanilla GAN](#Vanilla-GAN)\n",
        "    *   [Wasserstein GAN](#Wasserstein-GAN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root added to sys.path: C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\n",
            "All necessary modules imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to sys.path\n",
        "project_root = Path().resolve().parents[0]\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "print(f\"Project root added to sys.path: {project_root}\")\n",
        "\n",
        "# Import preprocessing utilities\n",
        "from data.preprocess import (\n",
        "    preprocess_data, \n",
        "    load_preprocessed_data,\n",
        "    create_dataset_from_preprocessed,\n",
        ")\n",
        "from utils.preprocess_utils import (\n",
        "    TimeSeriesDataset,\n",
        "    create_dataloaders\n",
        ")\n",
        "\n",
        "# Import BaseModel and all implemented models\n",
        "from models.base_model import BaseGenerativeModel, ParametricModel, DeepLearningModel\n",
        "from models.parametric.gbm import GeometricBrownianMotion\n",
        "from models.parametric.ou_process import OrnsteinUhlenbeckProcess\n",
        "\n",
        "from models.non_parametric.vanilla_gan import VanillaGAN\n",
        "from models.non_parametric.wasserstein_gan import WassersteinGAN\n",
        "\n",
        "print(\"All necessary modules imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "This section demonstrates how to preprocess a sample dataset (`GOOG.csv`) using the provided utilities and create PyTorch `DataLoader` objects. This data will be used to train and validate our generative models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data with config: {'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\testing_results\\\\preprocessed', 'dataset_name': 'goog_stock_validation', 'valid_ratio': 0.1, 'do_normalization': True, 'seed': 42}\n",
            "====================\n",
            "Data preprocessing with settings:{'original_data_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\data\\\\GOOG\\\\GOOG.csv', 'output_ori_path': 'C:\\\\Users\\\\14165\\\\Downloads\\\\Unified-benchmark-for-SDGFTS-main\\\\testing_results\\\\preprocessed', 'dataset_name': 'goog_stock_validation', 'valid_ratio': 0.1, 'do_normalization': True, 'seed': 42}\n",
            "Data shape: (1132, 125, 5)\n",
            "Preprocessing done. Preprocessed files saved to C:\\Users\\14165\\Downloads\\Unified-benchmark-for-SDGFTS-main\\testing_results\\preprocessed\\goog_stock_validation.\n",
            "====================\n",
            "\n",
            "\n",
            "Train data shape: (1018, 125, 5)\n",
            "Valid data shape: (114, 125, 5)\n",
            "Number of training batches: 32\n",
            "Number of validation batches: 4\n",
            "\n",
            "Inferred model output dimensions: length=125, num_channels=5\n"
          ]
        }
      ],
      "source": [
        "# Configuration for data preprocessing\n",
        "config_goog = {\n",
        "    'original_data_path': str(project_root / 'data' / 'GOOG' / 'GOOG.csv'),\n",
        "    'output_ori_path': str(project_root / 'testing_results' / 'preprocessed'),\n",
        "    'dataset_name': 'goog_stock_validation',\n",
        "    'valid_ratio': 0.1,\n",
        "    'do_normalization': True,\n",
        "    'seed': 42  # Reproducible shuffling\n",
        "}\n",
        "\n",
        "print(f\"Preprocessing data with config: {config_goog}\")\n",
        "\n",
        "# Preprocess the data\n",
        "train_data_np, valid_data_np = preprocess_data(config_goog)\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "batch_size = 32\n",
        "train_loader, valid_loader = create_dataloaders(\n",
        "    train_data_np, valid_data_np,\n",
        "    batch_size=batch_size,\n",
        "    train_seed=42,\n",
        "    valid_seed=123,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "# Display data shapes and DataLoader info\n",
        "print(f\"\\nTrain data shape: {train_data_np.shape}\")\n",
        "print(f\"Valid data shape: {valid_data_np.shape}\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(valid_loader)}\")\n",
        "\n",
        "# Get output dimensions for models\n",
        "num_samples_real, length, num_channels = train_data_np.shape\n",
        "print(f\"\\nInferred model output dimensions: length={length}, num_channels={num_channels}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric Model Validation\n",
        "\n",
        "This section validates the functionality of each parametric time series generative model. For each model, we will:\n",
        "1.  Instantiate the model with appropriate parameters.\n",
        "2.  Train the model using the preprocessed training data.\n",
        "3.  Generate new synthetic time series samples.\n",
        "4.  Verify the shape and basic statistics of the generated data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Geometric Brownian Motion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Geometric Brownian Motion (GBM)\n",
            "==================================================\n",
            "GBM Model instantiated: <models.parametric.gbm.GeometricBrownianMotion object at 0x00000247525E7470>\n",
            "Fitting GBM model...\n",
            "GBM model parameters after fitting: mu=tensor([-0.0026, -0.0026, -0.0025, -0.0028, -0.0002]), sigma=tensor([0.6190, 0.5936, 0.5898, 0.6390, 0.8405])\n",
            "Generated GBM data shape: torch.Size([100, 125, 5])\n",
            "GBM: Generated data shape is correct.\n",
            "GBM: Generated data min: 0.0894, max: 6.7962, mean: 0.9926\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Geometric Brownian Motion (GBM)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate GBM model\n",
        "gbm_model = GeometricBrownianMotion(length=length, num_channels=num_channels)\n",
        "print(f\"GBM Model instantiated: {gbm_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting GBM model...\")\n",
        "gbm_model.fit(train_loader)\n",
        "print(f\"GBM model parameters after fitting: mu={gbm_model.mu.data}, sigma={gbm_model.sigma.data}\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "gbm_generated_data = gbm_model.generate(num_generated_samples)\n",
        "print(f\"Generated GBM data shape: {gbm_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert gbm_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"GBM: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {gbm_generated_data.shape}\"\n",
        "print(\"GBM: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"GBM: Generated data min: {gbm_generated_data.min():.4f}, max: {gbm_generated_data.max():.4f}, mean: {gbm_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ornstein-Uhlenbeck Process\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Ornstein-Uhlenbeck (O-U) Process\n",
            "==================================================\n",
            "O-U Model instantiated: <models.parametric.ou_process.OrnsteinUhlenbeckProcess object at 0x00000247189D6E70>\n",
            "Fitting O-U model...\n",
            "O-U model parameters after fitting: theta=tensor([ 1.1829,  1.1025,  1.1259,  1.2380, 10.0000]), mu=tensor([0.4126, 0.4066, 0.4108, 0.4106, 0.1972]), sigma=tensor([0.3606, 0.3425, 0.3451, 0.3566, 1.0000])\n",
            "Generated O-U data shape: torch.Size([100, 125, 5])\n",
            "O-U: Generated data shape is correct.\n",
            "O-U: Generated data min: -0.6173, max: 1.1716, mean: 0.3638\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Ornstein-Uhlenbeck (O-U) Process\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate O-U model\n",
        "ou_model = OrnsteinUhlenbeckProcess(length=length, num_channels=num_channels)\n",
        "print(f\"O-U Model instantiated: {ou_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting O-U model...\")\n",
        "ou_model.fit(train_loader)\n",
        "print(f\"O-U model parameters after fitting: theta={ou_model.theta.data}, mu={ou_model.mu.data}, sigma={ou_model.sigma.data}\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "ou_generated_data = ou_model.generate(num_generated_samples)\n",
        "print(f\"Generated O-U data shape: {ou_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert ou_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"O-U: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {ou_generated_data.shape}\"\n",
        "print(\"O-U: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"O-U: Generated data min: {ou_generated_data.min():.4f}, max: {ou_generated_data.max():.4f}, mean: {ou_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Parametric Model Validation\n",
        "\n",
        "This section validates the functionality of each non-parametric (GAN-based) time series generative model. For each model, we will:\n",
        "1.  Instantiate the model with appropriate parameters.\n",
        "2.  Train the model using the preprocessed training data.\n",
        "3.  Generate new synthetic time series samples.\n",
        "4.  Verify the shape and basic statistics of the generated data.\n",
        "\n",
        "Note: GAN training can be unstable and convergence is not guaranteed with simple validation. This is primarily to check code execution and output format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vanilla GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Vanilla GAN\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla GAN Model instantiated: VanillaGAN(\n",
            "  (generator): Generator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=256, out_features=625, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=625, out_features=256, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.2)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "      (5): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (bce_loss): BCELoss()\n",
            ")\n",
            "Fitting Vanilla GAN model (this may take a while)...\n",
            "Vanilla GAN model fitting complete.\n",
            "Generated Vanilla GAN data shape: torch.Size([100, 125, 5])\n",
            "Vanilla GAN: Generated data shape is correct.\n",
            "Vanilla GAN: Generated data min: -0.3147, max: 1.4465, mean: 0.3548\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Vanilla GAN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate Vanilla GAN model\n",
        "# Using a smaller num_epochs for quicker validation, adjust as needed\n",
        "vanilla_gan_model = VanillaGAN(length=length, num_channels=num_channels, latent_dim=64, hidden_dim=128, lr=0.0002)\n",
        "print(f\"Vanilla GAN Model instantiated: {vanilla_gan_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting Vanilla GAN model (this may take a while)...\")\n",
        "vanilla_gan_model.fit(train_loader, num_epochs=5) # Reduced epochs for testing\n",
        "print(\"Vanilla GAN model fitting complete.\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "vanilla_gan_generated_data = vanilla_gan_model.generate(num_generated_samples)\n",
        "print(f\"Generated Vanilla GAN data shape: {vanilla_gan_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert vanilla_gan_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"Vanilla GAN: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {vanilla_gan_generated_data.shape}\"\n",
        "print(\"Vanilla GAN: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"Vanilla GAN: Generated data min: {vanilla_gan_generated_data.min():.4f}, max: {vanilla_gan_generated_data.max():.4f}, mean: {vanilla_gan_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wasserstein GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validating Wasserstein GAN\n",
            "==================================================\n",
            "Wasserstein GAN Model instantiated: <models.non_parametric.wasserstein_gan.WassersteinGAN object at 0x0000024754EE3140>\n",
            "Fitting Wasserstein GAN model (this may take a while)...\n",
            "Wasserstein GAN model fitting complete.\n",
            "Generated Wasserstein GAN data shape: torch.Size([100, 125, 5])\n",
            "Wasserstein GAN: Generated data shape is correct.\n",
            "Wasserstein GAN: Generated data min: -0.1976, max: 1.0327, mean: 0.3171\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Validating Wasserstein GAN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Instantiate Wasserstein GAN model\n",
        "# Using a smaller num_epochs for quicker validation, adjust as needed\n",
        "wasserstein_gan_model = WassersteinGAN(length=length, num_channels=num_channels, latent_dim=64, hidden_dim=128, lr=0.00005, n_critic=5, clip_value=0.01)\n",
        "print(f\"Wasserstein GAN Model instantiated: {wasserstein_gan_model}\")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting Wasserstein GAN model (this may take a while)...\")\n",
        "wasserstein_gan_model.fit(train_loader, num_epochs=5) # Reduced epochs for testing\n",
        "print(\"Wasserstein GAN model fitting complete.\")\n",
        "\n",
        "# Generate samples\n",
        "num_generated_samples = 100\n",
        "wasserstein_gan_generated_data = wasserstein_gan_model.generate(num_generated_samples)\n",
        "print(f\"Generated Wasserstein GAN data shape: {wasserstein_gan_generated_data.shape}\")\n",
        "\n",
        "# Validation checks\n",
        "assert wasserstein_gan_generated_data.shape == (num_generated_samples, length, num_channels), \\\n",
        "    f\"Wasserstein GAN: Generated data shape mismatch. Expected ({num_generated_samples}, {length}, {num_channels}), got {wasserstein_gan_generated_data.shape}\"\n",
        "print(\"Wasserstein GAN: Generated data shape is correct.\")\n",
        "\n",
        "print(f\"Wasserstein GAN: Generated data min: {wasserstein_gan_generated_data.min():.4f}, max: {wasserstein_gan_generated_data.max():.4f}, mean: {wasserstein_gan_generated_data.mean():.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
