# ğŸš€ Unified Benchmark for Synthetic Data Generation in Financial Time Series (SDGFTS)

> A unified, reproducible benchmark for evaluating synthetic time series generators in finance. All results, metrics, and experiment outputs are automatically tracked and organized using [MLFlow](https://mlflow.org/).

---

## âš¡ Quickstart

### 1. ğŸ› ï¸ Installation

- Python: 3.9 or newer (recommended)
- Install all dependencies:
  ```bash
  pip install -r requirements.txt
  ```

### 2. ğŸ“¥ Download Dataset

Fetch the required (Google stock, 5 years daily) dataset:
```bash
python src/data_downloader.py
```

This will save the data as `data/raw/GOOG/GOOG.csv`.

### 3. â–¶ï¸ Run the Benchmark

Execute the full benchmark and get all evaluation metrics, synthetic data, and logs:
```bash
python src/evaluator.py
```

**What happens:**
- âš™ï¸ **Data Preprocessing**:
  - **Non-parametric models**: The data is segmented into overlapping sub-sequences of shape `(R, l, N)` where `R` is the number of sequences, `l` is the sequence length, and `N` is the number of features.
  - **Parametric models**: The original time series is used without segmentation, resulting in data of shape `(l, N)`.
- ğŸ¤– Several generative models (both parametric and non-parametric) are trained.
- ğŸ§¬ Each model generates exactly **500 samples**.
- ğŸ“Š All taxonomy metrics (fidelity, diversity, efficiency, and stylized facts) are computed.
- Results are:
  - ğŸ–¥ï¸ Printed in the console.
  - ğŸ“ Saved to a detailed JSON file in the results directory.
  - ğŸ“¦ Tracked as an experiment in **MLFlow** with all parameters, scores, and output artifacts.

#### Customizing runs:
- âœï¸ Edit `dataset_config` and `models_config` dictionaries in [`src/evaluator.py`](src/evaluator.py) to change paths, sample counts, model parameters, etc.

### 4. ğŸ“Š Viewing Results in MLFlow

After you run the benchmark, use MLFlowâ€™s UI to explore and compare your experiments:

1. Start the MLFlow tracking UI (in your project root):
   ```bash
   mlflow ui
   ```
2. Visit [http://localhost:5000](http://localhost:5000) in your browser.
3. For each experiment/model, youâ€™ll see:
   - ğŸ“ Parameters/configurations
   - â±ï¸ Training time, generation time (for 500 samples)
   - ğŸ“ˆ All computed metrics (Fidelity, Diversity, Efficiency, Stylized Facts)
   - ğŸ“ Downloadable output artifacts (e.g., metrics JSON, visualization plots)
4. Use MLFlow to compare models across any metric, check plots, and download results.

---

## ğŸ—‚ï¸ Project Structure

```
Unified-benchmark-for-SDGFTS-main/
  â”œâ”€ data/                       # Raw and preprocessed datasets
  â”œâ”€ notebooks/                  # Interactive explorations, validation, test runs
  â”œâ”€ src/
  â”‚   â”œâ”€ models/                 # Generative model implementations
  â”‚   â”œâ”€ preprocessing/          # Data preprocessing and transformations
  â”‚   â”œâ”€ taxonomies/
  â”‚   â”‚   â”œâ”€ diversity.py        # Diversity metrics (e.g., ICD, ED, DTW)
  â”‚   â”‚   â”œâ”€ efficiency.py       # Efficiency metrics (runtime, memory)
  â”‚   â”‚   â”œâ”€ fidelity.py         # Fidelity/feature metrics (MDD, MD, SDD, KD, ACD, etc.)
  â”‚   â”‚   â””â”€ stylized_facts.py   # Stylized facts metrics (tails, autocorr, volatility)
  â”‚   â”œâ”€ utils/                  # Utility modules, IO, math, paths, etc.
  â”‚   â”œâ”€ data_downloader.py      # Dataset download utility
  â”‚   â””â”€ evaluator.py            # Main pipeline and evaluation runner
  â”œâ”€ configs/                    # Experiment and preprocessing config templates
  â”œâ”€ requirements.txt
  â””â”€ README.md
```

---

## ğŸ¤– Supported Models

The benchmark supports a range of both traditional parametric models and modern deep learning approaches:

<details>
<summary><strong>Parametric Models</strong></summary>

- <kbd>Geometric Brownian Motion (GBM)</kbd>
- <kbd>Ornstein-Uhlenbeck (OU) Process</kbd>
- <kbd>Merton Jump Diffusion (MJD)</kbd>
- <kbd>Double Exponential Jump Diffusion (DEJD)</kbd>
- <kbd>GARCH(1,1)</kbd>

</details>

<details>
<summary><strong>Non-parametric & Deep Learning Models</strong></summary>

- <kbd>Vanilla GAN</kbd>
- <kbd>Wasserstein GAN</kbd>
- <kbd>TimeGAN</kbd>

</details>

> ğŸ› ï¸ All models share a unified interface for training, sample generation, and comprehensive metric evaluation.

---

## ğŸ“ Metrics & Evaluation

### 1. Fidelity Metrics
- **Feature-based Distances**
  - Marginal Distribution Difference (MDD)
  - Mean Difference (MD)
  - Standard Deviation Difference (SDD)
  - Kurtosis Difference (KD)
  - AutoCorrelation Difference (ACD)
- **Visualization**
  - t-SNE Visualization
  - Distribution Comparison Plots

### 2. Diversity Metrics
- **Intra-Class Distance**
  - Euclidean Distance (ED)
  - Dynamic Time Warping (DTW)

### 3. Efficiency Metrics
- **Generation Time** (seconds for generating 500 samples)
- **Memory Usage** (peak MB during generation)

### 4. Stylized Facts Metrics
- **Heavy Tails (Excess Kurtosis)**
- **Lag-1 Autocorrelation of Returns**
- **Volatility Clustering**
- **Long Memory in Volatility**
- **Non-Stationarity Detection**

Refer to `src/taxonomies/` for implementation details and to `src/utils/` for utility functions.

---

## â• How To Add Your Own Model

1. Implement your model in `src/models/` and ensure you inherit from the appropriate base class (`ParametricModel` or `DeepLearningModel`).
2. Register your model in `src/evaluator.py` under the `models` dictionary in `run_complete_evaluation`.
3. Rerun the pipeline and review your new runs in MLFlow!

---

## ğŸ† Results

All results are available in:
- ğŸ–¥ï¸ The console (summary tables per model)
- ğŸ“ `data/evaluation_results/` directory (detailed JSON for each run)
- ğŸ“Š **MLFlow UI** (`mlruns/` directory, browsable at [http://localhost:5000](http://localhost:5000)) â€” all metrics, parameters, and artifacts are logged automatically.

---

## ğŸ‘¥ Contributors

| Name                  | Role                                 | Email                             |
|-----------------------|--------------------------------------|-----------------------------------|
| **Eddison Pham**      | Machine Learning Researcher/Engineer | eddison.pham@mail.utoronto.ca     |
| **Albert Lam Ho**     | Quantitative Researcher              | uyenlam.ho@mail.utoronto.ca       |
| **Yiqing Irene Huang**| Research Supervisor/Professor        | iy.huang@mail.utoronto.ca         |

---

## ğŸ“š More

- For detailed examples and model-by-model usage, see `notebooks/`.
- To report issues or contribute, see the **Contributing** section below.

---

