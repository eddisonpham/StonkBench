import torch.nn as nn
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler

from src.models.base.base_model import DeepLearningModel


def get_rnn_cell(module_name):
    """Basic RNN Cell.
      Args:
        - module_name: gru, lstm
      Returns:
        - rnn_cell: RNN Cell
    """
    assert module_name in ['gru', 'lstm']
    rnn_cell = None
    # GRU
    if module_name == 'gru':
        rnn_cell = nn.GRU
    # LSTM
    elif module_name == 'lstm':
        rnn_cell = nn.LSTM
    return rnn_cell


class Embedder(nn.Module):
    """Embedding network between original feature space to latent space.
        Args:
          - input: input time-series features. Size:(Num, Len, Dim) = (3661, 24, 6)
        Returns:
          - H: embedding features size: (Num, Len, Dim) = (3661, 24, 6)
        """

    def __init__(self, para):
        super(Embedder, self).__init__()
        rnn_cell = get_rnn_cell(para['module'])
        self.rnn = rnn_cell(input_size=para['input_dim'], hidden_size=para['hidden_dim'], num_layers=para['num_layer'],
                            batch_first=True)
        self.fc = nn.Linear(para['hidden_dim'], para['hidden_dim'])
        self.sigmoid = nn.Sigmoid()

    def forward(self, X):
        e_outputs, _ = self.rnn(X)
        H = self.fc(e_outputs)
        H = self.sigmoid(H)
        return H


class Recovery(nn.Module):
    """Recovery network from latent space to original space.
    Args:
      - H: latent representation
      - T: input time information
    Returns:
      - X_tilde: recovered data
    """

    def __init__(self, para):
        super(Recovery, self).__init__()
        rnn_cell = get_rnn_cell(para['module'])
        self.rnn = rnn_cell(input_size=para['hidden_dim'], hidden_size=para['input_dim'], num_layers=para['num_layer'],
                            batch_first=True)
        self.fc = nn.Linear(para['input_dim'], para['input_dim'])
        self.sigmoid = nn.Sigmoid()

    def forward(self, H):
        r_outputs, _ = self.rnn(H)
        X_tilde = self.fc(r_outputs)
        X_tilde = self.sigmoid(X_tilde)
        return X_tilde


class Generator(nn.Module):
    """Generator function: Generate time-series data in latent space.
    Args:
      - Z: random variables
    Returns:
      - E: generated embedding
    """

    def __init__(self, para):
        super(Generator, self).__init__()
        rnn_cell = get_rnn_cell(para['module'])
        self.rnn = rnn_cell(input_size=para['input_dim'], hidden_size=para['hidden_dim'], num_layers=para['num_layer'],
                            batch_first=True)
        self.fc = nn.Linear(para['hidden_dim'], para['hidden_dim'])
        self.sigmoid = nn.Sigmoid()

    def forward(self, Z):
        g_outputs, _ = self.rnn(Z)
        E = self.fc(g_outputs)
        E = self.sigmoid(E)
        return E


class Supervisor(nn.Module):
    """Generate next sequence using the previous sequence.
    Args:
      - H: latent representation
      - T: input time information
    Returns:
      - S: generated sequence based on the latent representations generated by the generator
    """

    def __init__(self, para):
        super(Supervisor, self).__init__()
        rnn_cell = get_rnn_cell(para['module'])
        self.rnn = rnn_cell(input_size=para['hidden_dim'], hidden_size=para['hidden_dim'], num_layers=para['num_layer'] - 1,
                            batch_first=True)
        self.fc = nn.Linear(para['hidden_dim'], para['hidden_dim'])
        self.sigmoid = nn.Sigmoid()

    def forward(self, H):
        s_outputs, _ = self.rnn(H)
        S = self.fc(s_outputs)
        S = self.sigmoid(S)
        return S


class Discriminator(nn.Module):
    """Discriminate the original and synthetic time-series data.
    Args:
      - H: latent representation
      - T: input time information
    Returns:
      - Y_hat: classification results between original and synthetic time-series
    """

    def __init__(self, para):
        super(Discriminator, self).__init__()
        rnn_cell = get_rnn_cell(para['module'])
        self.rnn = rnn_cell(input_size=para['hidden_dim'], hidden_size=para['hidden_dim'], num_layers=para['num_layer'],
                            batch_first=True)
        self.fc = nn.Linear(para['hidden_dim'], 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, H):
        d_outputs, _ = self.rnn(H)
        Y = self.fc(d_outputs)
        Y = self.sigmoid(Y)
        return Y

def extract_time(data):
    """Returns Maximum sequence length and each sequence length.
    Args:
      - data: original data (numpy array or list)
    Returns:
      - time: extracted time information
      - max_seq_len: maximum sequence length
    """
    time = list()
    max_seq_len = 0
    if isinstance(data, np.ndarray):
        # If data is numpy array with shape (N, seq_len) or (N, seq_len, 1)
        if data.ndim == 2:
            max_seq_len = data.shape[1]
            time = [data.shape[1]] * data.shape[0]
        elif data.ndim == 3:
            max_seq_len = data.shape[1]
            time = [data.shape[1]] * data.shape[0]
    else:
        # If data is list of arrays
        for i in range(len(data)):
            seq_len = data[i].shape[0] if isinstance(data[i], np.ndarray) else len(data[i])
            max_seq_len = max(max_seq_len, seq_len)
            time.append(seq_len)
    return time, max_seq_len


def random_generator(batch_size, z_dim, max_seq_len, *T):
    """Random vector generation.
    Args:
      - batch_size: size of the random vector
      - z_dim: dimension of random vector
      - T_mb: time information for the random vector
      - max_seq_len: maximum sequence length
    Returns:
      - Z_mb: generated random vector
    """
    Z_mb = list()
    for i in range(batch_size):
        if not T:
            temp = np.random.uniform(0., 1, [max_seq_len, z_dim])
        else:
            T_mb = T[0]
            temp = np.random.uniform(0., 1, [T_mb[i], z_dim])
        Z_mb.append(temp)
    return Z_mb


def batch_generator(data, time, batch_size):
    """Mini-batch generator.
    Args:
      - data: time-series data
      - time: time information
      - batch_size: the number of samples in each batch
    Returns:
      - X_mb: time-series data in each batch
      - T_mb: time information in each batch
    """
    no = len(data)
    idx = np.random.permutation(no)
    train_idx = idx[:batch_size]

    X_mb = list(data[i] for i in train_idx)
    T_mb = list(time[i] for i in train_idx)

    return X_mb, T_mb

class TimeGAN(DeepLearningModel):
    """
    TimeGAN model for generating synthetic time series.
    
    Input: DataLoader providing batches of shape (batch_size, seq_length) - assumes data is already log returns
    Output: Generated samples of shape (num_samples, generation_length) - outputs log returns
    """
    
    def __init__(
        self,
        seq_len: int = None,
        module: str = 'gru',
        hidden_dim: int = 24,
        num_layer: int = 3,
        batch_size: int = 128,
        print_times: int = 10,
        gamma: float = 1.0,
        learning_rate: float = 1e-4,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        super().__init__()
        
        self.seq_len = seq_len  # Will be inferred from data if None
        self.module = module
        self.hidden_dim = hidden_dim
        self.num_layer = num_layer
        self.batch_size = batch_size
        self.print_times = print_times
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.device = torch.device(device)
        
        # Networks (will be initialized in fit if seq_len is None)
        self.embedder = None
        self.recovery = None
        self.generator = None
        self.discriminator = None
        self.supervisor = None
        
        # Optimizers
        self.optim_embedder = None
        self.optim_recovery = None
        self.optim_generator = None
        self.optim_discriminator = None
        self.optim_supervisor = None
        
        # Preprocessing scaler
        self.scaler = StandardScaler()
        self.scaler_fitted = False
        
        # Training data storage
        self.ori_data = None
        self.ori_time = None
        self.max_seq_len = None
        self.input_dim = 1  # Univariate time series
        
        # Loss functions
        self.MSELoss = torch.nn.MSELoss()
        self.BCELoss = torch.nn.BCELoss()

    def _init_networks(self):
        """Initialize networks if not already initialized."""
        if self.embedder is None:
            para = {
                'module': self.module,
                'input_dim': self.input_dim,
                'hidden_dim': self.hidden_dim,
                'num_layer': self.num_layer
            }
            self.embedder = Embedder(para).to(self.device)
            self.recovery = Recovery(para).to(self.device)
            self.generator = Generator(para).to(self.device)
            self.discriminator = Discriminator(para).to(self.device)
            self.supervisor = Supervisor(para).to(self.device)
    
    def _init_optimizers(self):
        """Initialize optimizers."""
        self.optim_embedder = torch.optim.Adam(self.embedder.parameters(), lr=self.learning_rate)
        self.optim_recovery = torch.optim.Adam(self.recovery.parameters(), lr=self.learning_rate)
        self.optim_generator = torch.optim.Adam(self.generator.parameters(), lr=self.learning_rate)
        self.optim_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=self.learning_rate)
        self.optim_supervisor = torch.optim.Adam(self.supervisor.parameters(), lr=self.learning_rate)
    
    def _prepare_batch(self, batch: torch.Tensor) -> torch.Tensor:
        """Convert batch from (batch_size, seq_len) to (batch_size, seq_len, 1) for RNN."""
        if batch.dim() == 1:
            batch = batch.unsqueeze(0)
        x = batch.to(self.device)
        if x.dim() == 2:
            x = x.unsqueeze(-1)  # (batch_size, seq_len, 1) for RNN
        return x
    
    def _gen_batch(self):
        """Generate training batch from stored data."""
        self.X, self.T = batch_generator(self.ori_data, self.ori_time, self.batch_size)
        self.X = torch.tensor(np.array(self.X), dtype=torch.float32).to(self.device)
        # Random vector generation
        self.Z = random_generator(self.batch_size, self.input_dim, self.max_seq_len, self.T)
        self.Z = torch.tensor(np.array(self.Z), dtype=torch.float32).to(self.device)

    def _batch_forward(self):
        """Forward pass through all networks."""
        self.H = self.embedder(self.X)
        self.X_tilde = self.recovery(self.H)
        self.H_hat_supervise = self.supervisor(self.H)

        self.E_hat = self.generator(self.Z)
        self.H_hat = self.supervisor(self.E_hat)
        self.X_hat = self.recovery(self.H_hat)

        self.Y_real = self.discriminator(self.H)
        self.Y_fake = self.discriminator(self.H_hat)
        self.Y_fake_e = self.discriminator(self.E_hat)

    def _train_embedder(self, join_train=False):
        """Train embedder and recovery networks."""
        self.embedder.train()
        self.recovery.train()
        self.optim_embedder.zero_grad()
        self.optim_recovery.zero_grad()
        self.E_loss_T0 = self.MSELoss(self.X, self.X_tilde)
        self.E_loss0 = 10 * torch.sqrt(self.E_loss_T0)
        if not join_train:
            # E0_solver
            self.E_loss0.backward()
        else:
            # E_solver
            self.G_loss_S = self.MSELoss(self.H[:, 1:, :], self.H_hat_supervise[:, :-1, :])
            self.E_loss = self.E_loss0 + 0.1 * self.G_loss_S
            self.E_loss.backward()
        self.optim_embedder.step()
        self.optim_recovery.step()

    def _train_supervisor(self):
        """Train generator and supervisor networks with supervised loss only."""
        self.generator.train()
        self.supervisor.train()
        self.optim_generator.zero_grad()
        self.optim_supervisor.zero_grad()
        self.G_loss_S = self.MSELoss(self.H[:, 1:, :], self.H_hat_supervise[:, :-1, :])
        self.G_loss_S.backward()
        self.optim_generator.step()
        self.optim_supervisor.step()

    def _train_generator(self, join_train=False):
        """Train generator and supervisor networks."""
        self.optim_generator.zero_grad()
        self.optim_supervisor.zero_grad()
        self.G_loss_U = self.BCELoss(self.Y_fake, torch.ones_like(self.Y_fake))
        self.G_loss_U_e = self.BCELoss(self.Y_fake_e, torch.ones_like(self.Y_fake_e))
        # flatten over batch and time for each feature
        x_real = self.X.view(-1, self.input_dim)     # (batch*seq, feat)
        x_fake = self.X_hat.view(-1, self.input_dim)

        mean_real = torch.mean(x_real, dim=0)
        mean_fake = torch.mean(x_fake, dim=0)
        std_real = torch.std(x_real, dim=0) + 1e-6
        std_fake = torch.std(x_fake, dim=0) + 1e-6

        self.G_loss_V = torch.mean(torch.abs(mean_fake - mean_real)) + \
                torch.mean(torch.abs(std_fake - std_real))

        self.G_loss_S = self.MSELoss(self.H_hat_supervise[:, :-1, :], self.H[:, 1:, :])
        self.G_loss = self.G_loss_U + \
                      self.gamma * self.G_loss_U_e + \
                      torch.sqrt(self.G_loss_S) * 100 + \
                      self.G_loss_V * 100
        if not join_train:
            self.G_loss.backward()
        else:
            self.G_loss.backward(retain_graph=True)

        self.optim_generator.step()
        self.optim_supervisor.step()

    def _train_discriminator(self):
        """Train discriminator network."""
        self.discriminator.train()
        self.optim_discriminator.zero_grad()
        self.D_loss_real = self.BCELoss(self.Y_real, torch.ones_like(self.Y_real))
        self.D_loss_fake = self.BCELoss(self.Y_fake, torch.zeros_like(self.Y_fake))
        self.D_loss_fake_e = self.BCELoss(self.Y_fake_e, torch.zeros_like(self.Y_fake_e))
        self.D_loss = self.D_loss_real + \
                      self.D_loss_fake + \
                      self.gamma * self.D_loss_fake_e
        # Train discriminator (only when the discriminator does not work well)
        self.D_loss.backward()
        self.optim_discriminator.step()

    def fit(self, data_loader, num_epochs: int = 10, *args, **kwargs):
        """
        Train TimeGAN model using the 3-stage training process.
        
        Args:
            data_loader: DataLoader providing batches of shape (batch_size, seq_length)
                        Assumes data is already log returns.
                        Batches are provided as lists: batch = [torch.tensor(batchsize, seq_len)]
            num_epochs: Number of training epochs
        """
        # Collect all batches from DataLoader
        all_batches = []
        for batch in data_loader:
            batch_tensor = batch[0].to(self.device)
            # Ensure shape is (batch_size, seq_len)
            if batch_tensor.dim() == 1:
                batch_tensor = batch_tensor.unsqueeze(0)
            
            # Convert to numpy and ensure 2D shape (batch_size, seq_len)
            batch_np = batch_tensor.cpu().numpy()
            if batch_np.ndim == 1:
                batch_np = batch_np.reshape(1, -1)
            all_batches.append(batch_np)
        
        # Concatenate all batches along batch dimension: (total_samples, seq_len)
        all_data = np.concatenate(all_batches, axis=0)
        
        # Infer seq_len if not provided
        if self.seq_len is None:
            self.seq_len = all_data.shape[1]
            print(f"Inferred sequence length: {self.seq_len}")
        
        # Store original data as list of arrays for batch_generator compatibility
        self.ori_data = [all_data[i] for i in range(len(all_data))]
        self.ori_time, self.max_seq_len = extract_time(all_data)
        self.max_seq_len = max(self.ori_time)
        
        # Preprocess data with StandardScaler
        print("Preprocessing data...")
        all_data_preprocessed = self.scaler.fit_transform(all_data.reshape(-1, 1))
        self.scaler_fitted = True
        
        # Reshape for preprocessing: (num_samples, seq_len) -> (num_samples, seq_len, 1)
        all_data_preprocessed_reshaped = all_data_preprocessed.reshape(len(self.ori_data), self.seq_len, 1)
        self.ori_data = [all_data_preprocessed_reshaped[i] for i in range(len(all_data_preprocessed_reshaped))]
        
        # Initialize networks
        self._init_networks()
        self._init_optimizers()
        
        # Calculate iterations per stage based on num_epochs
        per_print_num = max(1, num_epochs // self.print_times)

        # 1. Embedding network training
        print('Start Embedding Network Training')
        for epoch in range(num_epochs):
            self._gen_batch()
            self._batch_forward()
            self._train_embedder()
            if epoch % per_print_num == 0:
                print(f'epoch: {epoch}/{num_epochs}, e_loss: {np.round(np.sqrt(self.E_loss_T0.item()), 4)}')
        print('Finish Embedding Network Training')

        # 2. Training only with supervised loss
        print('Start Training with Supervised Loss Only')
        for epoch in range(num_epochs):
            self._gen_batch()
            self._batch_forward()
            self._train_supervisor()
            if epoch % per_print_num == 0:
                print(f'epoch: {epoch}/{num_epochs}, g_loss_s: {np.round(np.sqrt(self.G_loss_S.item()), 4)}')
        print('Finish Training with Supervised Loss Only')

        # 3. Joint Training
        print('Start Joint Training')
        for epoch in range(num_epochs):
            # Generator training (twice more than discriminator training)
            for kk in range(2):
                self._gen_batch()
                self._batch_forward()
                self._train_generator(join_train=True)
                self._batch_forward()
                self._train_embedder(join_train=True)
            # Discriminator training
            self._gen_batch()
            self._batch_forward()
            self._train_discriminator()

            # Print multiple checkpoints
            if epoch % per_print_num == 0:
                print(f'epoch: {epoch}/{num_epochs}, '
                      f'd_loss: {np.round(self.D_loss.item(), 4)}, '
                      f'g_loss_u: {np.round(self.G_loss_U.item(), 4)}, '
                      f'g_loss_s: {np.round(np.sqrt(self.G_loss_S.item()), 4)}, '
                      f'g_loss_v: {np.round(self.G_loss_V.item(), 4)}, '
                      f'e_loss_t0: {np.round(np.sqrt(self.E_loss_T0.item()), 4)}')
        print('Finish Joint Training')
        print('TimeGAN training complete!')
        
    def generate(self, num_samples: int, generation_length: int, seed: int = 42, *args, **kwargs) -> torch.Tensor:
        """
        Generate synthetic log returns.
        
        Args:
            num_samples: Number of samples to generate
            generation_length: Length of each generated sample
            seed: Random seed
        
        Returns:
            Generated log returns of shape (num_samples, generation_length)
        """
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        if self.generator is None:
            raise RuntimeError("Model must be trained before generating samples.")
        
        if not self.scaler_fitted:
            raise RuntimeError("Scaler must be fitted before generating samples.")
        
        self.generator.eval()
        self.supervisor.eval()
        self.recovery.eval()
        
        # Generate using the stored ori_time for sequence lengths
        if self.ori_time is None:
            seq_lens = [generation_length] * num_samples
        else:
            # Use same distribution of sequence lengths as training data
            seq_lens = np.random.choice(self.ori_time, size=num_samples, replace=True)
            seq_lens = [min(s, generation_length) for s in seq_lens]
        
        with torch.no_grad():
            all_generated = []
            for batch_idx in range(0, num_samples, self.batch_size):
                batch_size_current = min(self.batch_size, num_samples - batch_idx)
                batch_seq_lens = seq_lens[batch_idx:batch_idx + batch_size_current]

                # Generate random Z
                Z_batch = random_generator(batch_size_current, self.input_dim,
                                        self.max_seq_len, batch_seq_lens)
                Z_batch = torch.tensor(np.array(Z_batch), dtype=torch.float32).to(self.device)

                # TimeGAN forward
                E_hat = self.generator(Z_batch)
                H_hat = self.supervisor(E_hat)
                X_hat = self.recovery(H_hat)                    # (batch, seq_len, 1)

                # ---- FIXED INVERSE TRANSFORM ----
                X_hat_np = X_hat.cpu().detach().numpy()        # (batch, seq_len, 1)
                b, t, f = X_hat_np.shape
                X_hat_np_2d = X_hat_np.reshape(-1, 1)  # (batch*seq_len, 1)
                X_hat_denorm_2d = self.scaler.inverse_transform(X_hat_np_2d)
                X_hat_denorm = X_hat_denorm_2d.reshape(b, t, f)

                # ---------------------------------

                # Extract sequences safely
                for i in range(batch_size_current):
                    seq_len_actual = min(generation_length, X_hat_denorm.shape[1])
                    seq = X_hat_denorm[i, :seq_len_actual, 0]
                    all_generated.append(seq)


            generated_array = np.array(all_generated)
            generated_array = generated_array - generated_array.mean()

            return torch.tensor(generated_array, dtype=torch.float32)
