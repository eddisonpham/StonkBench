\documentclass[sigconf]{acmart}
\usepackage{enumitem}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% \setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}



\begin{document}


\title{Stonk-Bench: Unified Benchmark for Synthetic Data Generation for Financial Time Series}



\author{Uyen Lam Ho}
\authornote{Both authors contributed equally to this research.}
\email{uyenlam.ho@mail.utoronto.ca}
\author{Eddison Pham}
\authornotemark[1]
\email{eddison.pham@mail.utoronto.ca}
\affiliation{%
  \institution{University of Toronto}
  \city{Toronto}
  \state{Ontario}
  \country{Canda}
}



\renewcommand{\shortauthors}{Ho and Pham}

\begin{abstract}
  Synthetic Data Generation (SDG) has become increasingly important in financial applications, particularly for 
  generating Financial Time Series (FTS) data. However, evaluation of synthetic FTS remains inconsistent, with ad 
  hoc methods limiting fair and reliable comparisons. To address this, we introduce \textit{StonkBench}, 
  a unified benchmark for synthetic data generation in financial time series (SDGFTS) that integrates four evaluation
  taxonomies: Fidelity, Diversity, Efficiency, and Utility, each comprising metrics specifically designed for forecasting tasks. 
  Utility evaluations leverage portfolio assessments generated by a deep hedger. StonkBench builds upon Stenger's taxonomies 
  and TSGBench's SDG framework, combining an enhanced standardization pipeline tailored for FTS, a comprehensive set of 
  evaluation metrics, and an open-source framework that unifies evaluation procedures, enabling consistent, 
  reproducible comparisons across datasets and models. Additionally, our benchmark evaluates statistical properties 
  observed in financial time series, ie, stylized facts, including volatility clustering, fat tails, 
  absence of autocorrelations in returns, and leverage effects. By formalizing SDGFTS evaluation, StonkBench 
  establishes a gold standard that facilitates the development of more effective generation methods, 
  supports fair benchmarking across studies, and advances both current and future financial forecasting and analysis.
\end{abstract}



\keywords{
    Synthetic Data Generation, 
    Financial Time Series,
    Benchmarking
}
\begin{teaserfigure}
  \centering
  \includegraphics[width=\textwidth,height=5.5cm,keepaspectratio,trim=0 150 0 150,clip]{tse-1981.png}
  \caption{Toronto Stock Exchange, Toronto, 1981}
  \Description{A panoramic view of the Toronto Stock Exchange building in 1981.}
  \label{fig:teaser}
\end{teaserfigure}


\maketitle

\section{Introduction}
  Synthetic Data Generation (SDG) has emerged as a crucial tool in financial technology, 
  particularly for Financial Time Series (FTS) data \cite{Potluru24}. The ability to generate high-quality 
  synthetic financial data addresses several critical challenges in the field, 
  replicating and simulating fat-tail behaviors observed in financial returns time series and develop back-testing
  procedures for prediction and deep hedging algorithms. \cite{Tanaka25}.

  Despite the growing importance of SDGFTS (Synthetic Data Generation for 
  Financial Time Series), there is a notable lack of standardization in evaluating 
  the quality and effectiveness of these generative models \cite{Stenger24}. Current evaluation methods 
  vary widely across studies, making it difficult to compare different approaches objectively 
  and determine their relative strengths and weaknesses.

  Our research addresses this gap by introducing a comprehensive benchmark framework that 
  encompasses:
  \begin{itemize}
      \item Statistical fidelity measures for comparing synthetic and real FTS
      \item Utility metrics that assess the practical value of synthetic data in 
      downstream tasks
      \item Performance evaluation across multiple SDGFTS models and datasets that is applicable to 
      many real-world financial applications.
  \end{itemize}

  By analyzing the results from our MLflow experiments and applying our unified benchmark, 
  we aim to provide clear guidelines for evaluating SDGFTS models and establish a standard 
  for future research in this domain.

  \subsection{Limitations in the SDGFTS Literature}
    Among other things, we observed that there is currently no universal, generally accepted approach 
    to evaluating synthetic time series\cite{Stenger24}; this issue extends beyond FTS to generative 
    frameworks like GANs as a whole \cite{wang20gan}. 
    Many evaluation measures are insufficiently defined and lack public implementations, making reuse 
    and reproduction troublesome and error-prone \cite{Stenger24}. This presents a challenge 
    unique to the generation task compared to areas like time series forecasting or classification 
    \cite{Stenger24}.
    Hence, future research would immensely benefit from a widely accepted, reasonably sized set of 
    qualified measures for the central evaluation criteria. Here are our observations on the current limitations
    in SDGFTS research and evaluation practices:
    \begin{enumerate}[label=\textbf{[L\arabic*]}]
      \item \textbf{Inconsistent Evaluation Metrics:} Different studies employ varying metrics, 
      hindering direct comparisons between models. For instance, some focus solely on statistical 
      similarity, while others emphasize utility in downstream tasks \cite{Ang23}.

      \item \textbf{Lack of Standardized Datasets:} The absence of common benchmark datasets 
      leads to evaluations on disparate data, making it challenging to assess model performance 
      uniformly \cite{Stenger24} in terms of both asset classes, data granularity, and data length.

      \item \textbf{Limited Scope of Evaluation:} Many evaluations concentrate on a narrow set 
      of criteria, such as marginal distributions, neglecting other important aspects like temporal 
      dependencies and multivariate relationships \cite{Boursin22}.

      \item \textbf{Reproducibility Issues:} The lack of open-source implementations and detailed 
      evaluation protocols impedes reproducibility and validation of results across different studies 
      \cite{Stenger24}. It is important to note that most model papers did not provide code for their evaluations.
    \end{enumerate}

  \subsection{Our Contributions}
    To put it in simple terms, we are adopting the time series evaluation taxonomy outlined from 
    Stenger et al. \cite{Stenger24}, develop a comprehensive and unified SDG benchmark expanded from 
    the works of Ang et al. \cite{Ang23} in specifics to FTS by incorporating a utility evaluation 
    framework inspired by Boursin et al. \cite{Boursin22} through portfolio evaluations generated by 
    a deep hedger.
    Our key contributions include:
    \begin{enumerate}[label=\textbf{[C\arabic*]}]
      \item \label{contribute:1} \textbf{Consolidated Evaluation Framework:}
        We systematically review and consolidate evaluation methods from leading papers (models and surveys)
        in SDG and financial time series \cite{Stenger24, Ang23, Boursin22}, creating a 
        comprehensive assessment toolkit based on established practices.

      \item \label{contribute:2} \textbf{Unified Statistical and Utility Measures:}
        For the first time, we integrate both statistical fidelity metrics and practical 
        utility measures in a single SDGFTS benchmark, providing a more complete 
        evaluation of synthetic data quality.

      \item \label{contribute:3} \textbf{Open Benchmark Platform:}
        We deliver an open-source benchmark framework for the research community, 
        aiming to establish a gold standard for SDGFTS model evaluation and 
        comparison.
    \end{enumerate}

\section{Preliminaries}
  \subsection{Problem Definition}
  Let us formally define the Synthetic Data Generation problem for Financial Time Series (FTS).  
  Consider a multivariate financial time series $\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_T)^T \in \mathbb{R}^{T \times N}$ of length $T$ with $N$ features (e.g., OHLC prices).  
  We extract overlapping subseries of length $L$ using a sliding window with stride $S$, denoted as $\mathbf{x}_i = (\mathbf{x}_{i,1}, \ldots, \mathbf{x}_{i,L}) \in \mathbb{R}^{L \times N}$ for $i = 1, \ldots, R=\lfloor\frac{T-L}{S}\rfloor +1$, where each $\mathbf{x}_{i,j}$ is the $N$-dimensional feature vector at time step $j$ within the $i$-th subseries.  
  
  Let $\mathcal{X} = \{\mathbf{x}_1, \ldots, \mathbf{x}_R\}$ denote the set of $R$ subseries extracted from the original time series, with underlying distribution $p_{\mathcal{X}}$.  
  The objective of synthetic data generation for financial time series is to generate a synthetic set of subseries 
  $\mathbf{\hat{X}} = \{\mathbf{\hat{x}}_1, \ldots, \mathbf{\hat{x}}_K\}$ of arbitrary size $K$, 
  such that the distribution $p_{\mathbf{\hat{X}}}$ closely approximates $p_{\mathcal{X}}$, while preserving the key 
  temporal and statistical characteristics of the original series, including autocorrelation structure, volatility 
  clustering, and distributional moments (mean, variance, skewness, and kurtosis).
  
  \subsection{Scope of Project}
    \subsubsection{Scope of Methods}
      Our benchmark encompasses a diverse range of SDGFTS methods, from traditional parametric approaches 
      to modern deep learning architectures. We selected models based on three main criteria:
      (1) proven industry adoption, (2) research community acceptance, and (3) recent methodological innovations.

      Our evaluation includes classical parametric models, which rely on predefined statistical distributions 
      and assumptions about the underlying data generation process. These models have been extensively used 
      in financial institutions for their interpretability and theoretical foundations. 

      We also evaluate modern non-parametric approaches, particularly deep learning-based models that learn 
      the data distribution directly from observations without assuming a specific form. These include 
      generative adversarial networks, variational autoencoders, and diffusion models, representing the 
      cutting edge in synthetic data generation.

      \subsubsection{Scope of Datasets}
        We evaluate models on diverse financial datasets covering multiple asset classes and temporal granularities, 
        ranging from high-frequency (1-minute, 5-minute) to low-frequency (daily) resolutions. 
        This range enables the assessment of generative performance across varying market dynamics and forecasting horizons.  
        \begin{itemize}
            \item \textbf{Stock market indices:} S\&P~500, NASDAQ, and TSX.
            \item \textbf{Individual equities:} Price series from major exchanges (NYSE, NASDAQ, TSX).
            \item \textbf{Foreign exchange (Forex):} Major trading pairs (e.g., EUR/USD, USD/JPY).
        \end{itemize}
        
      \subsubsection{Scope of Evaluation Taxonomical Criteria}
        Our evaluation framework adopts the taxonomy proposed by Stenger et al. as an organizing structure, 
        while drawing specific evaluation metrics primarily from TSGBench, which provides an existing and well-established benchmark for general synthetic time series benchmarking. 
        Although Stenger et al.\ introduce an extensive set of potential measures, our selection focuses on metrics that are 
        both widely used in recent synthetic time series benchmarks and empirically meaningful for financial applications. 
        Our reformulation leverages Stenger's taxonomy to provide a coherent organizational hierarchy, particularly Fidelity, Diversity, Efficiency, and Utility, while ensuring that each chosen metric remains interpretable, 
        computationally feasible, and directly comparable to prior benchmarks. 
        By grounding our framework in the pragmatic evaluation design of TSGBench and aligning it with 
        Stenger’s structured taxonomy, we ensure both methodological rigor and cross-study consistency in SDGFTS assessment.
    

\section{Overview of Synthetic FTS Methods}
    We categorize synthetic financial time series generation methods into two main families: classical parametric (stochastic) models, which rely on explicit assumptions about the data-generating process, and modern non-parametric (deep learning) models, which learn complex dependencies directly from data without predefined functional forms. The following subsections provide a detailed overview of the methods included in our benchmark.


  \subsection{Classical Stochastic (Parametric) Methods}
    Parametric models assume a specific functional form for the data-generating process, characterised by 
    a finite set of parameters. These models are interpretable and analytically tractable, allowing for 
    closed-form solutions in many cases. However, they may struggle to capture complex stylised facts 
    or high-dimensional dependencies beyond their structural assumptions. Below we outline several 
    widely-used parametric models in FTS.\\

      \begin{table}[h]
        \centering
        \caption{Common notation and variables.}
        \label{tab:notation}
        \begin{tabular}{ll}
        \toprule
        \textbf{Symbol} & \textbf{Description} \\
        \midrule
        \( S_t \) & Asset price \\
        \( X_t = \ln S_t \) & Log-price \\
        \( r_t = \ln(S_t / S_{t-1}) \) & Log-return \\
        \( \mu \) & Drift \\
        \( \sigma \) & Volatility \\
        \( W_t \) & Standard Brownian motion \\
        \( \Delta t \) & Discrete time step \\
        \bottomrule
        \end{tabular}
      \end{table}
        
        

      \begin{enumerate}[label=\textbf{[A\arabic*]}]
        \item \label{alg:gbm} \textbf{Geometric Brownian Motion (GBM).} 
        The price process follows a continuous-time stochastic process with constant drift and volatility. 
        Model-specific parameters are \(\mu\) and \(\sigma\).
        \begin{equation}
        dS_t = \mu S_t\,dt + \sigma S_t\,dW_t.
        \end{equation}
        The GBM parameters are estimated from historical log returns: the drift $\mu$ is set to the sample mean and the volatility $\sigma$ is set to the sample standard deviation. This corresponds to maximum likelihood estimation under the assumption that log returns are normally distributed.


        \item \label{alg:ou} \textbf{Ornstein–Uhlenbeck (OU).} 
        A mean-reverting Gaussian process for log-prices or spreads. Model-specific parameters are 
        \(\theta>0\) (reversion rate), \(\mu\) (long-term mean), and \(\sigma\) (volatility).
        \begin{equation}
        dX_t = \theta(\mu - X_t)\,dt + \sigma\,dW_t.
        \end{equation}
        The OU process parameters are estimated from historical log-prices by fitting a discrete-time AR(1) model $X_{t+1} = \phi X_t + c + \varepsilon_t$ and mapping the coefficients to continuous-time parameters. The mean-reversion rate $\theta$, long-term mean $\mu$, and volatility $\sigma$ are derived from $\phi$, $c$, and the residual standard deviation, corresponding to statistical estimation from data.


        \item \label{alg:mjd} \textbf{Merton Jump Diffusion (MJD).}  
        Extends GBM by incorporating normally-distributed jumps. Model-specific parameters are 
        \(\lambda\) (jump intensity) and \(\mu_j, \sigma_j\) (mean and standard deviation of jump sizes):
        \begin{align}
        \frac{dS_t}{S_{t^-}} &= \mu\,dt + \sigma\,dW_t + (e^Z-1)\,dN_t, \\
        Z &\sim \mathcal{N}(\mu_j, \sigma_j^2), \quad N_t \sim \text{Poisson}(\lambda t). \nonumber
        \end{align}
        
        Jumps in the log-returns $r_t$ are identified as returns exceeding a threshold of $k$ standard deviations from the mean, and the jump intensity $\lambda$ is estimated as
        \[
        \lambda = \frac{\text{number of jumps}}{T}.
        \]
        
        The jump size mean and variance are computed from the detected jumps:
        \begin{align}
        \mu_J &= \frac{1}{n_{\text{jumps}}} \sum_{\text{jumps}} r_t, \\
        \sigma_J^2 &= \frac{1}{n_{\text{jumps}}-1} \sum_{\text{jumps}} (r_t - \mu_J)^2.
        \end{align}
        
        The diffusion volatility $\sigma$ is obtained by removing the jump contribution from the total variance per unit time:
        \[
        \sigma = \sqrt{\frac{\mathrm{Var}(r_t)}{\delta t} - \lambda (\mu_J^2 + \sigma_J^2)},
        \]
        and the drift $\mu$ is corrected for the expected jump contribution:
        \[
        \mu = \frac{\mathbb{E}[r_t]}{\delta t} - \lambda \mu_J.
        \]
        

        \item \label{alg:dejd} \textbf{Double-Exponential Jump Diffusion (DEJD).}  
        A jump-diffusion model with asymmetric double-exponential jump sizes. Model-specific parameters are 
        \(p\) (probability of upward jump) and \(\eta_1, \eta_2\) (exponential parameters for upward/downward jumps):
        \begin{align}
        \frac{dS_t}{S_{t^-}} &= \mu\,dt + \sigma\,dW_t + (e^Y-1)\,dN_t,\\
        Y &=
        \begin{cases}
        \mathrm{Exp}(\eta_1), &\text{with probability } p,\\[2pt]
        -\mathrm{Exp}(\eta_2), &\text{with probability } 1-p.
        \end{cases} \nonumber
        \end{align}
        
        Jumps in the log-returns $r_t$ are identified using a robust threshold based on the median absolute deviation, and the jump intensity $\lambda$ is estimated as
        \[
        \lambda = \frac{\text{number of jumps}}{T},
        \]
        corresponding to the compound Poisson process $N_t$.
        
        The jump size distribution $Y$ is separated into positive and negative jumps to estimate the double-exponential parameters:
        \begin{align}
        p &= \frac{\text{number of positive jumps}}{\text{total jumps}}, \\
        \eta_1 &= \frac{1}{\text{mean of positive jumps}}, \\
        \eta_2 &= \frac{1}{-\text{mean of negative jumps}}.
        \end{align}
        
        Finally, the drift $\mu$ and diffusion volatility $\sigma$ of the continuous part are corrected for the expected jump contribution:
        \begin{align}
        \mu &= \frac{\mathbb{E}[r_t]}{\delta t} - \lambda \, \mathbb{E}[Y], \\
        \sigma &= \sqrt{\frac{\mathrm{Var}(r_t)}{\delta t} - \lambda \, \mathbb{E}[Y^2]},
        \end{align}
        with
        \begin{align}
        \mathbb{E}[Y] &= \frac{p}{\eta_1} - \frac{1 - p}{\eta_2}, \\
        \mathbb{E}[Y^2] &= 2 \left( \frac{p}{\eta_1^2} + \frac{1 - p}{\eta_2^2} \right).
        \end{align}
        
       \item \label{alg:garch} \textbf{GARCH(1,1).}  
        A discrete-time conditional heteroskedastic model for returns. Model-specific parameters are 
        \(\omega>0,\, \alpha\ge0,\, \beta\ge0\) (GARCH coefficients) and \(\mathcal{D}\) (innovation distribution, usually standard Normal or Student-$t$). The model captures volatility clustering via time-varying conditional variance:
        \begin{align}
        r_t &= \sigma_t z_t, \quad z_t \sim \mathcal{D}, \\
        \sigma_t^2 &= \omega + \alpha r_{t-1}^2 + \beta \sigma_{t-1}^2. \nonumber
        \end{align}
        The parameters \((\omega, \alpha, \beta)\) are estimated via maximum likelihood from the observed log-return series.
        
        
      \end{enumerate}


  \subsection{Deep Learning (Non-parametric) Methods}
    Non-parametric models, in this context, refer to generative (often implicit) models that do not assume a 
    fixed parametric form for the underlying data-generating process. Instead, they learn latent structures 
    directly from data through flexible architectures such as neural networks, enabling the modeling of highly 
    nonlinear, high-dimensional, and multi-modal dependencies (e.g., multivariate financial time series with 
    interactions across assets, time horizons, and features). While these models trade off interpretability and 
    analytical tractability, they offer substantially greater expressive power and flexibility. 
    This expressiveness, however, comes at the cost of increased data requirements, more intensive 
    hyperparameter tuning, and higher computational demands. 

    We categorize these approaches into three major families of deep generative models: 
    Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models.
    

  \subsubsection{Diffusion Models}
    Diffusion models (or score-based generative models) have emerged as powerful methods for modeling complex 
    data distributions via a forward (noise-adding) process and a learned reverse (denoising) process 
    \cite{ho2020denoising}. In the forward direction, one gradually corrupts a data sample 
    \(x_0\) into noise via a Markov chain:
   
    \begin{align}
      q(x_{1:T}\mid x_0) &= \prod_{t=1}^T q(x_t \mid x_{t-1}), \\
      q(x_t \mid x_{t-1}) &= \mathcal{N}\left(x_t; \sqrt{1-\beta_t}\,x_{t-1}, \beta_t I\right). \nonumber
    \end{align}
   
    
    The reverse (generative) model is parameterized as
    \[
      p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\bigl(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)\bigr),
    \]
    where \(\mu_\theta\) and \(\Sigma_\theta\) are often expressed via a neural network that estimates the 
    score \(\nabla_{\theta} \log p_\theta(x_t)\). A common training objective is the simplified denoising score-matching 
    loss:
    \[
      \mathbb{E}_{t, x_0, \epsilon}\Bigl[ \bigl\| \epsilon - \epsilon_\theta(x_t, t)\bigr\|^2 \Bigr],
    \]
    where \(x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon\).
    
    In finance/time-series, diffusion models have recently been adapted to generate synthetic asset paths by 
    converting multivariate time series into suitable representations (e.g. wavelet-transformed images) and 
    then sampling via reverse diffusion \cite{Tanaka25}. For instance, 
    Takahashi and Mizuno shows that such an approach can 
    reproduce key statistical properties of financial data (e.g.\ volatility clustering, tail behavior) 
    \cite{Takahashi24}.  
    Another relevant work is “A Financial Time Series Denoiser Based on Diffusion Model”, which shows 
    how diffusion can help improve downstream predictability and reduce noise in financial signals 
    \cite{wang24denoiser}.  
    
    \subsubsection{Variational Autoencoders (VAEs)}
    Variational Autoencoders \cite{kingma2022vae} are latent-variable generative 
    models that posit a probabilistic encoder \(q_\phi(z \mid x)\) and decoder \(p_\theta(x \mid z)\). 
    One maximizes the evidence lower bound (ELBO):
    \[
    \mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z \mid x)}\bigl[\log p_\theta(x \mid z)\bigr] - 
    \mathrm{KL}\bigl(q_\phi(z \mid x)\,\Vert\,p(z)\bigr).
    \]
    Often one uses a standard Gaussian prior \(p(z) = \mathcal{N}(0, I)\). The encoder–decoder structure 
    allows sampling by first drawing \(z \sim p(z)\), then generating \(x \sim p_\theta(x \mid z)\).
    
    In financial time-series generation, specialized versions such as the Time-Causal VAE (TC-VAE) have 
    been proposed to enforce causality in the encoding/decoding of temporal data. For instance, 
    Acciaio et al.\ (2024) propose TC-VAE, which ensures that generated paths respect temporal causality 
    and show that the model reproduces stylized facts (e.g., heavy tails, volatility clustering) on real 
    markets \cite{acciaio2024vae}. Because VAEs provide a principled latent representation, 
    they are useful in finance to model underlying latent drivers of asset dynamics and to sample 
    coherent trajectories.
    
    \subsubsection{Generative Adversarial Networks (GANs)}
    Generative Adversarial Networks \cite{goodfellow2014gan} approach generation as a 
    two-player game between a generator \(G\) and a discriminator \(D\). The discriminator is trained 
    to maximize the probability of correctly classifying real data and generated data, with loss
    \[
    L_D = -\mathbb{E}_{x \sim p_{\text{data}}}\bigl[\log D(x)\bigr] - \mathbb{E}_{z \sim p(z)}\bigl[\log (1 - D(G(z)))\bigr].
    \]
    The generator aims to fool the discriminator and is trained to minimize
    \[
    L_G = -\mathbb{E}_{z \sim p(z)}[\log D(G(z))].
    \]
    Many GAN variants, such as Wasserstein GAN or Least Squares GAN, modify these loss functions to promote more stable training and address issues like mode collapse.
    
    In financial data synthesis, GANs have been widely used to generate realistic synthetic time series. For example, GAN-based financial data generation models (often using WGAN or improvements) show that one can improve the authenticity and predictive ability of generated financial statements or time series \cite{qi2025gan_financial}. Another example is VRNNGAN, which uses a recurrent VAE as the generator and a recurrent discriminator to capture temporal dependencies in synthetic sequence generation \cite{lee2022vrnngan}. GANs are relevant in finance because they can capture complex joint distributions and dependencies in multivariate time-series without requiring explicit likelihood models.
    
  \subsection{Miscellaneous}
  This section consists of models that are classified as neither parametric nor non-parametric.
  \begin{enumerate}[label=\textbf{[A\arabic*]}]
        \item \label{alg:blockbootstrap} \textbf{Block Bootstrap (non-parametric).} 
        A resampling technique that preserves short-range dependence by sampling contiguous blocks of returns. 
        Although not parametric, it is included here for completeness and baseline comparison.
      \end{enumerate}


\section{Stonk Bench Architecture}
  \subsection{Datasets and Preprocessing}
    The preprocessing procedure builds upon the TSGBench pipeline, which standardizes segmentation, 
    normalization, and train/test splitting of multivariate time series. Several modifications were 
    introduced to better accommodate financial time series and stochastic models.

    \subsubsection{Data type.} Stock price data (e.g., AAPL) at daily frequency with OHLC columns
    (Open, High, Low, Close) are used. Let \(X \in \mathbb{R}^{L \times N}\) denote a multivariate time series 
    with \(N=4\) channels and length \(L\).

    \subsubsection{Transformations.} Raw prices are converted to log-returns per channel via

    \[ r_t = \log P_t - \log P_{t-1} = \log(P_t / P_{t-1}) \,, \quad r_t \in \mathbb{R}^N, \]
    
    yielding a transformed series of length \(L-1\).

    \subsubsection{Model-specific preprocessing.}  
    The preprocessing follows the general TSGBench framework, with the following adaptations:

    \begin{itemize}
        \item Log returns are used directly instead of TSGBench's min-max normalization, preserving the time serie's distribution along with
        financial properties such as heavy tails and volatility clustering.
        \item For parametric models, the transformed series is divided into contiguous training, 
        validation, and test segments with ratios \((1-\alpha-\beta):\alpha:\beta\), where 
        \(\alpha=0.1\) and \(\beta=0.1\). This ensures that parametric models fit parameters directly 
        to contiguous historical data.
        \item For non-parametric models, overlapping sliding windows \(\mathcal{W} \in \mathbb{R}^{R \times L \times N}\) 
        with stride 1 are extracted. Unlike TSGBench, which employs an autocorrelation-based hard-coded 
        window of 125 (often yielding a size of 1 for stock data due to fast autocorrelation decay), 
        the window length \(L\) is determined via the partial autocorrelation function (PACF). Specifically, 
        PACF is computed for each channel up to \(\lfloor 10\log_{10} n \rfloor\) lags, and the lag corresponding 
        to the maximum significant PACF peak across channels is selected, resulting in \(L=13\). This approach 
        captures relevant temporal dependencies within the data.
        \item The dataset is split into train, validation, and test sets while preserving temporal order to prevent future information from leaking into the past. Only the training set is shuffled during model training to improve convergence. In contrast, TSG-Bench shuffles time series samples before splitting, which introduces data leakage. Our approach ensures strictly forward-looking evaluation for realistic forecasting.
        proper evaluation, consistent with TSGBench.
    \end{itemize}

    \subsubsection{Data loaders.} For non-parametric models, mini-batches are generated from the windowed 
    data \(\mathcal{W}\) using independent fixed seeds for training, validation, and test sets, 
    enabling reproducible epoch-wise evaluation. Training batches are shuffled, while validation and test 
    sets maintain sequential ordering. To ensure fair comparison, parametric models generate sequences 
    of identical length \(L\) and matching sample count to the windowed data used by non-parametric models.


  \subsection{Statistical Evaluation Measures}
    All metrics below are applied \textbf{per channel}, i.e., each univariate time series (OHLC column) is evaluated independently. For multivariate data, results are reported as channel-wise statistics (e.g., mean or distribution across channels).
    
    \subsubsection{Feature-based Distance Measures}
    These metrics quantify how well synthetic data captures key marginal and second-order properties of real data.
    
    \begin{enumerate}[label=\textbf{[M\arabic*]}]
      \item \label{metric:mdd} \textbf{Marginal Distribution Difference (MDD):}  
      Distance between real and synthetic marginals (e.g., 1-Wasserstein distance) per channel:
      \[
      \begin{aligned}
        \mathrm{MDD}_j &= W_1\big(\widehat{F}^j_{real}, \widehat{F}^j_{synth}\big), \\
        W_1(F, G) &= \int_0^1 \big|F^{-1}(u) - G^{-1}(u)\big|\,du,
      \end{aligned}
      \]
      where \(\widehat{F}^j_{real}\) and \(\widehat{F}^j_{synth}\) are empirical CDFs of channel \(j\).
    
      \item \label{metric:md} \textbf{Mean Difference (MD):}  
      Absolute difference of per-channel means:
      \[
        \mathrm{MD}_j = \big|\mu^j_{real} - \mu^j_{synth}\big|.
      \]
    
      \item \label{metric:sdd} \textbf{Standard Deviation Difference (SDD):}  
      Absolute difference of per-channel standard deviations:
      \[
        \mathrm{SDD}_j = \big|\sigma^j_{real} - \sigma^j_{synth}\big|.
      \]
    
      \item \label{metric:kd} \textbf{Kurtosis Difference (KD):}  
      Absolute difference of per-channel excess kurtosis:
      \[
      \begin{aligned}
      \mathrm{KD}_j &= \big|\kappa^{\mathrm{ex}}(X^j_{\text{real}}) - \kappa^{\mathrm{ex}}(X^j_{\text{synth}})\big|, \\
      \kappa^{\mathrm{ex}}(X^j) &= \frac{\mathbb{E}[(X^j - \mu^j)^4]}{(\sigma^j)^4} - 3.
      \end{aligned}
      \]
    
    
      \item \label{metric:acd} \textbf{AutoCorrelation Difference (ACD):}  
      Absolute difference in lag-1 autocorrelation per channel:
      \[
      \begin{aligned}
        \rho^j(1) &= \frac{\sum_{t=2}^{L} (x^j_t - \bar{x}^j)(x^j_{t-1} - \bar{x}^j)}
                       {\sum_{t=1}^{L} (x^j_t - \bar{x}^j)^2}, \\
        \mathrm{ACD}_j &= \big|\rho^j_{real}(1) - \rho^j_{synth}(1)\big|.
      \end{aligned}
      \]
    \end{enumerate}
    
    \subsubsection{Visualization Methods}
    Visual tools complement numeric metrics for face-validity and communication \cite{Ang23}.
    \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
      \item \label{metric:tsne} \textbf{t-SNE:}  
      2D embeddings of window-level features per channel for real vs. synthetic overlap and cluster structure.
    
      \item \label{metric:dist} \textbf{Distribution:}  
      Channel-wise histograms of returns to visualize empirical distribution.
    \end{enumerate}
    
    \subsubsection{Diversity Metrics}
    Measures that prevent mode collapse, computed per channel. Pairwise distances only consider the same channel across samples.
    
    \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
      \item \label{metric:ed} \textbf{Intra-Class Euclidean Distance (ICD-ED):}  
      \[
        \mathrm{ICD\text{-}ED}^j = \frac{2}{R(R-1)} \sum_{1 \le a < b \le R} \left\| \textbf{x}_a^j - \textbf{x}_b^j \right\|_2,
      \]
      where \(\textbf{x}_a^j \in \mathbb{R}^{L}\) is the \(j\)-th channel of the \(a\)-th sample.
    
      \item \label{metric:dtw} \textbf{Intra-Class Dynamic Time Warping (ICD-DTW):}  
      \[
        \mathrm{ICD\text{-}DTW}^j = \frac{2}{R(R-1)} \sum_{1 \le a < b \le R} d_{\mathrm{DTW}}\bigl(\textbf{x}_a^j, \textbf{x}_b^j\bigr).
      \]
    \end{enumerate}
    
    \subsubsection{Efficiency Assessment}
    \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
      \item \label{metric:gentime} \textbf{Generation Time:}  
      Wall-clock time to generate \(S\) samples:
      \[
        T_{\mathrm{gen}} = t_1 - t_0 \quad (\text{seconds for } S \text{ samples}).
      \]
    \end{enumerate}
    
    \subsubsection{Stylized Facts Verification}
    Canonical stylized facts, measured per channel:
    
    \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
      \item \label{metric:heavytails} \textbf{Heavy Tails:}  
      Per-channel excess kurtosis:
      \[
        \kappa^{\mathrm{ex}}(X^j) = \frac{\mathbb{E}[(X^j - \mathbb{E}[X^j])^4]}{(\mathrm{Var}[X^j])^2} - 3.
      \]
    
      \item \label{metric:autocorr} \textbf{Return Autocorrelation:}  
      Lag-1 autocorrelation per channel, should be close to zero for liquid assets.
    
      \item \label{metric:volclustering} \textbf{Volatility Clustering:}  
      Lag-1 autocorrelation of squared or absolute returns per channel:
      \[
      \begin{aligned}
      \rho^j_{x^2}(1) &= \mathrm{Corr}\big((x^j_t)^2, (x^j_{t-1})^2\big), \\
      \rho^j_{|x|}(1) &= \mathrm{Corr}\big(|x^j_t|, |x^j_{t-1}|\big).
      \end{aligned}
      \]
        
    \end{enumerate}
    
  
  \subsection{Utility Evaluation Measures: Deep Hedging}
    Utility measures are critical for evaluating synthetic data beyond statistical metrics, 
    as they assess the practical value of generated data in real-world financial applications \cite{Boursin22}. 
    Our benchmark incorporates deep hedging as a utility measure for several key reasons:

    \begin{enumerate}[label=\textbf{[U\arabic*]}]
      \item \label{utility:application}\textbf{Real-world Application Testing:}
      Deep hedging provides a concrete way to evaluate how synthetic data performs in 
      actual financial tasks, particularly in derivatives pricing and risk management.

      \item \label{utility:industry}\textbf{Industry-relevant Metrics:}
      By comparing hedging strategies trained on synthetic versus real data, we can 
      assess the practical utility of synthetic data through metrics that matter to 
      financial practitioners, such as replication errors and hedging performance.

      \item \label{utility:validation}\textbf{Model Robustness Validation:}
      Deep hedging helps verify if synthetic data maintains the complex relationships 
      and market dynamics necessary for developing reliable trading strategies.
    \end{enumerate}

    \subsubsection{Deep Hedger Problem Defintion}
      We consider a continuous-time financial market defined on a probability space 
      $(\Omega, \mathcal{F}, \mathbb{P})$, over a finite time horizon $0 < T < \infty$, 
      equipped with a filtration $\mathcal{F} = (\mathcal{F}_t)_{0 \leq t \leq T}$ 
      that represents the evolution of information through time.  
      The market consists of $d + 1$ tradable assets $S = (S^0, \dots, S^d)$, where 
      $S_t^j$ denotes the price of asset $j$ at time $t$. For simplicity, we assume a 
      zero interest rate environment.

      We study the hedging problem associated with a contingent claim delivering a payoff 
      $g(S_T)$ at maturity $T$, where $S_T$ represents the terminal value of the underlying 
      asset vector.  
      The hedging strategy is implemented at a discrete set of times 
      $0 = t_0 < t_1 < \dots < t_{N-1} < t_N = T$.  
      A \textit{self-financing portfolio} is described by a $d$-dimensional $\mathcal{F}_t$
      -adapted process $\Delta_t$, and its terminal wealth, denoted $X_T^{\Delta, p}$, 
      is given by
      \begin{equation}
          X_T^{\Delta, p} = p + \sum_{i=1}^{d} \sum_{j=0}^{N-1} \Delta_{t_j}^i 
          \left( F_{t_{j+1}}^i - F_{t_j}^i \right),
      \end{equation}
      where $p \in \mathbb{R}$ represents the initial premium.

      The objective is to determine the optimal premium and trading strategy 
      $(p^{\text{opt}}, \Delta^{\text{opt}})$ that minimize the expected squared hedging error:
      \begin{equation}
          (p^{\text{opt}}, \Delta^{\text{opt}}) 
          = \operatorname*{Argmin}_{p, \Delta} 
          \mathbb{E} \left[ \big( X_T^{\Delta} - g(S_T) \big)^2 \right].
      \end{equation}

      To address this optimization problem, we employ the global approach proposed by 
      Fécamp \textit{et al.} (2020), chosen for its computational efficiency and scalability.  
      In this framework, the control policy $\Delta$ is approximated by a feed-forward neural 
      network—referred to as a \textit{deep hedger}—which jointly learns the optimal trading 
      strategy and corresponding premium through end-to-end training.
    \subsubsection{Deep Hedger Architecture}
      \begin{table}[h]
        \centering
        \caption{Notations for Deep Hedger Architecture.}
        \label{tab:deephedger-notation}
        \begin{tabular}{ll}
        \toprule
        \textbf{Symbol}     & \textbf{Description} \\
        \midrule
        \( D \)             & Time-series dataset             \\
        \( T \)             & Task                            \\
        \( S \)             & Score                           \\
        \( A \)             & Deep hedger algorithm           \\
        \( \mathcal{A}_n \) & Set of n deep hedger algorithms \\
        \( M \)             & Deep hedger model               \\
        \( \mathcal{M}_{TS} \)   & (SDGFTS) model                  \\
        \bottomrule
        \end{tabular}
      \end{table}

      Our deep hedger architecture centeres around the Train-Synthetic-Test-Real (TSTR)
      evaluation framework \cite{Leznik21}, adapted for financial time series generation.
      We begin with our real FTS dataset \( D\)\textemdash partitioned to a training, validation, 
      and test set respectively as
      \( D_r = \{D_r^{train}, D_r^{validate}, D_r^{test}\} \)\textemdash along with a specific 
      hedging task \(T\) and a performance score \(S\) to evaluate hedging effectiveness, 
      to which we will go indepth in the next subsubsection.
      We consider a deep hedger algorithm \(A\) (a 5-layer perceptron) that takes as 
      input the dataset \(D_r\) and task \(T\),

      In the context of SDGFTS evaluation, we consider a SDGFTS model \( \mathcal{M}_{TS} \) 
      trained on \( D_r^{train} \) and validated with \( D_r^{validate} \) to generate 
      synthetic FTS data \( D_g \).

      Now we can train a deep hedger algorithm \( A \) on the datasets \( D_r^{train} \) and 
      \( D_g^{train} \) respectively, resulting in two deep hedger models
      \( M_r \) and \( M_g \).

      Finally, we evaluate both models on the real test set \( D_r^{test} \) to obtain the
      corresponding performance scores \( s_r \) and \( s_g \).

      The goal of the deep hedger portfolio evaluation is to evaluate the effectiveness of the
      model \( \mathcal{M}_{TS} \) with downstream tasks (deep hedging) by comparing the scores 
      \( s_r \) and \( s_g \). The model \( \mathcal{M}_{TS} \) is considered effective if the 
      the results yielded similar scores, i.e. \( s_r \approx s_g \) \textemdash preferably having 
      higher performance with results of \( s_g > s_r \) \textemdash indicating that the synthetic 
      data generated by \( \mathcal{M}_{TS} \) is useful for training deep hedger models 
      \cite{Stenger24}.

    \subsubsection{Deep Hedger Extentions}
    We proposed two extensions to the standard deep hedger architecture to better suit our 
    benchmark need that drew inspiration from recent literature \cite{Stenger24}.
      \begin{enumerate}[label=\textbf{[E\arabic*]}]
        \item \label{ext:augment} \textbf{Augmented Testing \cite{Stenger24}}  
        To better evaluate the performance of deep hedgers trained on synthetic data, 
        we train our deep hedger \( A \) on a new dataset \( D_{aug} \) that combines
        both real training data \( D_r^{train} \) and synthetic data \( D_g\). In other words,
        we traid model \( M_g \) on \( D_{aug} \) where \( D_{aug} := D_r^{train} \cup D_g\).
        Then we proceed to evaluate scores between models\( M_g \) and \( M_r \) 
        on the real test set, where \( M_r \) i s still trained on only real data \( D_r^{train} \).


        \item \label{ext:algcomparsion} \textbf{Algorithm Comparison \cite{Lin21}}  
        Another extension is to compare multiple deep hedger algorithms indicating to what 
        degree each algorithm performs equally on the generated data relative to the other
        algorithms, compared to their performance on real data.

        Given a set of \( n \) deep hedger algorithms 
        \( \mathcal{A}_n = \{ A_1, \ldots, A_n \} \), we train each algorithm 
        \( A_i \in \ mathcal{A}_n \) on both real training data \( D_r^{train} \) and
        synthetic data \( D_g^{train} \) respectively, resulting in two sets of deep 
        hedger models \(\{M_r^1, M_r^2, \ldots, M_r^n\} \) and \( \{M_g^1, M_g^2, \ldots, M_g^n\} \)
        and two ordered setes of performance scores \( s_r := \{s_r^1, s_r^2, \ldots, s_r^n\} \) and 
        \( s_g := \{s_g^1, s_g^2, \ldots, s_g^n\} \) respectively.

        Finally we compare the relative performance of each algorithm on real data versus synthetic data
        by computing the Spearman's rank correlation coefficient \( r_S \) \cite{Lin21} between 
        the two score sets \( s_r \) and \( s_g \):
        \begin{equation}
          r_S = 1 - \frac{6 \sum_{i=1}^{n} ( \mathrm{rank}(s_r^i) - \mathrm{rank}(s_g^i) )^2}{n(n^2 - 1)},
        \end{equation}
        where \( \mathrm{rank}(s_r^i) \) and \( \mathrm{rank}(s_g^i) \) denote the ranks of
        scores \( s_r^i \) and \( s_g^i \) within their respective sets.

        We interpret a high positive correlation (i.e., \( r_S \) close to 1) as an indication that
        the synthetic data  generated by \( mathcal{M}_{TS} \) preserves the relative performance 
        of different deep hedger algorithms.
      \end{enumerate}
    % \subsubsection{Deep Hedger Portfolio Evaluation}
    %   In the question of how to effectively evaluate the deep hedging utility of synthetic 
    %   financial time series data, we wish to choose scores \( S \) that reflect the practical
    %   utility of the data in real-world hedging tasks.
    %   We propose the following evaluation scores:
    %   \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
    %     \item \label{metric:replicationerror} \textbf{Replication Error}
    %     \item \label{metric:erm} \textbf{ERM}
    %     \item \label{metric:cvar} \textbf{CVaR}
    %   \end{enumerate}

    \section{Results and Analysis}

    In this section, we present a comprehensive evaluation of the synthetic financial time series 
    generated by all models under consideration. The evaluation focuses on multiple statistical, 
    temporal, and diversity metrics to assess how well each model reproduces the characteristics 
    of real financial data. Detailed tables of evaluation results can be found in Appendix~\ref{app:evaluation-tables}, 
    while visualizations including T-SNE embeddings and distribution comparisons are presented 
    in Appendix~\ref{app:visualizations}. A comparative view of evaluation metrics across models 
    is provided in Appendix~\ref{app:metrics-comparison}. 
    
    \subsection{Statistical Evaluation Results}
        Analysis of the evaluation metrics reveals several noteworthy findings. 
        First, the GBM, OU, and GARCH(1,1) model all fail to fully capture 
        key stylized facts of financial time series, particularly extreme 
        events and fat-tailed behavior. This limitation is evident in the 
        kurtosis metrics, where these models substantially underestimate the 
        frequency and magnitude of rare, high-impact returns (Appendix~\ref{app:metrics-comparison}). 
        These shortcomings reflect the inherent assumptions of each model: GBM assumes log-normal 
        returns with constant volatility, OU is mean-reverting with Gaussian noise, and GARCH(1,1), 
        despite modeling conditional heteroskedasticity, does not sufficiently reproduce the tails 
        of the empirical return distribution. Collectively, these models exhibit large deviations 
        in kurtosis and other distributional metrics, highlighting their inability to generate the 
        extreme events observed in real financial time series.
        
        Among all models, the Block Bootstrap approach consistently performs well across nearly 
        all statistical metrics. This strong performance is expected, as Block Bootstrap samples 
        directly from historical log returns, inherently preserving the empirical distribution, 
        including autocorrelation structures and heavy tails. Kurtosis and skewness deviations 
        are minimal, and temporal similarity metrics remain high (Appendix~\ref{app:evaluation-tables}), 
        confirming its ability to retain higher-order moments and realistic sequence dynamics.
        
        Deep learning-based models generally underperform relative to parametric approaches. 
        In particular, TimeVAE and Takahashi Diffusion generate distributional plots that fail 
        to reproduce key features of the real data (Appendix~\ref{app:visualizations}). 
        This underperformance is likely attributable to insufficient data expressivity and 
        limited training: the batch size was set to 32, the number of training epochs to 20, 
        and the window length selected via the Partial Correlation Function (PCF) was 13. 
        As a result, only $620/9035$ time series samples were effectively used to train each model. 
        Under these conditions, the models were unable to capture complex temporal dependencies 
        and higher-order moments, leading to underfitting, as evidenced by deviations in both 
        statistical and temporal metrics. Future work will address these limitations by 
        increasing batch size and training epochs, as well as experimenting with higher-granularity 
        index data (e.g., minute-level stock prices), consistent with the data granularity used 
        to successfully train the Takahashi DDPM.
        
        Merton Jump Diffusion (MJD) and, especially, Double Exponential Jump Diffusion (DEJD) 
        display extremely high kurtosis, consistent with their intended design. DEJD was 
        explicitly developed to model extreme market movements and heavy tails for robust 
        backtesting, producing rare but impactful price jumps. In T-SNE visualizations 
        (Appendix~\ref{app:visualizations}), DEJD-generated data forms distinct clusters 
        separated from the main cluster, highlighting the presence of extreme outliers. 
        MJD exhibits moderate kurtosis increases relative to GBM, reflecting its 
        capacity to generate jumps.
        
        Overall, these results illustrate the trade-offs between model simplicity, 
        parametric assumptions, and the ability to replicate complex statistical properties 
        of financial time series. Parametric jump models effectively capture extreme events, 
        Block Bootstrap preserves empirical distribution fidelity, and deep learning models 
        require larger and more expressive datasets to reach their theoretical potential. 
        The comparative metrics in Appendix~\ref{app:metrics-comparison} further highlight 
        the relative strengths and weaknesses of each approach in both quantitative and 
        visual terms.
  
  \subsection{Utility Evaluation Results}
  Results of the deep hedging evaluation (replication error, P\&L distribution, risk-adjusted metrics) can be summarized in tables analogous to the 
  above once computed.

  \subsection{Comprehensive Model Comparison}
  Provide radar plots or scorecards that combine normalized metrics (fidelity, diversity, stylized facts, efficiency) into composite ranks per task.

  \subsection{Ranking Analysis}
  Discuss trade-offs: fidelity vs. diversity, training time vs. quality, and sensitivity to window length and sample count.

\section{Conclusion and Future Work}
  In our research, we recognize the existing limitations in the evaluation of synthetic time series, 
  particularly the absence of a universally accepted framework. To address this, we adopt the 
  evaluation taxonomy proposed by Stenger et al. and expand upon it to create a comprehensive benchmark 
  specifically tailored for Synthetic Data Generation for Financial Time Series (SDGFTS). 
  Our contributions include the development of a consolidated evaluation framework that systematically 
  reviews and integrates various assessment methods from leading studies in the field. 
  This approach not only enhances the rigor of evaluations but also facilitates the comparison of 
  different SDGFTS models, ultimately providing a more standardized and reliable means of assessing 
  synthetic data quality.

  \subsection{Summary of Findings}
    We presented Stonk-Bench, a comprehensive benchmark for evaluating Synthetic Data Generation 
    for Financial Time Series (SDGFTS) models. Our benchmark integrates statistical fidelity, 
    diversity, stylized facts verification, and utility evaluation through deep hedging.
    We evaluated a suite of 5 traditional, 3 deep learning-based, and 1 other SDGFTS models on

  \subsection{Implications for SDGFTS Research}
    We believe that Stonk-Bench will serve as a valuable resource for researchers and 
    practitioners in the field of synthetic financial time series generation.
    We hope that our benchmark will facilitate more rigorous and standardized evaluations of SDGFTS models, 
    ultimately advancing the state of the art in this important area of research.
    For the broader field of synthetic data generation, our work highlights the importance of a 
    comprehensive and unifying evaluation frameworks that consider both statistical properties 
    and practical utility that is proposed by Stenger \textit{et al.} .

  \subsection{Stonk-Bench Limitation}
    While Stonk-Bench represents a significant step forward in the evaluation of SDGFTS models, 
    we acknowledge several limitations in our current work:
    \begin{itemize}
      \item \textbf{Incompatibility with other models:} StonkBench has so far been primarily evaluated on classical stochastic models 
      (e.g., GBM, OU, GARCH) and select deep learning models—Diffusion models, GANs, and VAEs. Other model families 
      (e.g., reinforcement learning-based generators, normalizing flows, autoregressive networks, hybrid approaches) 
      have not been benchmarked, so StonkBench's applicability to them is not yet established. As a result, 
      some metrics or procedures may not directly translate, limiting the framework’s generalizability to all synthetic financial time 
      series generation methods.
      \item \textbf{No hyperparameter tuning:} For consistency and comparability across models, StonkBench evaluations were performed 
      using default model configurations, without performing hyperparameter optimization. This ensures a fair baseline but may not reflect 
      the absolute best performance achievable by each model.
      \item \textbf{Limited input data:} The benchmark strictly uses raw historical stock price data (Open, High, Low, Close) as input. 
      External information, such as causal graphs, news sentiment, macroeconomic indicators, or alternative data sources, is not incorporated, 
      even though such information can significantly influence the accuracy of future price predictions.
      \item \textbf{Data Leakage from Public Datasets:} Current models can potentially overfit to 
      publicly available datasets \textemdash the same datasets that Stonk-Bench is using\textemdash 
      leading to inflated performance metrics that do not generalize well to unseen data. One possible 
      solution is to either curate proprietary datasets or implement a grace period so the model can be 
      tested on new data that was not publicly available during its development.
      \item \textbf{Computational Resource Requirements:} The comprehensive nature of Stonk-Bench \textemdash
      particularly accomodating any submited deep hedging model and the deep hedging utility evaluation \textemdash 
      demands significant computational resources. This may limit accessibility for researchers with constrained resources.
    \end{itemize}
  \subsection{Future Research Directions}
    \subsubsection{Progress for the remaining duration of the project} In regard to the next portion 
      of our project, we aim to expand Stonk-Bench in several key areas:
      \begin{itemize}
        \item \textbf{Complete Utility Evaluations:} As of now, our utility evaluation 
        through deep hedging is still in progress. We plan to finalize this component and integrate the 
        results into the overall benchmark.
        \item \textbf{Matching Distribution Metrics:} We intend to incorporate additional distribution matching metrics, such 
        as the the Komogorov-Smirnov (KS) statistic 
        to further enhance the fidelity evaluation.
        \item \textbf{Expansive Datasets:} We plan to test Stonk-Bench on a wider variety of 
        financial time series datasets, including indices and intraday (1-minute) data, to assess 
        the generalizability of our benchmark and its performance in downstreamq tasks evaluations.
      \end{itemize}

    \subsubsection{Beyond this project.} In regards the larger scope of our Stonk-Bennch project, 
    we envision several avenues for future research and project development:
      \begin{itemize}
        \item \textbf{Incorporation of Advanced SDGFTS Models:} As new models are developed, 
        we plan to continuously update Stonk-Bench to include these advancements, 
        ensuring that our benchmark remains relevant and comprehensive.
  
        \item \textbf{Develop rigorous evaluation metric selection:} We aim to refine and 
        expand the selection of evaluation metrics used in Stonk-Bench, ensuring they capture the 
        multifaceted nature of synthetic financial time series data. A proposed direction 
        is to perform ablation studies to identify the most informative metrics for 
        different types of SDGFTS models with its known stylized facts and characteristics \cite{Stenger24}.

        \item \textbf{Host a benchmark website:} To facilitate wider adoption and 
        accessibility, we plan to develop a dedicated website for Stonk-Bench. 
        This platform will provide comprehensive documentation, code repositories, 
        and resources for researchers and practitioners interested in utilizing the benchmark.
        We envision this website to also feature a ranking leaderboard where researchers can 
        submit their SDGFTS models and compare their performance against existing 
        models evaluated using Stonk-Bench.

        \item \textbf{Community Engagement and Collaboration:} We hope to foster a 
        collaborative environment where researchers can contribute to the development and refinement 
        of Stonk-Bench, promoting shared progress in the field of synthetic data generation.
      \end{itemize}


\begin{acks}
To professor Irene Huang, University of Toronto, for her supervision and guidance throughout the project.
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{stonk-bench}

\newpage
\appendix
\onecolumn
\section{Evaluation Tables}
\label{app:evaluation-tables}

\begin{table*}[h]
  \centering
  \caption{Fidelity metrics by model and channel (lower $\downarrow$ is better for differences; best value in each column bolded). Channels O, H, L, C correspond to dataset channels (Open, High, Low, Close).}
  \begin{tabular}{l|cccc|cccc|cccc}
    \toprule
    & \multicolumn{4}{c|}{MDD $\downarrow$} & \multicolumn{4}{c|}{MD $\downarrow$} & \multicolumn{4}{c}{SDD $\downarrow$} \\
    Model & O & H & L & C & O & H & L & C & O & H & L & C \\
    \midrule
    GBM & 4.35 & 4.48 & 4.48 & 3.41 & 0.0010 & 0.0004 & 0.0005 & 0.0014 & 0.0115 & 0.0108 & 0.0119 & 0.0110 \\
    OU\_Process & 4.31 & 4.48 & 4.43 & 3.37 & 0.00095 & \textbf{0.00027} & 0.00046 & 0.0014 & 0.0115 & 0.0107 & 0.0119 & 0.0110 \\
    MJD & 3.74 & 3.89 & 3.86 & 2.87 & 0.0014 & 0.00041 & 0.00042 & 0.00045 & 0.0104 & 0.0115 & 0.0133 & 0.0107 \\
    GARCH11 & \textbf{2.60} & \textbf{2.47} & 2.88 & \textbf{1.92} & \textbf{0.000004} & 0.000079 & 0.000096 & 0.000080 & \textbf{0.00041} & \textbf{0.00045} & 0.0020 & \textbf{0.0012} \\
    DEJD & 2.79 & \textbf{2.38} & \textbf{2.55} & 2.17 & 0.0015 & 0.00094 & 0.0018 & 0.0011 & 0.0138 & 0.0116 & 0.0160 & 0.0141 \\
    BlockBootstrap & 2.82 & 2.75 & 2.78 & 2.20 & 0.0014 & 0.00049 & 0.00033 & 0.00073 & 0.0102 & 0.0102 & 0.0108 & 0.0118 \\
    TimeGAN & 5.32 & 5.55 & 6.00 & 4.42 & 0.0026 & 0.0022 & 0.0034 & 0.0018 & 0.0081 & 0.0084 & 0.0094 & 0.0077 \\
    QuantGAN & 3.97 & 4.04 & 4.67 & 2.83 & 0.0056 & 0.00034 & 0.0079 & 0.0032 & 0.0085 & 0.0084 & 0.0118 & 0.0068 \\
    TimeVAE & 9.98 & 9.08 & 9.32 & 7.25 & 0.0018 & 0.0017 & 0.0011 & 0.0017 & 0.0188 & 0.0157 & 0.0170 & 0.0194 \\
    Takahashi & 5.81 & 6.33 & 6.26 & 5.10 & 0.145 & 0.0170 & 0.0222 & 0.104 & 1.23 & 1.22 & 1.28 & 1.26 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[h]
  \centering
  \caption{Efficiency: time (in seconds) taken by each model to generate 500 sample time series. Lower $\downarrow$ is better.}
  \begin{tabular}{l|c}
    \toprule
    Model & Time to generate 500 samples (s) $\downarrow$ \\
    \midrule
    GBM & 0.0024 \\
    OU\_Process & \textbf{0.0017} \\
    MJD & 0.0023 \\
    GARCH11 & 0.0029 \\
    DEJD & 0.0078 \\
    BlockBootstrap & 0.0175 \\
    TimeGAN & 0.0145 \\
    QuantGAN & 0.4991 \\
    TimeVAE & 0.0082 \\
    Takahashi & 102.51 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[h]
  \centering
  \caption{Temporal and diversity metrics by model and channel (lower $\downarrow$ is better for differences; higher $\uparrow$ is better for diversity distances). Best value in each column bolded.}
  \begin{tabular}{l|cccc|cccc}
    \toprule
    & \multicolumn{4}{c|}{ICD-ED $\uparrow$} & \multicolumn{4}{c}{ICD-DTW $\uparrow$} \\
    Model & O & H & L & C & O & H & L & C \\
    \midrule
    GBM             & 0.150 & 0.133 & 0.145 & 0.150 & 0.100 & 0.088 & 0.097 & 0.099 \\
    OU\_Process     & 0.150 & 0.133 & 0.145 & 0.149 & 0.100 & 0.088 & 0.096 & 0.100 \\
    MJD             & 0.140 & 0.130 & 0.143 & 0.142 & 0.096 & 0.092 & 0.101 & 0.098 \\
    GARCH11         & 0.090 & 0.077 & 0.095 & 0.088 & 0.061 & 0.051 & 0.064 & 0.060 \\
    DEJD            & 0.135 & 0.118 & 0.132 & 0.143 & 0.102 & 0.091 & 0.104 & 0.108 \\
    BlockBootstrap  & 0.138 & 0.125 & 0.132 & 0.146 & 0.100 & 0.091 & 0.097 & 0.107 \\
    TimeGAN         & 0.133 & 0.121 & 0.132 & 0.131 & 0.081 & 0.073 & 0.080 & 0.079 \\
    QuantGAN        & 0.137 & 0.122 & 0.146 & 0.130 & 0.092 & 0.081 & 0.097 & 0.086 \\
    TimeVAE         & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 \\
    Takahashi       & \textbf{6.091} & \textbf{6.076} & \textbf{6.366} & \textbf{6.253} & \textbf{4.204} & \textbf{4.245} & \textbf{4.358} & \textbf{4.364} \\
    \bottomrule
  \end{tabular}
\end{table*}


\newpage
\section{Visualizations}
\label{app:visualizations}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/GBM.png}
    \caption{Geometric Brownian Motion (GBM)}
    \label{fig:vis-gbm}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/OU Process.png}
    \caption{Ornstein-Uhlenbeck (OU) Process}
    \label{fig:vis-ou}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/MJD.png}
    \caption{Merton Jump Diffusion (MJD)}
    \label{fig:vis-mjd}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/DEJD.png}
    \caption{Double-Exponential Jump Diffusion (DEJD)}
    \label{fig:vis-dejd}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/GARCH11.png}
    \caption{GARCH(1,1)}
    \label{fig:vis-garch}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/BlockBootstrap.png}
    \caption{Block Bootstrap}
    \label{fig:vis-bootstrap}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/TimeGAN.png}
    \caption{TimeGAN}
    \label{fig:vis-timegan}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/TimeVAE.png}
    \caption{TimeVAE}
    \label{fig:vis-timevae}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/QuantGAN.png}
    \caption{QuantGAN}
    \label{fig:vis-quantgan}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/visualizations/Takahashi DDPM.png}
    \caption{Takahashi DDPM}
    \label{fig:vis-takahashi}
  \end{minipage}
\end{figure}

\newpage
\section{Evaluation Metrics Comparisons}
\label{app:metrics-comparison}
\newpage

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/distribution_metrics.png}
    \caption{Distribution Metrics Comparison}
    \label{fig:distribution-metrics}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/feature_based.png}
    \caption{Feature-Based Metrics Comparison}
    \label{fig:feature-based}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/similarity_metrics.png}
    \caption{Similarity Metrics Comparison}
    \label{fig:similarity-metrics}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/performance_generation_time.png}
    \caption{Generation Time Performance}
    \label{fig:generation-time}
  \end{minipage}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/stylized_fact_heavy_tails.png}
    \caption{Heavy Tails Stylized Fact}
    \label{fig:heavy-tails}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/stylized_fact_autocorr_raw.png}
    \caption{Return Autocorrelation Stylized Fact}
    \label{fig:autocorr-raw}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../evaluation_plots/stylized_fact_volatility_clustering.png}
    \caption{Volatility Clustering Stylized Fact}
    \label{fig:volatility-clustering}
  \end{minipage}
\end{figure}


\end{document}