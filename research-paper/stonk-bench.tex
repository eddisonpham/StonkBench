\documentclass[sigconf]{acmart}
\usepackage{enumitem}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}



\begin{document}


\title{Stonk Bench: Unified Benchmark for Synthetic Data Generation for Financial Time Series}



\author{Uyen Lam Ho}
\authornote{Both authors contributed equally to this research.}
\email{uyenlam.ho@mail.utoronto.ca}
\author{Eddison Pham}
\authornotemark[1]
\email{eddison.pham@mail.utoronto.ca}
\affiliation{%
  \institution{University of Toronto}
  \city{Toronto}
  \state{Ontario}
  \country{Canda}
}



\renewcommand{\shortauthors}{Ho and Pham}

\begin{abstract}
Synthetic Data Generation (SDG) has become increasingly important in
financial applications, particularly for generating Financial Time Series
(FTS) data. However, evaluation of synthetic FTS remains inconsistent, with
ad hoc methods limiting fair and reliable comparisons. To address this, we
introduce StonkBench, a unified benchmark for synthetic data generation in
financial time series (SDGFTS) that integrates four evaluation taxonomies:
Fidelity, Diversity, Efficiency, and Utility, each comprising metrics
specifically designed for forecasting tasks. Utility evaluations leverage
portfolio assessments generated by a (deep) hedger. StonkBench builds upon
Stenger's taxonomies and TSGBench's SDG framework, combining an enhanced
standardization pipeline tailored for FTS, a comprehensive set of evaluation
metrics, and an open-source framework that unifies evaluation procedures,
enabling consistent, reproducible comparisons across datasets and models.
Additionally, our benchmark evaluates statistical properties observed in
financial time series, i.e., stylized facts, including volatility clustering
and fat tails.
\end{abstract}


\keywords{
    Synthetic Data Generation, 
    Financial Time Series,
    Benchmarking
}
\begin{teaserfigure}
  \centering
  \includegraphics[width=\textwidth,height=5.5cm,keepaspectratio,
    trim=0 150 0 150,clip]{tse-1981.png}
  \caption{Toronto Stock Exchange, Toronto, 1981}
  \Description{A panoramic view of the Toronto Stock Exchange 
    building in 1981.}
  \label{fig:teaser}
\end{teaserfigure}


\maketitle

\section{Introduction}
  Synthetic Data Generation (SDG) has emerged as a crucial tool in
  financial technology, particularly for Financial Time Series (FTS)
  data \cite{Potluru24}. The ability to generate high-quality
  synthetic financial data addresses several critical challenges in
  the field, like replicating and simulating fat-tail behaviors
  observed in financial returns time series and developing
  back-testing procedures for prediction and (deep) hedging
  algorithms \cite{Stenger24}.

  Despite the growing importance of SDGFTS (Synthetic Data 
  Generation for Financial Time Series), there is a notable lack 
  of standardization in evaluating the quality and effectiveness 
  of these generative models \cite{Stenger24}. Current evaluation 
  methods vary widely across studies, making it difficult to 
  compare different approaches objectively and determine their 
  relative strengths and weaknesses.

  \subsection{Limitations in the SDGFTS Literature}
    Among other things, we observed that there is currently no 
      universal, generally accepted approach to evaluating synthetic 
      time series \cite{Stenger24}; this issue extends beyond FTS 
      to generative frameworks like GANs as a whole \cite{Wang20}. 
      Many evaluation measures are insufficiently defined and lack 
      public implementations, making reuse and reproduction 
      troublesome and error-prone \cite{Stenger24}. This presents a 
      challenge unique to the generation task compared to areas like 
      time series forecasting or classification \cite{Stenger24}. 
      Hence, future research would immensely benefit from a widely 
      accepted, reasonably sized set of qualified measures for the 
      central evaluation criteria. Here we outline observations on 
      the current limitations in SDGFTS research and evaluation 
      practices:
      
      \begin{enumerate}[label=\textbf{[L\arabic*]}]
        \item \label{limitation:metrics}
        \textbf{Inconsistent Evaluation Metrics:} Different 
        studies employ varying metrics, hindering direct comparisons 
        between models. For instance, some focus on statistical 
        similarity, while others emphasize utility in downstream tasks 
        \cite{Stenger24}.
        
        \item \textbf{Lack of Standardized Datasets and 
        Preprocessing:} The absence of common benchmark datasets leads 
        to evaluations on disparate data, making it challenging to 
        assess model performance uniformly \cite{Stenger24} in terms 
        of both asset classes, data granularity, and data length. 
        Furthermore, preprocessing steps (e.g., normalization, 
        windowing) vary widely, affecting results and complicating 
        comparisons.
        
        \item \textbf{Narrow Scope of Evaluation:} Many evaluations 
        concentrate on a narrow set of criteria, such as marginal 
        distributions, neglecting other important aspects like 
        temporal dependencies and generation diversity
        \cite{Stenger24}.
        
        \item \textbf{Reproducibility Issues:} The lack of 
        open-source implementations and detailed evaluation protocols 
        impedes reproducibility and validation of results across 
        different studies \cite{Stenger24,Potluru24}. It is important
        to note that most model papers did not provide code for their 
        evaluations.
      \end{enumerate}

  \subsection{Our Contributions}
  To put it in simple terms, we are adopting the time series evaluation
  taxonomy outlined from Stenger et al. \cite{Stenger24}, develop a
  comprehensive and unified SDG benchmark expanded from the works of
  Ang et al. \cite{Ang23} in specifics to FTS by incorporating a utility
  evaluation framework inspired by Boursin et al. \cite{Boursin22} through
  portfolio evaluations generated by a (deep) hedger.
  Our key contributions include:
  \begin{enumerate}[label=\textbf{[C\arabic*]}]
    \item \label{contribute:1} \textbf{Consolidated Evaluation
      Framework:}
      In effort to address \ref{limitation:metrics}, we systematically
      review and consolidate evaluation methods from leading papers
      (models and surveys) in SDG and financial time series
      \cite{Stenger24, Ang23, Boursin22}, creating a comprehensive
      assessment toolkit based on established practices.

    \item \label{contribute:2} \textbf{Unified Statistical and
      Utility Measures:}
      For the first time, we integrate both statistical fidelity metrics
      and practical utility measures in a single SDGFTS benchmark,
      providing a more complete evaluation of synthetic data quality.

    \item \label{contribute:3} \textbf{Outline Benchmark Framework:}
      We deliver an open-source benchmark framework to share with the
      research community, aiming to establish cohesion and standard for
      future SDGFTS research in evaluation and comparison.
  \end{enumerate}

  \subsection{Research Questions}
  Our benchmark aims to address the following research questions:
  \begin{enumerate}[label=\textbf{[Q\arabic*]}]
    \item \label{rq:1} \textbf{How do different sequence lengths during
      training affect STGFTS performance?}
      We investigate the impact of varying training sequence lengths on
      the performance of synthetic financial time series generation as an
      extension of existing work like Allen et al. \cite{Allen2024}.
      The selected training sequence lengths are described in 
      §\ref{sec:model-specific}.

    \item \label{rq:2} \textbf{Is there a trade-off between certain
      metrics or categories of metrics in SDGFTS?}
      In an effort to incorporate an ensemble of evaluation metrics, we
      are interested in understanding potential trade-offs between these
      metrics and how they impact the overall performance of SDG
      \cite{Ang23}. Particularly, we are interested in how a SDGFTS
      model's ability to balance fidelity and diversity as highlighted in
      recent studies \cite{Lu25}. Another obvious trade-off to
      investigate is between models' generation efficiency and the
      quality of generated data.

    \item \label{rq:3} \textbf{Whether new deep learning models
      actually outperform classical parametric models in SDGFTS tasks?}
     Most existing literature focuses on deep learning-based methods tend 
     to dominate the field. We aim to provide a comprehensive comparison 
     between deep learning-based approaches and traditional methods, focusing on
      their respective strengths and weaknesses in generating high-quality
      synthetic financial time series data.

      \end{enumerate}

\section{Preliminaries}
  \subsection{Problem Definition}
    Let us formally define the Synthetic Data Generation problem 
    for Financial Time Series. Given a financial time series 
    $\textbf{X} = (\textbf{x}_1,...,\textbf{x}_R)^T$ with $R$ 
    individual series represented as an $L$-dimensional vector 
    $\textbf{x}_i=(x_{i1},...,x_{iL})$, where $x_{ij}$ denotes 
    the $j$-th point of time series $\textbf{x}_i$. Let 
    $p(\textbf{x}_1,...,\textbf{x}_R)$ denote the real distribution 
    of the given time series $\textbf{X}$. The objective of SDG 
    for FTS is to generate a synthetic time series 
    $\mathbf{\hat{X}} = (\mathbf{\hat{x}}_1, \ldots, 
    \mathbf{\hat{x}}_n)^{T}$ such that its distribution 
    $p(\mathbf{\hat{x}}_1, \ldots, \mathbf{\hat{x}}_n)$ 
    approximates $p(\textbf{x}_1,...,\textbf{x}_R)$, while 
    preserving the key properties exhibited in the real financial 
    time series, such as autocorrelation structure, volatility 
    clustering, and distributional moments (mean, variance, 
    skewness, kurtosis).

  \subsection{Scope of Project}
    \subsubsection{Scope of Methods}
      Our benchmark encompasses a diverse range of SDGFTS methods, 
      from traditional parametric approaches to modern deep 
      learning architectures. We selected models based on three 
      main criteria: (1) proven industry adoption, (2) research 
      community acceptance, and (3) recent methodological 
      innovations.

      Our evaluation includes classical parametric models, which 
      rely on predefined statistical distributions and assumptions 
      about the underlying data generation process. These models 
      have been extensively used in financial institutions for 
      their interpretability and theoretical foundations. 

      We also evaluate modern deep learning approaches, 
      particularly deep learning-based models that learn the data 
      distribution directly from observations without assuming a 
      specific form. These include generative adversarial 
      networks, and variational autoencoders, 
      representing the cutting edge in synthetic data generation.

      \subsubsection{Scope of Datasets}
      We evaluate models on high-granularity financial time series 
      indices prices from HistData.com \cite{histdata}. Currently, we 
      focus on univariate time series data generation. The S\&P\,500 
      (SPXUSD) index is chosen, using the minute-level closing price 
      from 2021 through 2023. Future work may extend to multivariate 
      series involving multiple assets or additional features.

      \subsubsection{Scope of Evaluation Taxonomical Criteria}
      Our evaluation framework follows the taxonomy structure 
      proposed by Stenger et al. \cite{Stenger24}, incorporating 
      various evaluation measures from different papers in the 
      field.

      We systematically integrate evaluation metrics from multiple 
      sources while maintaining this structured taxonomical 
      approach, ensuring comprehensive coverage of all critical 
      aspects of SDGFTS evaluation and setting a standard for 
      future research in time series SDG as a whole.

\section{Overview of Synthetic FTS Methods}
  We categorize synthetic financial time series generation methods into
  two main families: classical stochastic (parametric) models, which
  rely on explicit assumptions about the data-generating process, and
  modern deep learning (deep learning) models, which learn complex
  dependencies directly from data without predefined functional forms.
  The following subsections provide a detailed overview of the methods
  included in our benchmark.


  \subsection{Classical Stochastic (Parametric) Methods}
  \label{sub:parametric-methods}
    Parametric models assume a specific functional form for the 
    data generating process, characterised by a finite set of 
    parameters. These models are interpretable and analytically 
    tractable, allowing for closed-form solutions in many cases. 
    However, they may struggle to capture complex stylised facts 
    or high-dimensional dependencies beyond their structural 
    assumptions. Below we outline several widely-used parametric 
    models in FTS.\\

      \begin{table}[h]
        \centering
        \caption{Common notation and variables.}
        \label{tab:notation}
        \begin{tabular}{ll}
        \toprule
        \textbf{Symbol} & \textbf{Description} \\
        \midrule
        \( S_t \) & Asset price \\
        \( X_t = \ln S_t \) & Log-price \\
        \( r_t = \ln(S_t / S_{t-1}) \) & Log-return \\
        \( \mu \) & Drift \\
        \( \sigma \) & Volatility \\
        \( W_t \) & Standard Brownian motion \\
        \( \Delta t \) & Discrete time step \\
        \bottomrule
        \end{tabular}
      \end{table}
        
        

      \begin{enumerate}[label=\textbf{[A\arabic*]}]
        \item \label{alg:gbm} \textbf{Geometric Brownian Motion 
        (GBM).} 
        The price process follows a continuous-time stochastic 
        process with constant drift and volatility. Model-specific 
        parameters are \(\mu\) and \(\sigma\).
        \begin{equation}
        dS_t = \mu S_t\,dt + \sigma S_t\,dW_t.
        \label{eq:gbm}
        \end{equation}

        \item \label{alg:ou} \textbf{Ornstein–Uhlenbeck (OU).} 
        A mean-reverting Gaussian process for log-prices or 
        spreads. Model-specific parameters are \(\theta>0\) 
        (reversion rate), \(\mu\) (long-term mean), and \(\sigma\) 
        (volatility).
        \begin{equation}
        dX_t = \theta(\mu - X_t)\,dt + \sigma\,dW_t.
        \end{equation}

        \item \label{alg:mjd} \textbf{Merton Jump Diffusion (MJD).} 
        Extends the GBM equation \eqref{eq:gbm} by incorporating 
        normally-distributed jumps. 
        Model-specific parameters are \(\lambda\) (jump intensity), 
        \(\mu_j\) and \(\sigma_j\) (mean and standard deviation of 
        jump sizes).
        \begin{equation}
        \frac{dS_t}{S_t} = \mu\,dt + \sigma\,dW_t + J\,dN_t,\\
        \end{equation}
        where $J \sim \mathcal N(\mu_j, \sigma_j^2),\quad N_t\sim\mathrm{Poisson}(\lambda t)$.


        \item \label{alg:dejd} \textbf{Double-Exponential Jump 
        Diffusion (DEJD).} 
        Another jump-diffusion model with asymmetric double-exponential 
        jump sizes modifying equation \eqref{eq:gbm}. 
        Model-specific parameters are \(p\) 
        (probability of upward jump) and \(\eta_1, \eta_2\) 
        (exponential parameters for upward/downward jumps).
        \begin{align}
          \frac{dS_t}{S_t} &= \mu\,dt + \sigma\,dW_t + Y\,dN_t,\\
          Y &=
          \begin{cases}
          \mathrm{Exp}(\eta_1), &\text{with probability } p,\\[4pt]
          -\mathrm{Exp}(\eta_2), &\text{with probability } 1-p.
          \end{cases} \nonumber
        \end{align}

        \item \label{alg:garch} \textbf{GARCH(1,1).} 
        A discrete-time conditional heteroskedastic model for 
        returns. Model-specific parameters are \(\omega>0,\ 
        \alpha\ge0,\ \beta\ge0\) (GARCH coefficients) and 
        \(\mathcal D\) (innovation distribution, usually standard 
        Normal or Student-t). Captures volatility clustering via 
        time-varying conditional variance.
        \begin{align}
          r_t &= \sigma_t z_t,\quad z_t\sim\mathcal D,\\
          \sigma_t^2 &= \omega + \alpha r_{t-1}^2 + \beta \sigma_{t-1}^2. \nonumber
        \end{align}
      \end{enumerate}


  \subsection{Deep Learning Methods}\label{sub:deep-learning-methods}
    Deep learning models, in this context, refer to generative 
    (often implicit) models that do not assume a fixed parametric 
    form for the underlying data-generating process. Instead, they 
    learn latent structures directly from data through flexible 
    architectures such as neural networks, enabling the modeling 
    of highly nonlinear, high-dimensional, and multi-modal 
    dependencies (e.g., multivariate financial time series with 
    interactions across assets, time horizons, and features). 
    While these models trade off interpretability and analytical 
    tractability, they offer substantially greater expressive 
    power and flexibility. This expressiveness, however, comes at 
    the cost of increased data requirements, more intensive 
    hyperparameter tuning, and higher computational demands. 

    We categorize these approaches into two major families of
    deep generative models, considered in this paper: 
    Generative Adversarial Networks (GANs)
    and Variational Autoencoders (VAEs).
    

  %   \subsubsection{Diffusion Models}
  %     Diffusion models (or score-based generative models) have 
  %     emerged as powerful methods for modeling complex data 
  %     distributions via a forward (noise-adding) process and a 
  %     learned reverse (denoising) process \cite{ho2020denoising}. 
  %     In the forward direction, one gradually corrupts a data sample 
  %     \(x_0\) into noise via a Markov chain:
  %    
  %     \begin{align}
  %       q(x_{1:T}\mid x_0) &= \prod_{t=1}^T q(x_t \mid x_{t-1}), \\
  %       q(x_t \mid x_{t-1}) &= \mathcal{N}\left(x_t; \sqrt{1-\beta_t}\,x_{t-1}, \beta_t I\right). \nonumber
  %     \end{align}
  %    
  %     
  %     The reverse (generative) model is parameterized as
  %     \[
  %       p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\bigl(x_{t-1}; 
  %       \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)\bigr),
  %     \]
  %     where \(\mu_\theta\) and \(\Sigma_\theta\) are often 
  %     expressed via a neural network that estimates the score 
  %     \(\nabla_{\theta} \log p_\theta(x_t)\). A common training 
  %     objective is the simplified denoising score-matching loss:
  %     \[
  %       \mathbb{E}_{t, x_0, \epsilon}\Bigl[ \bigl\| \epsilon - \epsilon_\theta(x_t, t)\bigr\|^2 \Bigr],
  %     \]
  %     where \(x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon\).
  %     
  %     In finance/time-series, diffusion models have recently been 
  %     adapted to generate synthetic asset paths by converting 
  %     multivariate time series into suitable representations (e.g. 
  %     wavelet-transformed images) and then sampling via reverse 
  %     diffusion \cite{Tanaka25}. For instance, Takahashi and Mizuno 
  %     shows that such an approach can reproduce key statistical 
  %     properties of financial data (e.g.\ volatility clustering, 
  %     tail behavior) \cite{Takahashi24}.  
  %     Another relevant work is "A Financial Time Series Denoiser 
  %     Based on Diffusion Model", which shows how diffusion can help 
  %     improve downstream predictability and reduce noise in 
  %     financial signals \cite{wang24denoiser}.
    \subsubsection{Variational Autoencoders (VAEs)}
    VAEs \cite{kingma2022vae, 
    rezende2014stochastic} are latent-variable generative models 
    that posit a probabilistic encoder \(q_\phi(z \mid x)\) and 
    decoder \(p_\theta(x \mid z)\). One maximizes the evidence 
    lower bound (ELBO):
    \[
    \mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\phi(z \mid x)}
    \bigl[\log p_\theta(x \mid z)\bigr] - \mathrm{KL}\bigl(
    q_\phi(z \mid x)\,\Vert\,p(z)\bigr).
    \]
    Often one uses a standard Gaussian prior \(p(z) = 
    \mathcal{N}(0, I)\). The encoder–decoder structure allows 
    sampling by first drawing \(z \sim p(z)\), then generating 
    \(x \sim p_\theta(x \mid z)\).
    
    In financial time-series generation, specialized versions such 
    as the Time-Causal VAE (TC-VAE) have been proposed to enforce 
    causality in the encoding/decoding of temporal data. For 
    instance, Acciaio et al.\ (2024) propose TC-VAE, which 
    ensures that generated paths respect temporal causality and 
    show that the model reproduces stylized facts (e.g., heavy 
    tails, volatility clustering) on real markets 
    \cite{acciaio2024vae}. Because VAEs provide a principled 
    latent representation, they are useful in finance to model 
    underlying latent drivers of asset dynamics and to sample 
    coherent trajectories.
    \begin{enumerate}[label=\textbf{[A\arabic*]},resume]
      \item \label{alg:timevae} \textbf{TimeVAE.}
      A time-series specific VAE variant that combines causal
      temporal encoders (e.g., causal convolutions or RNNs) with an
      autoregressive decoder to preserve path coherence. Training
      typically augments the ELBO with prediction or stepwise
      reconstruction losses to improve short- and long-range
      dependencies; TimeVAE is used in our benchmark as a
      dedicated VAE baseline for temporal data \cite{Desai21}.
    \end{enumerate}
    
    \subsubsection{Generative Adversarial Networks (GANs)}
    GANs models approach 
    generation as a two-player game between a generator \(G\) and 
    a discriminator \(D\) \cite{Goodfellow14}. The discriminator 
    is trained to 
    maximize the probability of correctly classifying real data 
    and generated data, with loss
    \[
    L_D = -\mathbb{E}_{x \sim p_{\text{data}}}\bigl[\log D(x)
    \bigr] - \mathbb{E}_{z \sim p(z)}\bigl[\log (1 - D(G(z)))
    \bigr].
    \]
    The generator aims to fool the discriminator and is trained to 
    minimize
    \[
    L_G = -\mathbb{E}_{z \sim p(z)}[\log D(G(z))].
    \]
    Many GAN variants, such as Wasserstein GAN or Least Squares 
    GAN, modify these loss functions to promote more stable 
    training and address issues like mode collapse \cite{Goodfellow14} .
    
    In financial data synthesis, GANs have been widely used to 
    generate realistic synthetic time series. For example, 
    GAN-based financial data generation models (often using WGAN 
    or improvements) show that one can improve the authenticity 
    and predictive ability of generated financial statements or 
    time series \cite{Qi25}. Another example is 
    VRNNGAN, which uses a recurrent VAE as the generator and a 
    recurrent discriminator to capture temporal dependencies in 
    synthetic sequence generation \cite{Lee22}. GANs are 
    relevant in finance because they can capture complex joint 
    distributions and dependencies in multivariate time-series 
    without requiring explicit likelihood models.
    
    \begin{enumerate}[label=\textbf{[A\arabic*]},resume]
      \item \label{alg:quantgan} \textbf{QuantGAN.}
      A GAN variant tailored for financial applications that
      incorporates quantile-aware objectives or conditional
      quantile matching and temporal architectures (e.g.,
      temporal convnets or transformers). QuantGANs aim to better
      capture tail behaviour and risk-sensitive statistics
      (e.g., VaR/CVaR) important for downstream financial tasks
      \cite{Wiese2020}.
    \end{enumerate}
  \subsection{Miscellaneous}
  This section consists of models that does not fall in the 
  classification of §\ref{sub:parametric-methods} or §
  \ref{sub:deep-learning-methods}.
  \begin{enumerate}[label=\textbf{[A\arabic*]}, resume]
        \item \label{alg:blockbootstrap2} \textbf{Block Bootstrap 
        (Resampling Method).} 
        A resampling technique that preserves short-range 
        dependence by sampling contiguous blocks of returns. 
        Although not parametric, it is included here for 
        completeness and baseline comparison.
      \end{enumerate}

      \begin{figure*}[h]
        \centering
        \includegraphics[width=\textwidth,keepaspectratio]{../results/stonk_bench_architecture.png}
        \caption{Stonk Bench system architecture overview.}
        \Description{Diagram illustrating the complete Stonk Bench pipeline including data preprocessing, model evaluation, and benchmark framework components.}
        \label{fig:stonk_bench_architecture}
      \end{figure*}


\section{Stonk Bench Architecture}
  \subsection{Datasets and Preprocessing}
    The preprocessing procedure builds upon the TSGBench pipeline, 
    which standardizes segmentation, normalization, and train/test 
    splitting of the time series \cite{Ang23}. Several
    modifications were introduced to better accommodate financial
    time series and stochastic models.

    \subsubsection{Data type.} Index price data at 
      minute frequency at closing price are 
      used. Let \(X \in \mathbb{R}^{L \times N}\) denote a 
      time series with \(N=1\) channel(s) and length 
      \(L\).

    \subsubsection{Transformations.}\label{sec:transformations}
      Raw prices are converted to log-returns per channel via

      \[ r_t = \log S_t - \log S_{t-1} = \log(S_t / S_{t-1}) \,, 
      \quad r_t \in \mathbb{R}^N, \]
      
      yielding a transformed series of length \(L-1\). Log returns 
      are preferred in finance due to their variance-stabilizing 
      properties, preserving heavy tails and volatility clustering,
      and removal of the first order integration (random walk) 
      commonly exhibited in price series \cite{Takahashi24}.

    \subsubsection{Sequence lengths specification}\label{sec:sequence_length}
      To address research question \ref{rq:1}, we 
        experiment with varying training sequence lengths \(L\).
        For deep learning models, overlapping sliding windows 
        \(\mathcal{W} \in \mathbb{R}^{R \times L \times N}\) with 
        stride 1 are extracted.
        Specifically, we consider the following lengths \( L \):
        60, 120, 240, 300 minutes, and the maximum significant 
        partial autocorrelation lag.
      
      Unlike TSGBench, which employs an 
        autocorrelation-based hard-coded window of 125 (often 
        yielding a size of 1 for financial returns due to fast 
        autocorrelation decay \cite{Takahashi24,Cont01}), the window
        length \(L\) is alternatively
        determined via the partial autocorrelation function (PACF).
        The PACF aims to remove indirect correlations from in-between
        lags due to auto-regressive structures by finding the 
        best linear-predictor of the intermediate lagged values
        \cite{Durbin60}. Particularly for a lag \(h\), the PACF
        \(\phi_{hh}\) is defined between \( X_t \) and \( X_{t+h}\) as
        \begin{equation*}
        \phi_{hh} = \text{corr}\bigl(X_t - \hat{X}_t, X_{t+h} - \hat{X}_{t+h}\bigr),
        \end{equation*}
        where \(\hat{X}_t, \hat{X}_{t+h}\) is the best linear 
        predictor of \(X_t, X_{t+h}\),respectively, given the
        intermediate lags \(X_{t+1}, \ldots, X_{t+h-1}\).
        Specifically, PACF is computed up to 
        \(\lfloor 10\log_{10} n \rfloor\) lags (this value is determined
        from TSGBench \cite{Ang23}), and the lag 
        corresponding to the maximum significant PACF peak across 
        channels is selected, resulting in \(L=52\). This approach 
        captures relevant temporal dependencies within the data.

    \subsubsection{Model-specific preprocessing.}\label{sec:model-specific}
    The preprocessing follows the general TSGBench framework,
    with the following adaptations:

    \begin{itemize}
        \item Log returns are used directly instead of TSGBench's 
        min-max normalization for the reasons outlined in 
        §\ref{sec:transformations}. 
        \item For parametric models, the transformed series is 
        divided into contiguous training, validation, and test 
        segments with ratios \((1-\alpha-\beta):\alpha:\beta\), 
        where \(\alpha=0.1\) and \(\beta=0.1\). This ensures that 
        parametric models fit parameters directly to contiguous 
        historical data.
  
        \item The dataset is split into train, validation, and test 
        sets while preserving temporal order to prevent future 
        information from leaking into the past. Only the training 
        set is shuffled during model training to improve 
        convergence. In contrast, TSG-Bench shuffles time series 
        samples before splitting, which introduces data leakage. 
        Our approach ensures strictly forward-looking evaluation 
        for realistic forecasting.

        proper evaluation, consistent with TSGBench.
    \end{itemize}

    \subsubsection{Data loaders.} For deep learning models, 
    mini-batches are generated from the windowed data 
    \(\mathcal{W}\) using independent fixed seeds for training, 
    validation, and test sets, enabling reproducible epoch-wise 
    evaluation. Training batches are shuffled, while validation 
    and test sets maintain sequential ordering. To ensure fair 
    comparison, parametric models generate sequences of identical 
    length \(L\) and matching sample count to the windowed data 
    used by deep learning models.


  \subsection{Statistical Evaluation Measures}
    All metrics below are applied \textbf{per channel}, i.e., each 
    univariate time series (OHLC column) is evaluated 
    independently. For multivariate data, results are reported as 
    channel-wise statistics (e.g., mean or distribution across 
    channels).
    
    \subsubsection{Fidelity Measures}
    These metrics quantify how well synthetic data captures key 
    marginal and second-order properties of real data. We employ
    the following feature-based metrics:

      \begin{enumerate}[label=\textbf{[M\arabic*]}]
        \item \label{metric:mdd} \textbf{Marginal Distribution 
        Difference (MDD):}  
        To measure the distance between empirical marginal distributions
        of real versus synthetic data, we use the 1-Wasserstein distance
        (a.k.a. Earth Mover's distance). This was chosen over the 
        popular Kullback-Leibler divergence or the derivatives
        Jensen-Shannon divergence due to its better numerical stability
        and ability tocapture distributional differences even when
        supports do not overlap \cite{Arjovsky17}.
        \[
        \begin{aligned}
          \mathrm{MDD} &= W_1\big(\widehat{F}_{\text{real}}, 
          \widehat{F}_{\text{synth}}\big), \\
          W_1(F, G) &= \int_0^1 \big|F^{-1}(u) - G^{-1}(u)\big|\,du,
          \end{aligned}
          \]
          where \(\widehat{F}_{\text{real}}\) and \(\widehat{F}_{\text{synth}}\) 
          are empirical CDFs.
          
          \end{enumerate}
          The following four metrics capture differences in the first four
          statistical (central) moments between real and synthetic data.
          \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
          
          \item \label{metric:md} \textbf{Mean Difference (MD):}  
          \[
          \mathrm{MD} = \big|\mu_{\text{real}} - \mu_{\text{synth}}\big|.
          \]
          
          \item \label{metric:sdd} \textbf{Standard Deviation Difference (SDD):}  
          \[
          \mathrm{SDD} = \big|\sigma_{\text{real}} - \sigma_{\text{synth}}\big|.
          \]

          \item \label{metric:skewness} \textbf{Skewness Difference (SD):}  
          \begin{align*}
          \mathrm{SD} =& \big|\gamma(r_{\text{real}}) - \gamma(r_{\text{synth}})\big|, \\
          \gamma(r) =& \frac{\mathbb{E}[(r - \mu)^3]}{(\sigma)^3}.
          \end{align*}
          
          \item \label{metric:kd} \textbf{Kurtosis Difference (KD):}  
          \begin{align*}
          \mathrm{KD} &= \big|\kappa^{\mathrm{ex}}(r_{\text{real}}) 
          - \kappa^{\mathrm{ex}}(r_{\text{synth}})\big|, \\
          \kappa^{\mathrm{ex}}(r) &= \frac{\mathbb{E}[(r - \mu)^4]}{(\sigma)^4} - 3.
          \end{align*}
          
          \end{enumerate}
          
          \subsubsection{Visualization Methods} \label{sec:visualization_methods}
          Visual tools complement numeric metrics for face-validity and 
          communication \cite{Ang23}.
          \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
          \item \label{metric:tsne} \textbf{t-SNE:}  
          2D embeddings of window-level features for real 
          versus synthetic overlap and cluster structure.
          
          \item \label{metric:dist} \textbf{Distribution:}  
          Histograms of returns to visualize empirical 
          distribution.
          \end{enumerate}
          
          \subsubsection{Diversity Measures}
          We are also interested if the SDGFTS can generate data
          not observed but follow the same distribution as the real data.
          Therefore, we include the following intra-class distance
          metrics. The inclusion of
          these metrics are chose to ensure that mode collapse 
          is prevented. 

          
          \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
          \item \label{metric:ed} \textbf{Intra-Class Euclidean 
          Distance (ICD-ED):}  
          \[
          \mathrm{ICD\text{-}ED} = \frac{2}{R(R-1)} 
          \sum_{1 \le a < b \le R} \left\| \textbf{r}_a - 
          \textbf{r}_b \right\|_2,
          \]
          where \(\textbf{r}_a \in \mathbb{R}^{L}\) is the \(a\)-th 
          sample.
          
          \item \label{metric:dtw} \textbf{Intra-Class Dynamic Time 
          Warping (ICD-DTW):}  
          \[
          \mathrm{ICD\text{-}DTW} = \frac{2}{R(R-1)} 
          \sum_{1 \le a < b \le R} d_{\mathrm{DTW}}\bigl(
          \textbf{r}_a, \textbf{r}_b\bigr).
          \]
          \end{enumerate}
          
          \subsubsection{Efficiency Measures}
          Generation efficiency is crucial for practical applications,
          especially in high-frequency trading or real-time risk
          management, where rapid data synthesis is required.
          Thus, we include the following efficiency metric \cite{Stenger24}.
          \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]
          \item \label{metric:gentime} \textbf{Generation Time:}  
          Wall-clock time to generate \(S\) samples:
          \[
          T_{\mathrm{gen}} = t_1 - t_0 \quad (\text{seconds for } 1,000
          \text{ samples}).
          \]
          \end{enumerate}
          
          \subsubsection{Stylized Facts Measures} \label{sec:stylized-facts}
          Canonical stylized facts:
          
          \begin{enumerate}[label=\textbf{[M\arabic*]}, resume]

          \item \label{metric:acd} \textbf{Autocorrelation Difference 
          (ACD):}  
          Absolute difference in lag-1 autocorrelation:
          \[
          \begin{aligned}
          \rho(1) &= \frac{\sum_{t=2}^{L} (r_t - \bar{r})
          (r_{t-1} - \bar{r})}
               {\sum_{t=1}^{L} (r_t - \bar{r})^2}, \\
          \mathrm{ACD} &= \big|\rho_{\text{real}}(1) - 
          \rho_{\text{synth}}(1)\big|.
          \end{aligned}
          \]
          
          \item \label{metric:volclustering} \textbf{Volatility 
          Clustering (VC):}  
          Lag-1 autocorrelation of squared or absolute returns measures 
          volatility clustering, a stylized fact where large price movements 
          tend to cluster in time \cite{Cont01}:
          \[
          \begin{aligned}
          \rho_{r^2}(1) &= \mathrm{Corr}\big((r_t)^2, 
          (r_{t-1})^2\big), \\
          \rho_{|r|}(1) &= \mathrm{Corr}\big(|r_t|, 
          |r_{t-1}|\big).
          \end{aligned}
          \]

        \item \label{metric:slow_decay} \textbf{Long Memory / Slow Decay (LMSD):}  
        To quantify long-range dependence in volatility, we follow
        Cont (2001) \cite{Cont01} and study the autocorrelation of
        absolute (or squared) returns:
        \[
        C_\alpha(\tau) \,=\, \mathrm{corr}\!\big(|r_{t+\tau}|^\alpha,\,
        |r_t|^\alpha\big), \qquad \alpha \in \{1,2\}.
        \]
        Empirically, this correlation decays slowly according to a
        power law,
        \[
        C_\alpha(\tau) \,\sim\, \frac{A}{\tau^{\beta}}, \qquad
        \beta \in [0.2,\,0.4],
        \]
        indicating persistent volatility fluctuations (long memory).
        For evaluation, we estimate the decay exponent by a
        log--log linear fit over lags 
        \(\tau \in \{1,\ldots, \tau_{\max}\}\) where
        \(C_\alpha(\tau) > 0\):
        \[
        \widehat{\beta} \,=\, -\,\text{slope}\Big(\log C_\alpha(\tau)
        \;\text{vs.}\; \log \tau\Big).
        \]
        Our metric is the absolute difference between decay exponents
        estimated on real and synthetic data:
        \[
        \mathrm{LMSD} \,=\, \big|\widehat{\beta}_{\text{real}} -
        \widehat{\beta}_{\text{synth}}\big|.
        \]
        Unless otherwise stated, we set \(\alpha=1\) (absolute returns)
        and report \(\widehat{\beta}\) together with the goodness-of-fit
        of the linear regression.
        
      \end{enumerate}
  \subsection{Utility Evaluation Measures: (Deep) hedging}
    Utility measures are critical for evaluating synthetic data 
    beyond statistical metrics, as they assess the practical value 
    of generated data in real-world financial applications 
    \cite{Boursin22}. Our benchmark incorporates (deep) hedging as a 
    utility measure for several key reasons:

    \begin{enumerate}[label=\textbf{[U\arabic*]}]
      \item \label{utility:application}\textbf{Real-world 
      Application Testing:}
      (Deep) hedging provides a concrete way to evaluate how 
      synthetic data performs in actual financial tasks, 
      particularly in derivatives pricing and risk management.

      \item \label{utility:industry}\textbf{Industry-relevant 
      Metrics:}
      By comparing hedging strategies trained on synthetic versus 
      real data, we can assess the practical utility of synthetic 
      data through metrics that matter to financial practitioners, 
      such as replication errors and hedging performance.

      \item \label{utility:validation}\textbf{Model Robustness 
      Validation:}
      (Deep) hedging helps verify if synthetic data maintains the 
      complex relationships and market dynamics necessary for 
      developing reliable trading strategies.
    \end{enumerate}

    \subsubsection{(Deep) hedger Problem Defintion}
      We consider a continuous-time financial market defined on a 
      probability space $(\Omega, \mathcal{F}, \mathbb{P})$, over 
      a finite time horizon $0 < T < \infty$, equipped with a 
      filtration $\mathcal{F} = (\mathcal{F}_t)_{0 \leq t \leq T}$ 
      that represents the evolution of information through time.  
      The market consists of asset $S_t$. For simplicity, we assume a zero 
      interest rate environment.
      The asset \(S_t\) follow a stochastic differential
      equation. We focus on a single underlying asset \(S_t\), and we are
      concerned with hedging a European call option with strike price \(K\)
      and maturity \(T\) so the payoff is \(g(S_T) = \max(S_T - K, 0)\).

      We study the hedging problem associated with a contingent 
      claim delivering a payoff $g(S_T)$ at maturity $T$, where 
      $S_T$ represents the terminal value of the underlying asset 
      vector.  
      The hedging strategy is implemented at a discrete set of 
      times $0 = t_0 < t_1 < \dots < t_{N-1} < t_N = T$.  
      A \textit{self-financing portfolio} is described by a 
      $d$-dimensional $\mathcal{F}_t$-adapted process $\Delta_t$, 
      and its terminal wealth, denoted $X_T^{\Delta, p}$, is 
      given by
      \begin{equation}
          X_T^{\Delta, p} = p + \sum_{i=1}^{d} \sum_{j=0}^{N-1} 
          \Delta_{t_j}^i \left( F_{t_{j+1}}^i - F_{t_j}^i \right),
      \end{equation}
      where $p \in \mathbb{R}$ represents the initial premium.

      The objective is to determine the optimal premium and 
      trading strategy $(p^{\text{opt}}, \Delta^{\text{opt}})$ 
      that minimize the expected squared hedging error:
      \begin{equation}
          (p^{\text{opt}}, \Delta^{\text{opt}}) 
          = \operatorname*{Argmin}_{p, \Delta} 
          \mathbb{E} \left[ \big( X_T^{\Delta} - g(S_T) 
          \big)^2 \right].
      \end{equation}

      To address this optimization problem, we employ the global 
      approach proposed by Fécamp \textit{et al.} (2020), chosen 
      for its computational efficiency and scalability.  
      In this framework, the control policy $\Delta$ is 
      approximated by a feed-forward neural network—referred to as 
      a \textit{deep hedger}—which jointly learns the optimal 
      trading strategy and corresponding premium through 
      end-to-end training \cite{Fecamp20}.

      \begin{figure}[h]
        \centering
        \includegraphics[width=0.3\textwidth,keepaspectratio]{../results/deep_hedger_architecture.png}
        \caption{Architecture overview of the (deep) hedger evaluation framework, 
        illustrating the flow from data preparation through model training to 
        performance evaluation.}
        \Description{Diagram showing the complete pipeline of the deep hedger 
        architecture including data splits, model training, and evaluation steps.}
        \label{fig:deep_hedger_architecture}
      \end{figure}
    
  \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=8cm, keepaspectratio]
      {../results/main_experiment/figure_1_per_metric_heatmaps/figure_1_all_taxonomies_ranknorm_heatmap.png}
      \caption{Normalized ranking heatmap across all evaluation 
      taxonomies.
      For diversity metrics, a score of (1) indicates
      highest metric value (most diverse), while for other metrics,
      a score of (1) indicates lowest metric value (best performance).
      Raw minimum and maximum values are provided for each metric on 
      top of each respective column.
      (Specification: sequence length based on
      maximum significant PACF lag)}
      \Description{The heatmap displays the normalized rankings of various
      models across different evaluation metrics.}
      \label{fig:heatmap_all_taxonomies}
  \end{figure*}
  
    \subsubsection{(Deep) hedger Architecture}
      \begin{table}[h]
        \centering
        \caption{Notations for (deep) hedger architecture.}
        \label{tab:deephedger-notation}
        \begin{tabular}{ll}
        \toprule
        \textbf{Symbol}     & \textbf{Description} \\
        \midrule
        \( D \)             & Time-series dataset             \\
        \( T \)             & Task                            \\
        \( S \)             & Score                           \\
        \( A \)             & (deep) hedger algorithm           \\
        \( \mathcal{A}_n \) & Set of n (deep) hedger algorithms \\
        \( M \)             & (deep) hedger model               \\
        \( \mathcal{M}_{TS} \)   & (SDGFTS) model                  \\
        \bottomrule
        \end{tabular}
      \end{table}

      Our (deep) hedger architecture centers around the 
      Train-Synthetic-Test-Real (TSTR) evaluation framework 
      \cite{Leznik21}, adapted for financial time series 
      generation. We begin with our real FTS dataset 
      \( D\)\textemdash partitioned to a training, validation, 
      and test set respectively as
      \( D_r = \{D_r^{train}, D_r^{validate}, D_r^{test}\} 
      \)\textemdash along with a specific hedging task \(T\) and a 
      performance score \(S\) to evaluate hedging effectiveness, to 
      which we will go indepth in the next subsubsection.
      We consider a (deep) hedger algorithm \(A\) (e.g. a 5-layer 
      perceptron) that takes as input the dataset \(D_r\) and 
      task \(T\),

      In the context of SDGFTS evaluation, we consider a SDGFTS 
      model \( \mathcal{M}_{TS} \) trained on \( D_r^{train} \) 
      and validated with \( D_r^{validate} \) to generate synthetic 
      FTS data \( D_g \).

      Now we can train a (deep) hedger algorithm \( A \) on the 
      datasets \( D_r^{train} \) and \( D_g^{train} \) 
      respectively, resulting in two (deep) hedger models
      \( M_r \) and \( M_g \).

      Finally, we evaluate both models on the real test set 
      \( D_r^{test} \) to obtain the corresponding performance 
      scores \( s_r \) and \( s_g \).

      The goal of the (deep) hedger portfolio evaluation is to 
      evaluate the effectiveness of the model 
      \( \mathcal{M}_{TS} \) with downstream tasks (hedging) 
      by comparing the scores \( s_r \) and \( s_g \). The model 
      \( \mathcal{M}_{TS} \) is considered effective if the 
      the results yielded similar scores, i.e. 
      \( s_r \approx s_g \) \textemdash preferably having higher 
      performance with results of \( s_g > s_r \) \textemdash 
      indicating that the synthetic data generated by 
      \( \mathcal{M}_{TS} \) is useful for training a (deep) hedger.

      \subsubsection{Augmented Testing}
      \label{sec:augmented-testing}
      To better evaluate the performance of (deep) hedgers trained 
      on synthetic data, we train our (deep) hedger \( A \) on a 
      new dataset \( D_{aug} \) that combines both real training 
      data \( D_r^{train} \) and synthetic data \( D_g\). In 
      other words, we train model \( M_g \) on \( D_{aug} \) 
      where \( D_{aug} := D_r^{train} \cup D_g\).
      Then we proceed to evaluate scores between models
      \( M_g \) and \( M_r \) on the real test set, where 
      \( M_r \) is still trained on only real data 
      \( D_r^{train} \).
      This extension, inspired by Stenger et al. \cite{Stenger24}, 
      allows us to assess whether synthetic data provides additional 
      value when combined with real training data.


      \subsubsection{Algorithm Comparison}
      \label{sec:algorithm-comparison}
      Another extension is to compare multiple (deep) hedger 
      algorithms to determine the degree to which each algorithm 
      performs equally on generated data relative to other algorithms, 
      compared to their performance on real data.

      Given a set of \( n \) (deep) hedger algorithms 
      \( \mathcal{A}_n = \{ A_1, \ldots, A_n \} \), we train 
      each algorithm \( A_i \in \mathcal{A}_n \) on both real 
      training data \( D_r^{train} \) and synthetic data 
      \( D_g^{train} \) respectively, resulting in two sets of 
      (deep) hedger models \(\{M_r^1, M_r^2, \ldots, M_r^n\} \) 
      and \( \{M_g^1, M_g^2, \ldots, M_g^n\} \) and two ordered 
      sets of performance scores 
      \( s_r := \{s_r^1, s_r^2, \ldots, s_r^n\} \) and 
      \( s_g := \{s_g^1, s_g^2, \ldots, s_g^n\} \) respectively.

      We compare the relative performance of each 
      algorithm on real data versus synthetic data by computing 
      the Spearman's rank correlation coefficient \( r_S \) 
      \cite{Lin21} between the two score sets \( s_r \) and 
      \( s_g \):
      \begin{equation}
        \begin{aligned}
          r_S &= 1 - \frac{6 \sum_{i=1}^{n} ( \mathrm{rank}(s_r^i) 
          - \mathrm{rank}(s_g^i) )^2}{n(n^2 - 1)},
        \end{aligned}
        \label{eq:Spearman}
      \end{equation}
      where \( \mathrm{rank}(s_r^i) \) and 
      \( \mathrm{rank}(s_g^i) \) denote the ranks of scores 
      \( s_r^i \) and \( s_g^i \) within their respective sets.

      A high positive correlation (i.e., \( r_S \) 
      close to 1) indicates that the synthetic data 
      generated by \( \mathcal{M}_{TS} \) preserves the relative 
      performance of different (deep) hedger algorithms, suggesting 
      that the synthetic data is suitable for algorithm evaluation 
      and comparison.

      The (deep) hedger algorithms \( \mathcal{A}_n \) considered in 
      this benchmark can be found in appendix 
      \ref{appendix:deep-hedger-algorithms}.

      \subsubsection{(Deep) hedger Portfolio Evaluation}
      We evaluate the suite of hedger algorithms trained on 
      synthetic and real data using the portfolio replication error 
      or replication loss metric \cite{Boursin22}.



\section{Results and Analysis}
  We present a concise, comprehensive evaluation of the synthetic
  financial time series produced by each model. The analysis covers
  marginal and second-order statistics, temporal dependencies,
  diversity measures, stylized-facts verification, and generation
  efficiency. Results are reported on the minute-level closing price
  and summarized in the tables and figures that follow: fidelity
  metrics, diversity metrics, stylized-fact comparisons, and
  generation-time measurements. Best-performing entries are
  highlighted where appropriate and comparative discussion appears
  in the subsequent analysis subsections.

  In §\ref{sec:stats_results} and §\ref{sec:hedger_results}, results
  displayed and analysis are presented based on SDGFTS traingin with
  sequenc length determined by the maximum significant PACF lag
  (§\ref{sec:model-specific}).


  \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=8cm,keepaspectratio]
      {../results/main_experiment/figure_2_tsne_distribution/figure_2_tsne_distribution.png}
      \caption{t-SNE visualization and distribution plots comparing real and synthetic financial time series data across models.
      (Specification: sequence length based on
      maximum significant PACF lag)}
      \Description{t-SNE embeddings and histograms showing the overlap and distributional properties of real versus synthetic data generated by different SDGFTS models.}
      \label{fig:tsne_distribution}
  \end{figure*}

  \subsection{Statistical Evaluation Results}\label{sec:stats_results}
    The following 2 sections are basesd on the results summarized in
    figure \ref{fig:heatmap_all_taxonomies}.
    \subsubsection{Stylized Facts and Fidelity}
    The block bootstrap \ref{alg:blockbootstrap2} consistently 
    achieves the strongest 
    performance across both distributional metrics and stylized facts. 
    This outcome is expected: by resampling contiguous blocks from the 
    training data, the block bootstrap directly inherits empirical 
    marginal distributions \ref{metric:mdd} and short- to 
    medium-range dependence 
    structures. As a result, it naturally preserves key stylized facts 
    (e.g., heavy tails \ref{metric:kd}, volatility clustering 
    \ref{metric:volclustering}) as long as they are present within 
    the chosen block length.

    Among parametric models, GARCH \ref{alg:garch} performs second 
    best overall. This is
    consistent with its widespread use in financial time series modeling:
    GARCH is explicitly designed to capture volatility clustering and
    volatility persistence. While standard GARCH models do not exhibit
    true long-memory behavior (in the strict statistical sense), they
    nonetheless approximate slowly decaying volatility autocorrelations
    well enough to score favorably on stylized-fact metrics relative to
    simpler parametric models such as OU process and GBM.

    The DEJD \ref{alg:dejd} model underperforms on kurtosis-
    based metrics \ref{metric:kd} despite its ability to generate fat tails.
    Although the jump component introduces excess kurtosis relative to pure
    diffusion processes, the model is not inherently calibrated to
    reproduce empirical tail thickness precisely. Its weaker performance
    likely reflects limitations in parameter estimation and
    mismatch between assumed jump dynamics and the empirical return
    distribution.

    Volatility models such as GBM \ref{alg:gbm} and the OU process \ref{alg:ou}
     perform poorly on
    stylized facts (§ \ref{sec:stylized-facts}). This result is unsurprising: 
    both models assume
    conditionally Gaussian increments with constant (GBM) or mean-reverting
    (OU) volatility, and therefore cannot reproduce volatility clustering
    or persistent heteroskedasticity observed in real financial time
    series.

    A notable result is that QuantGAN performs significantly worse on
    distributional metrics while capturing stylized facts relatively well.
    This suggests that the adversarial objective is effective at matching
    temporal dependence and higher-order dynamics, but less effective at
    aligning marginal distributions without explicit distributional
    constraints. TimeVAE performs poorly overall, which is consistent with
    posterior collapse when trained on log returns—limiting its ability to
    generate meaningful variability and capture higher-order financial
    structure.

    \subsubsection{Efficiency}

    Efficiency results largely follow model complexity. GBM, as the
    simplest model both conceptually and computationally, is the fastest
    to train and sample from. In contrast, QuantGAN is the slowest by
    design, reflecting the cost of adversarial training and high-capacity
    neural architectures.

    \subsubsection{Visualization}

    Figure \ref{fig:tsne_distribution} presents t-SNE embeddings and
    return distribution plots (see §\ref{sec:visualization_methods}) 
    for real versus synthetic data across
    models. The block bootstrap \ref{alg:blockbootstrap2}shows 
    near-perfect overlap in t-SNE space, reflecting its direct 
    resampling from real data. QuantGAN \ref{alg:quantgan} and
    DEJD \ref{alg:dejd} also shows better distribution alignment,
    but does not full capture the clusters in the t-SNE plots.

    While other parametric models do not capture these clusters fully,
    they generally corroborate the quantitative findings: GARCH
    \ref{alg:garch} shows better distributional alignment than GBM
    \ref{alg:gbm} and OU process \ref{alg:ou}, while TimeVAE
    \ref{alg:timevae} exhibits the worst overlap and distributional
    mismatch, as it appears to suffer from posterior collapse.

    \subsubsection{Diveristy metrics}\label{sec:diversity_introduce}

    Diversity alone is ambiguous: high variability does not guarantee
    realism and may violate structural dependencies. We therefore evaluate
    diversity jointly with fidelity to ensure variability remains
    plausible. We revisit this in research question \ref{rq:2} and in
    §\ref{sec:metric_tradeoffs}.
    
    \subsection{Utility Evaluation Results}\label{sec:hedger_results}
    \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=8cm,keepaspectratio]
      {../results/main_experiment/figure_1_per_metric_heatmaps/figure_1_utility_and_algorithm_ranknorm_heatmap.png}
      \caption{Normalized ranking heatmap for mean replication values
      of each hedger trained on SDGFTS/real data. Raw minimum and 
      maximum values are provided for each metric column.
     (Specification: Augmented datasets and sequence length based on
      maximum significant PACF lag).}
      \Description{Heatmap displaying normalized utility metric rankings 
      and algorithm comparison results across different SDGFTS models.}
      \label{fig:heatmap_utility_algorithm}
    \end{figure*}

    As see in Figure \ref{fig:heatmap_utility_algorithm},
    for the augmented testing \ref{sec:augmented-testing}, we can make the observation that for the
    hedging models, the neural network models generally perform better
    than the parametric models in estimating the deltas. This is expected
    as the neural network models are able to capture the higher-order
    dynamics of the index series, which the parametric models are not
    able to capture \cite{Fecamp20}.

    In the augmented-testing setting, with algorithms compared across
    datasets (see §\ref{sec:algorithm-comparison}), we observe that
    the rank-normalized values of the
    heatmaps obtained using QuantGAN \ref{alg:quantgan}
    and BlockBootstrap \ref{alg:blockbootstrap2}
     remain highly consistent with those derived from real-only
    data. In particular, rank-normalized hedging performance under
    QuantGAN augmentation closely matches the baseline ordering, while
    BlockBootstrap augmentation yields nearly identical results. This
    behavior is expected for BlockBootstrap, which preserves empirical
    dependence structures by construction, and suggests that
    QuantGAN-generated samples do not distort the hedger-relevant
    dynamics when mixed with real data. Thus, both augmentation
    strategies maintain the relative sensitivities and preferences of
    hedger models observed under real-data training, indicating that
    these synthetic generators can be used for augmentation without
    materially altering hedging-based performance assessments.

    For Spearman correlation (see Equation \ref{eq:Spearman}),
    recommended by Stenger et al., it's important to note that this metric
    considers only the ranking of model performances, completely ignoring
    the actual delta values between them \cite{Stenger24}. This can be 
    problematic: for
    example, TimeVAE \ref{alg:timevae} \textemdash despite suffering 
    from mode collapse and generating
    less realistic outputs \textemdash may still achieve a high Spearman correlation
    if it preserves the order of model performances, even though the
    deltas may be substantially off or meaningless. Conversely, QuantGAN,
    which better captures the structure of financial data, might produce
    more realistic values but achieve a worse Spearman score if the
    ranking changes slightly. Thus, relying solely on Spearman
    correlation can be misleading, as it overlooks the magnitude and
    quality of differences between models and may obscure meaningful
    distinctions in model performance.
  
  \subsection{\ref{rq:1} Metric Response on Sequence Length Training}
    We investigate how training sequence length affects evaluation
    metrics as posed in research question \ref{rq:1}.
    As previous sections describe how sequence length is determined
    based on model-specific PACF analysis (§\ref{sec:model-specific})
    how they affect the Stonk Bench architecture.

    Recall that the sequence lengths considered are 60, 120, 180, 240,
    and 300 minutes respectively.

  We analyze each metric as the training sequence length varies from
  60 to 300 minutes.

\subsubsection{Fidelity}
  Rank-normalized MDD values are essentially invariant to sequence
  length, which is expected for a marginal distribution metric unless
  models generate time-varying distributions (not observed here).
  Skewness differences show minimal variation for GARCH(1,1), GBM, OU,
  and MJD, suggesting limited sensitivity to sequence length. Modest
  rank improvements for BlockBootstrap and DEJD likely reflect better
  capture of asymmetric tail events. Kurtosis differences for GARCH,
  MJD, OU, and GBM change marginally with sequence length, consistent
  with limited extreme-tail generation; BlockBootstrap improves with
  longer sequences by resampling rare extremes. See the fidelity
  ablation heatmap in Figure~\ref{fig:ablation_fidelity}.

\subsubsection{Diversity}
  Diversity rankings remain stable across sequence lengths, consistent
  with largely unchanged marginal distributions. However, the minimum
  and maximum diversity values increase with sequence length, as longer
  windows permit greater variability across time steps. See the
  diversity ablation heatmap in Figure~\ref{fig:ablation_diversity}.

\subsubsection{Efficiency}
  Maximum generation time increases monotonically with sequence length,
  while model rankings remain unchanged, reflecting computational
  complexity rather than algorithmic reordering. See the efficiency
  ablation heatmap in Figure~\ref{fig:ablation_efficiency}.

\subsubsection{Stylized facts}
  The maximum values for lag-1 autocorrelation of absolute returns and
  volatility clustering increase with sequence length, whereas long
  memory measures tend to decrease. Rankings for QuantGAN,
  BlockBootstrap, and DEJD are largely preserved across lengths. See
  the stylized-facts ablation heatmap in
  Figure~\ref{fig:ablation_stylized_facts}.

\subsubsection{Spearman correlation}
  Spearman's rank correlation summarizes ordering only and can be
  misleading when magnitude differences matter; reliance on it alone is
  therefore discouraged. See the utility correlation comparison in
  Figure~\ref{fig:ablation_utility_spearman}.


  \subsection{\ref{rq:2} Metric Tradeoffs and Interactions}
  \label{sec:metric_tradeoffs}
    We dedicate this subsection to answer the research question
    \ref{rq:2} regarding trade-offs and interactions between different
    evaluation metrics.

    \subsubsection{Diversity versus Fidelity}
      As introduced in §\ref{sec:diversity_introduce}, diversity
      metrics must be interpreted in conjunction with fidelity and
      stylized-fact metrics. How should we interpret these relationships? 
      Recall that "difference" here means the discrepancy between real 
      and synthetic data—the lower the value, the better the model is 
      matching the real data.

      The MDD \ref{metric:mdd} and SDD \ref{metric:sdd} 
      both show a notably strong positive correlation with ICD 
      Euclidean diversity \ref{metric:ed}. For example, MDD has a 
      Pearson correlation coefficient of \( r = 0.68 \) (p = 0.09), 
      and STD difference has an even higher \( r = 0.97 \) 
      (p = 1.90e-4) (see Figure \ref{fig:eclidian_fidelity_diversity}). 
      This indicates that as the marginal or standard deviation 
      difference between real and synthetic data increases (i.e., as 
      fidelity to the real data worsens), the diversity, as measured 
      by ICD Euclidean, also tends to increase.

      The findings indicate that increased diversity, as measured by ICD 
      Euclidean, often correlates with reduced fidelity in basic 
      distributional statistics. This suggests that models generating 
      more diverse samples may deviate significantly from the real data 
      distribution, highlighting the necessity of balancing diversity 
      with fidelity. While some diversity is beneficial for generative 
      models, excessive diversity can lead to less realistic outputs. 
      Therefore, it is crucial to evaluate both fidelity and diversity 
      metrics together when assessing generative time series models.

    \subsubsection{Diversity versus Stylized Facts}
      Figure \ref{fig:eclidian_stylized_facts_diversity} presents
      scatter plots illustrating the relationship between ICD Euclidean
      diversity \ref{metric:ed} and key stylized-fact metrics:
      ACD \ref{metric:acd}, volatility clustering \ref{metric:volclustering},
      and LMSD \ref{metric:slow_decay}.

      Volatility clustering shows a weak positive correlation with
      diversity (p \( \approx \) 0.1). This suggests that more diverse models
      tend to preserve stronger volatility clustering, indicating
      that allowing a broader range of sample paths may help retain
      persistent volatility dynamics, though the evidence is
      marginal.

      Long memory in volatility is also positively and mildly
      correlated with diversity. This implies that models producing
      more varied samples are more likely to exhibit persistent,
      slowly decaying volatility correlations, consistent with
      richer temporal dynamics emerging from increased generative
      flexibility.

    \begin{figure}[h]
      \centering
      \includegraphics[width=0.45\textwidth,height=6cm,keepaspectratio]
      {../results/appendix/figure_6_stylized_facts_vs_diversity/figure_6_icd_euclidean_2x2_grid.png}
      \caption{Scatter plots showing the relationship between 
      ICD Euclidean diversity and stylized facts metrics (ACD, 
      volatility clustering, and LMSD). Each point represents a model, 
      with trend lines indicating correlation direction and strength.}
      \Description{Scatter plots illustrating the correlation between 
      ICD Euclidean diversity and stylized facts metrics (ACD, 
      volatility clustering, and LMSD)
      across different models.}
      \label{fig:eclidian_stylized_facts_diversity}
    \end{figure}

    \subsubsection{Efficiency versus other metrics}
      Efficiency, measured by generation time, shows no significant
      correlation with fidelity, diversity, or stylized-fact metrics.
      This indicates that faster models do not necessarily compromise
      on quality or realism, nor do slower models guarantee better
      performance. Thus, efficiency appears to be largely independent
      of other evaluation dimensions in this benchmark.

\section{Conclusion and Future Work}
  In our research, we recognize the existing limitations in the
    evaluation of synthetic time series, particularly the absence of a
    universally accepted framework. To address this, we adopt the
    evaluation taxonomy proposed by Stenger et al. and expand upon it to
    create a comprehensive benchmark specifically tailored for Synthetic
    Data Generation for Financial Time Series (SDGFTS).
    Our contributions include the development of a consolidated evaluation
    framework that systematically reviews and integrates various
    assessment methods from leading studies in the field. This approach
    not only enhances the rigor of evaluations but also facilitates the
    comparison of different SDGFTS models, ultimately providing a more
    standardized and reliable means of assessing synthetic data quality.

  We believe that Stonk Bench will serve as a valuable resource for
    researchers and practitioners in the field of synthetic financial
    time series generation.
    We hope that our benchmark will facilitate more rigorous and
    standardized evaluations of SDGFTS models, ultimately advancing
    the state of the art in this important area of research.
    For the broader field of synthetic data generation, our work
    highlights the importance of a comprehensive and unifying
    evaluation frameworks that consider both statistical properties
    and practical utility that is proposed by Stenger \textit{et al.}.
  \subsection{Summary of Findings}
  We presented Stonk-Bench, a comprehensive benchmark for evaluating 
  Synthetic Data Generation for Financial Time Series (SDGFTS). This 
  benchmark integrates statistical fidelity, diversity, stylized facts 
  verification, and utility evaluation through (deep) hedging. We evaluated 
  a suite of 5 traditional, 3 deep learning-based, and 1 other SDGFTS 
  models on the S\&P 500 minute-level dataset.
  \subsection{Stonk Bench Shortcomings}\label{subsub:shortcomings}
      While Stonk Bench represents a significant step forward in the
      evaluation of SDGFTS models, we acknowledge several limitations
      in our current work:
    \begin{enumerate}[label=\textbf{[S\arabic*]}]
      \item \textbf{Incompatibility with other models:}
        Stonk Bench has so far been evaluated mainly on classical
        stochastic models (e.g., GBM, OU, GARCH) and a subset of
        deep-learning approaches (GANs, VAEs).
        Other model families — such as
        reinforcement-learning-based generators, normalizing flows,
        autoregressive networks, and hybrid methods — have not yet
        been benchmarked.
      \item \textbf{No hyperparameter tuning:} For consistency and
        comparability across models, StonkBench evaluations were
        performed using default model configurations, without
        performing hyperparameter optimization. This ensures a fair
        baseline but may not reflect the absolute best performance
        achievable by each model.
      \item \textbf{Metric and procedure generalizability:}
        As a consequence, some metrics or evaluation procedures may
        not directly translate to these untested families, which
        limits the framework's generalizability across all synthetic
        financial time-series generation methods.
      \item \textbf{Data Leakage from Public Datasets:} Current models
        can potentially overfit to publicly available datasets
        \textemdash the same datasets that Stonk Bench is using
        \textemdash leading to inflated performance metrics that do
        not generalize well to unseen data. One possible solution is
        to either curate proprietary datasets or implement a grace
        period so the model can be tested on new data that was not
        publicly available during its development.
      \item \textbf{Computational Resource Requirements:} The
        comprehensive nature of Stonk Bench \textemdash particularly
        accommodating any submitted (deep) hedging model and the deep
        hedging utility evaluation \textemdash demands significant
        computational resources. This may limit accessibility for
        researchers with constrained resources.
    \end{enumerate}
  \subsection{Future Research Directions}
    \subsubsection{Remaining progress} In regard to the next portion
    of our project, we aim to expand Stonk Bench in several key areas.
    \begin{enumerate}[label=\textbf{[F\arabic*]}]
      \item \textbf{Multivariate time series predictions.}
        Extend Stonk Bench to evaluate models on multivariate time
        series data, enabling a more comprehensive assessment of their
        performance in realistic scenarios. We wish test SDGFTS models
        on datasets with multiple correlated assets and datasets with
        Bid-Ask spreads and trading volume to evaluate their ability to
        capture inter-asset dependencies and market microstructure
        effects \cite{Takahashi24}.
        \item \textbf{Data granularity evaluation.}
        We wish to test SDGFTS models across multiple data granularities
        (minute-level, daily, and weekly) to evaluate model performance
        on different temporal scales and their ability to capture both
        short-term and long-term dependencies. The most relevant granularities
        that are commonly used in financial analysis and trading strategies
        are 5-minute, 30-minute, and daily data \cite{Wang20, Cont01}.
    \end{enumerate}

    \subsubsection{Beyond the project} We envision Stonk Bench
    evolving into a community-driven benchmark for SDGFTS evaluation.
    As we acknowledge the shortcomings outlined in
    §\ref{subsub:shortcomings}, we propose the following future
    directions:
    \begin{enumerate}[label=\textbf{[F\arabic*]}, resume]
      \item \textbf{Continuous model inclusion.}
      Maintain Stonk Bench as a living benchmark by incorporating
      newly developed SDGFTS techniques (normalizing flows,
      autoregressive generators, advanced diffusion models,
      reinforcement-learning-based generators, and hybrids)
      \cite{Potluru24}.
      \item \textbf{Rigorous metric selection and ablations.}
      As mentioned in Stenger et al., we wish to rigorously conduct
      ablation studies to identify the most informative and robust
      metrics for different model families and stylized facts;
      produce guidance for a compact, interpretable metric set
      \cite{Stenger24}.
      \item \textbf{Categories of datasets} We wish to format Stonk
      Bench to cover a variaty datasets categorized by asset class (
      equities, forex, commodities, cryptocurrencies), 
      granularity (minute, five-minute, daily), and datatypes (OHCL,
      BAV).
      \item \textbf{Public benchmark platform.}
      Launch a website with documentation, code, datasets, and a
      leaderboard for submitted SDGFTS models; provide CI-enabled
      evaluation pipelines for reproducible comparisons.
      \item \textbf{Community engagement.}
      Foster collaboration via contribution guidelines, open issues,
      and reproducible example workflows so researchers can extend
      and validate the benchmark.
    \end{enumerate}

\begin{acks}
    To professor Irene Huang, University of Toronto, for her
    supervision and guidance throughout the project.
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{stonk-bench}



\appendix

\section{Utility Evaluation Logistics}
  Detailed procedures for (deep) hedging evaluation, including
  dataset splits, model architectures, training hyperparameters,
  and evaluation metrics.

  \subsection{Hedging Tasks Specifications}
    We consider the hedging of a European call option on a single
    underlying asset \( S_t \) with strike price \( K \) and
    maturity \( T \). The option payoff at maturity is given by
    \( g(S_T) = \max(S_T - K, 0) \).

    For each sequence length tested in §\ref{sec:sequence_length},
    we have set the corresponding option maturity \( T \) to match
    the length of the generated sequences. For example, if the
    sequence length is 60 minutes, then the option maturity \( T \)
    is set to 60 minutes.

  \subsection{Algorithm Comparison Models}\label{appendix:deep-hedger-algorithms}
    In §\ref{sec:algorithm-comparison}, we described the procedure
    for comparing multiple (deep) hedger algorithms to evaluate the
    relative performance preservation on synthetic data versus real
    data.
  
      We have considered the following deep hedger algorithms:
      \begin{enumerate}[label=\textbf{[H\arabic*]}]
        \item \label{hedger:feedforwardlag} \textbf{Feedforward Layers
        Neural 
        Network (FLNN).} A standard multi-layer perceptron with 
        fully connected layers, ReLU activations, and dropout for 
        regularization.

        \item \label{hedger:feedforward} \textbf{Feedforward Time
        Neural Network (FTNN).} A feed-forward neural network designed 
          to process time series data by accepting lagged observations 
          as input features. This architecture uses fully connected 
          layers with ReLU activations and dropout for regularization, 
          enabling it to learn non-linear relationships between past 
          and future values without explicit recurrence.
        
        \item \label{hedger:lstm} \textbf{Long Short-Term Memory 
        (LSTM).} A recurrent neural network architecture designed 
        to capture temporal dependencies in sequential data, 
        particularly effective for time series tasks.
        
        \item \label{hedger:rnn} \textbf{Recurrent Neural Network (RNN).}
        An attention-based architecture that models long-range 
        dependencies in sequences without relying on recurrence, 
        utilizing self-attention mechanisms to enhance performance 
        on sequential data.
      
      \end{enumerate}
    
    We also considered the following (non-deep) hedging models:
    \begin{enumerate}[label=\textbf{[H\arabic*]}, resume]
      \item \label{hedger:black-scholes} \textbf{Black-Scholes 
      Delta Hedging (BSD).} A classical hedging strategy based on the 
      Black-Scholes option pricing model, which computes the 
      delta of the option to determine the optimal hedge ratio.

      \item \label{hedger:binomial} \textbf{Binomial Delta-Gamma Hedging (BDG).}
      A discrete-time hedging strategy that uses a binomial tree
      model to compute both delta and gamma of the option, allowing
      for more accurate hedging in the presence of non-linear price
      movements.

      \item \label{hedger:lreg} \textbf{Linear Regression Hedging (LR).} 
      A hedging strategy that uses linear regression to estimate 
      the hedge ratios based on historical data.

      \item \label{hedger:xgboost} \textbf{XGBoost Hedging (XGB).}
      A machine learning-based hedging strategy that employs
      gradient boosting decision trees to learn optimal hedge ratios
      from historical data.  
    \end{enumerate}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/ablation_studies/figure_3_metric_heatmaps_per_metric/figure_3_fidelity_per_metric_ranknorm.png}
  \caption{Ablation study: fidelity per-metric rank-normalized heatmap across sequence lengths.}
  \Description{Heatmap showing fidelity metrics across sequence length ablations.}
  \label{fig:ablation_fidelity}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/ablation_studies/figure_3_metric_heatmaps_per_metric/figure_3_diversity_per_metric_ranknorm.png}
  \caption{Ablation study: diversity per-metric rank-normalized heatmap across sequence lengths.}
  \Description{Heatmap showing diversity metrics across sequence length ablations.}
  \label{fig:ablation_diversity}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/ablation_studies/figure_3_metric_heatmaps_per_metric/figure_3_stylized-facts_per_metric_ranknorm.png}
  \caption{Ablation study: stylized-facts per-metric rank-normalized heatmap across sequence lengths.}
  \Description{Heatmap showing stylized-facts metrics across sequence length ablations.}
  \label{fig:ablation_stylized_facts}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/ablation_studies/figure_3_metric_heatmaps_per_metric/figure_3_efficiency_per_metric_ranknorm.png}
  \caption{Ablation study: efficiency per-metric rank-normalized heatmap across sequence lengths.}
  \Description{Heatmap showing efficiency metrics across sequence length ablations.}
  \label{fig:ablation_efficiency}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/ablation_studies/figure_3_metric_heatmaps_per_metric/figure_3_utility_spearman_and_mixed_per_seq.png}
  \caption{Ablation study: utility (Spearman) per-sequence-length comparison across hedger algorithms between real and sythetic data.}
  \Description{Heatmap or plot showing utility Spearman correlations per sequence length.}
  \label{fig:ablation_utility_spearman}
\end{figure*}

      \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=10cm,keepaspectratio]
      {../results/appendix/figure_5_fidelity_vs_diversity/figure_5_icd_euclidean_2x3_grid.png}
      \caption{Scatter plots and linear regression analysis showing the relationship between 
      fidelity metrics (MDD, MD, SDD, SD, KD, by row) and intra-class Euclidean distance (ICD-ED) 
      across all SDGFTS models. Each subplot displays the fitted regression line with 
      corresponding R² value and p-value, indicating the strength and statistical significance 
      of the linear relationship between fidelity and diversity.}
      \Description{A heatmap displaying the normalized rankings of diversity metrics for
      different SDGFTS models, highlighting the performance scores and their implications for model diversity.}
      \label{fig:eclidian_fidelity_diversity}
    \end{figure*}

      \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=10cm,keepaspectratio]
      {../results/appendix/figure_5_fidelity_vs_diversity/figure_5_icd_dtw_2x3_grid.png}
      \caption{Scatter plots and linear regression analysis showing the relationship between 
      fidelity metrics (MDD, MD, SDD, SD, KD, by row) and intra-class DTW distance (ICD-DTW) 
      across all SDGFTS models. Each subplot displays the fitted regression line with 
      corresponding R² value and p-value, indicating the strength and statistical significance 
      of the linear relationship between fidelity and diversity measured by DTW.}
      \Description{Scatter plots showing the correlation between fidelity metrics and 
      ICD-DTW diversity across different SDGFTS models.}
      \label{fig:dtw_fidelity_diversity}
    \end{figure*}

      \begin{figure*}[h]
      \centering
      \includegraphics[width=\textwidth,height=10cm,keepaspectratio]
      {../results/appendix/figure_6_stylized_facts_vs_diversity/figure_6_icd_dtw_2x2_grid.png}
      \caption{Scatter plots showing the relationship between stylized facts metrics 
      (ACD, volatility clustering, and LMSD) and intra-class DTW distance (ICD-DTW) 
      across all SDGFTS models. Each point represents a model, with trend lines 
      indicating correlation direction and strength.}
      \Description{Scatter plots illustrating the correlation between stylized facts 
      metrics and ICD-DTW diversity across different models.}
      \label{fig:dtw_stylized_facts_diversity}
    \end{figure*}

\section{Supplementary t-SNE Visualizations}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/appendix/figure_4_tsne_supplementary/figure_4_seq_60.png}
  \caption{Supplementary t-SNE visualization for sequence length 60 minutes.}
  \Description{t-SNE embeddings comparing real and synthetic data for 60-minute sequences.}
  \label{fig:tsne_seq_60}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/appendix/figure_4_tsne_supplementary/figure_4_seq_120.png}
  \caption{Supplementary t-SNE visualization for sequence length 120 minutes.}
  \Description{t-SNE embeddings comparing real and synthetic data for 120-minute sequences.}
  \label{fig:tsne_seq_120}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/appendix/figure_4_tsne_supplementary/figure_4_seq_180.png}
  \caption{Supplementary t-SNE visualization for sequence length 180 minutes.}
  \Description{t-SNE embeddings comparing real and synthetic data for 180-minute sequences.}
  \label{fig:tsne_seq_180}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/appendix/figure_4_tsne_supplementary/figure_4_seq_240.png}
  \caption{Supplementary t-SNE visualization for sequence length 240 minutes.}
  \Description{t-SNE embeddings comparing real and synthetic data for 240-minute sequences.}
  \label{fig:tsne_seq_240}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth,keepaspectratio]
  {../results/appendix/figure_4_tsne_supplementary/figure_4_seq_300.png}
  \caption{Supplementary t-SNE visualization for sequence length 300 minutes.}
  \Description{t-SNE embeddings comparing real and synthetic data for 300-minute sequences.}
  \label{fig:tsne_seq_300}
\end{figure*}

\section{Augmented Testing Analysis}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth,keepaspectratio]
  {../results/appendix/figure_7_augmented_testing_delta/figure_7_augmented_testing_2x4_delta.png}
  \caption{Augmented testing performance comparison showing delta metrics 
  across different hedger algorithms and SDGFTS models. Each subplot compares 
  performance when training on real data only versus augmented data 
  (real + synthetic).}
  \Description{Grid of plots showing augmented testing delta metrics for 
  different hedger models and synthetic data generators.}
  \label{fig:augmented_testing_delta}
\end{figure*}



\end{document}